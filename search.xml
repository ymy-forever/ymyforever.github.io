<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>HelloWorld!</title>
    <url>/2020/08/29/HelloWorld/</url>
    <content><![CDATA[<h2 id="A-new-world"><a href="#A-new-world" class="headerlink" title="A new world!"></a>A new world!</h2><p>哈喽！历经一个下午博客终于搭建好了，原来是那么容易的一件事情，大一的时候想的很复杂，迟迟没能动手，现在也终于有了自己的小博客啦~</p>
<p>未来灌水的文章还是会首先发在CSDN上，这里会分享一些重大的经历~已经大三了！要更努力学习！不要被些奇奇怪怪的事情干扰，奥里给！</p>
]]></content>
  </entry>
  <entry>
    <title>经典模型：Vision Transformer</title>
    <url>/2022/09/26/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AVision-Transformer/</url>
    <content><![CDATA[<h2 id="模型介绍"><a href="#模型介绍" class="headerlink" title="模型介绍"></a>模型介绍</h2><ul>
<li>可参考博客：<a href="https://blog.csdn.net/qq_39478403/article/details/118704747">https://blog.csdn.net/qq_39478403/article/details/118704747</a></li>
</ul>
<h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>ViT主要使用Transformer的encoder部分</p>
<img src="/2022/09/26/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AVision-Transformer/image-20220926083713750.png" alt="image-20220926083713750" style="zoom: 67%; margin:auto;">

<ul>
<li>将一张图像分成若干个大小固定且相同的patch，将每个patch投影到线性空间中再加上位置编码</li>
<li>除了每个patch作为一个token外，在序列中添加一个额外的classification token</li>
</ul>
<p><strong>位置编码：（看下代码咋实现的）</strong></p>
<ul>
<li>使用可学习的一维位置编码（作者发现使用更高维的位置编码并没有带来显著的精度提升）</li>
</ul>
<p><strong>混合结构：</strong></p>
<ul>
<li>可以将原始图像使用CNN进行特征提取，将特征图按patch划分送入Transformer中。</li>
</ul>
<h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><p><strong>ViT使用：</strong></p>
<ul>
<li>ViT与Bert类似，先在大数据集上<strong>训练</strong>，再在downstream任务上<strong>微调</strong>。在微调时，将预训练用的预测头换成一个用0初始化的DxK的前馈层，其中K代表下游任务总的类别数。</li>
<li>在微调时，使用更高分辨率的图像可以获得更好的结果。（patch大小不变，输入序列变成，不影响网络结构），这样会导致之前训练得到的<strong>位置编码</strong>无意义，因此在原来的位置编码上进行一个2D的插值。</li>
</ul>
<p><strong>实验结果：</strong></p>
<ul>
<li>当<strong>考虑预训练的训练代价</strong>时，ViT以更低的代价达到了SOTA水平</li>
<li>在<strong>自监督</strong>问题上，ViT很有应用前景</li>
</ul>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><ul>
<li>Official Code：<a href="https://github.com/google-research/vision_transformer">https://github.com/google-research/vision_transformer</a></li>
<li>timm code：<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py">https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py</a></li>
</ul>
<h3 id="Patch-Embeddings"><a href="#Patch-Embeddings" class="headerlink" title="Patch Embeddings"></a>Patch Embeddings</h3><p>实现功能：将输入图像划分为若干个patch，并将每个patch拉平投影到D维。通过一个二维的卷积操作即可实现。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/patch_embed.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 2D Image to Patch Embedding</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">            self,</span></span><br><span class="line"><span class="params">            img_size=<span class="number">224</span>,</span></span><br><span class="line"><span class="params">            patch_size=<span class="number">16</span>,<span class="comment"># 每个patch大小为16x16x3</span></span></span><br><span class="line"><span class="params">            in_chans=<span class="number">3</span>,</span></span><br><span class="line"><span class="params">            embed_dim=<span class="number">768</span>,<span class="comment"># 将patch映射到768维</span></span></span><br><span class="line"><span class="params">            norm_layer=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">            flatten=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">            bias=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        img_size = to_2tuple(img_size)</span><br><span class="line">        patch_size = to_2tuple(patch_size)</span><br><span class="line">        self.img_size = img_size</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.grid_size = (img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>], img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>])</span><br><span class="line">        self.num_patches = self.grid_size[<span class="number">0</span>] * self.grid_size[<span class="number">1</span>]<span class="comment"># 每张图像对应patch个数</span></span><br><span class="line">        self.flatten = flatten</span><br><span class="line">        </span><br><span class="line">		<span class="comment"># 通过一步卷积操作实现嵌入</span></span><br><span class="line">        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias)</span><br><span class="line">        self.norm = norm_layer(embed_dim) <span class="keyword">if</span> norm_layer <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, C, H, W = x.shape</span><br><span class="line">        _<span class="keyword">assert</span>(H == self.img_size[<span class="number">0</span>], <span class="string">f&quot;Input image height (<span class="subst">&#123;H&#125;</span>) doesn&#x27;t match model (<span class="subst">&#123;self.img_size[<span class="number">0</span>]&#125;</span>).&quot;</span>)</span><br><span class="line">        _<span class="keyword">assert</span>(W == self.img_size[<span class="number">1</span>], <span class="string">f&quot;Input image width (<span class="subst">&#123;W&#125;</span>) doesn&#x27;t match model (<span class="subst">&#123;self.img_size[<span class="number">1</span>]&#125;</span>).&quot;</span>)</span><br><span class="line">        x = self.proj(x)<span class="comment"># 投影</span></span><br><span class="line">        <span class="keyword">if</span> self.flatten:</span><br><span class="line">            x = x.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># BCHW -&gt; BNC</span></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h3 id="可学习的嵌入"><a href="#可学习的嵌入" class="headerlink" title="可学习的嵌入"></a>可学习的嵌入</h3><ul>
<li>cls_token：为了与Bert保持一致，设置可学习的嵌入向量作为用于分类的类别向量。</li>
<li>pos_embed：由于自注意力机制具有<strong>扰动不变性</strong>（打乱tokens中的顺序并不会改变结果），因此需要位置编码标识位置信息。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cls_token</span></span><br><span class="line">self.cls_token = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, embed_dim)) <span class="keyword">if</span> class_token <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"><span class="comment"># pos_embed</span></span><br><span class="line">embed_len = num_patches <span class="keyword">if</span> no_embed_class <span class="keyword">else</span> num_patches + self.num_prefix_tokens</span><br><span class="line">self.pos_embed = nn.Parameter(torch.randn(<span class="number">1</span>, embed_len, embed_dim) * <span class="number">.02</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_pos_embed</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="keyword">if</span> self.no_embed_class:</span><br><span class="line">        <span class="comment"># 先加位置编码再拼接cls_token</span></span><br><span class="line">        <span class="comment"># position embedding does not overlap with class token, add then concat</span></span><br><span class="line">        x = x + self.pos_embed</span><br><span class="line">        <span class="keyword">if</span> self.cls_token <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = torch.cat((self.cls_token.expand(x.shape[<span class="number">0</span>], -<span class="number">1</span>, -<span class="number">1</span>), x), dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 先拼接cls_token再加位置编码</span></span><br><span class="line">        <span class="comment"># pos_embed has entry for class token, concat then add</span></span><br><span class="line">        <span class="keyword">if</span> self.cls_token <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = torch.cat((self.cls_token.expand(x.shape[<span class="number">0</span>], -<span class="number">1</span>, -<span class="number">1</span>), x), dim=<span class="number">1</span>)</span><br><span class="line">        x = x + self.pos_embed</span><br><span class="line">    <span class="keyword">return</span> self.pos_drop(x)</span><br></pre></td></tr></table></figure>

<h3 id="多头注意力机制"><a href="#多头注意力机制" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, num_heads=<span class="number">8</span>, qkv_bias=<span class="literal">False</span>, attn_drop=<span class="number">0.</span>, proj_drop=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">assert</span> dim % num_heads == <span class="number">0</span>, <span class="string">&#x27;dim should be divisible by num_heads&#x27;</span></span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        head_dim = dim // num_heads</span><br><span class="line">        self.scale = head_dim ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        self.qkv = nn.Linear(dim, dim * <span class="number">3</span>, bias=qkv_bias)</span><br><span class="line">        self.attn_drop = nn.Dropout(attn_drop)</span><br><span class="line">        self.proj = nn.Linear(dim, dim)</span><br><span class="line">        self.proj_drop = nn.Dropout(proj_drop)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, N, C = x.shape</span><br><span class="line">        qkv = self.qkv(x).reshape(B, N, <span class="number">3</span>, self.num_heads, C // self.num_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        q, k, v = qkv.unbind(<span class="number">0</span>)   <span class="comment"># make torchscript happy (cannot use tensor as tuple)</span></span><br><span class="line"></span><br><span class="line">        attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) * self.scale<span class="comment"># QK</span></span><br><span class="line">        attn = attn.softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        attn = self.attn_drop(attn)</span><br><span class="line"></span><br><span class="line">        x = (attn @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(B, N, C)<span class="comment"># 乘以权重</span></span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        x = self.proj_drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h3 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Mlp</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; MLP as used in Vision Transformer, MLP-Mixer and related networks</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_features, hidden_features=<span class="literal">None</span>, out_features=<span class="literal">None</span>, act_layer=nn.GELU, bias=<span class="literal">True</span>, drop=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        out_features = out_features <span class="keyword">or</span> in_features</span><br><span class="line">        hidden_features = hidden_features <span class="keyword">or</span> in_features</span><br><span class="line">        bias = to_2tuple(bias)</span><br><span class="line">        drop_probs = to_2tuple(drop)</span><br><span class="line"></span><br><span class="line">        self.fc1 = nn.Linear(in_features, hidden_features, bias=bias[<span class="number">0</span>])</span><br><span class="line">        self.act = act_layer()</span><br><span class="line">        self.drop1 = nn.Dropout(drop_probs[<span class="number">0</span>])</span><br><span class="line">        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias[<span class="number">1</span>])</span><br><span class="line">        self.drop2 = nn.Dropout(drop_probs[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.act(x)</span><br><span class="line">        x = self.drop1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.drop2(x)</span><br><span class="line">        <span class="keyword">return</span> </span><br></pre></td></tr></table></figure>

<h3 id="Block"><a href="#Block" class="headerlink" title="Block"></a>Block</h3><p>实现功能：ViT的每个Block包括一层Attention和一层MLP。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Block</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">            self,</span></span><br><span class="line"><span class="params">            dim,</span></span><br><span class="line"><span class="params">            num_heads,</span></span><br><span class="line"><span class="params">            mlp_ratio=<span class="number">4.</span>,</span></span><br><span class="line"><span class="params">            qkv_bias=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">            drop=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">            attn_drop=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">            init_values=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">            drop_path=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">            act_layer=nn.GELU,</span></span><br><span class="line"><span class="params">            norm_layer=nn.LayerNorm</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.norm1 = norm_layer(dim)</span><br><span class="line">        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)</span><br><span class="line">        self.ls1 = LayerScale(dim, init_values=init_values) <span class="keyword">if</span> init_values <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        <span class="comment"># <span class="doctag">NOTE:</span> drop path for stochastic depth, we shall see if this is better than dropout here</span></span><br><span class="line">        self.drop_path1 = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">        self.norm2 = norm_layer(dim)</span><br><span class="line">        self.mlp = Mlp(in_features=dim, hidden_features=<span class="built_in">int</span>(dim * mlp_ratio), act_layer=act_layer, drop=drop)</span><br><span class="line">        self.ls2 = LayerScale(dim, init_values=init_values) <span class="keyword">if</span> init_values <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        self.drop_path2 = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))</span><br><span class="line">        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>







]]></content>
      <tags>
        <tag>经典模型</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：IDPT, Interconnected Dual Pyramid Transformer for Face Super-Resolution</title>
    <url>/2022/08/14/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AIDPT/</url>
    <content><![CDATA[<ul>
<li><p>研究问题：人脸超分辨率技术</p>
</li>
<li><p>FSR：关注于恢复重要的面部结构</p>
</li>
<li><p>创新点：提出了一个新的、有效的基于Transformer的人脸超分辨率架构</p>
<ol>
<li>设计了金字塔结构的encode&#x2F;decoder的Transformer架构：分别提取粗糙纹理和精细纹理。</li>
<li>通过一个底部的金字塔特征提取器，将双重金字塔Transformer建立起联系。</li>
<li>在每个spatial layer插入一个新的融合调制模块：使用粗糙纹理完善对应的精细纹理，融合浅层的粗糙纹理和对应的深层的精细纹理。</li>
</ol>
</li>
<li><p>FSR研究现状</p>
<ol>
<li>现有技术在解决超低分辨率问题上表现很差</li>
<li>卷积难以描述不同域间的关联和捕捉远域间的依赖</li>
</ol>
</li>
<li><p>网络结构： </p>
<img src="/2022/08/14/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AIDPT/1.png" alt="image-20220821153651772" style="zoom:80%;"></li>
</ul>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>FSR</tag>
        <tag>组内文章</tag>
      </tags>
  </entry>
  <entry>
    <title>经典模型：Swin Transformer</title>
    <url>/2022/09/26/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9ASwin-Transformer/</url>
    <content><![CDATA[<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>Swin（即Shifted Windows） Transformer可以作为CV的一种通用主干，用在分类、检测、语义分割等多种视觉任务上。</p>
<p>Swin Transformer的提出解决了ViT具有的以下两个问题：</p>
<ol>
<li>ViT中，由于每个token的size大小相同，难以捕捉<strong>多尺度</strong>信息。</li>
<li>ViT的自注意力<strong>计算复杂度</strong>是图像大小的二次方。</li>
</ol>
<p>Swin-T构造了层次化特征图，并将自注意力的计算复杂度降为线性相关。</p>
<h3 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h3><p>Swin-T的整体架构如下图所示：</p>
<img src="https://img-blog.csdnimg.cn/20210908164930810.png" alt="img" style="zoom:100%; margin:auto;">

<h3 id="Patch-Partition"><a href="#Patch-Partition" class="headerlink" title="Patch Partition"></a>Patch Partition</h3><p>对于每个为$H \times W \times 3$的输入，划分为$4 \times 4 \times 3$大小的patch，每张图像被拆分为个patches，将每个patch展平作为一个token。</p>
<h3 id="Linear-Embedding"><a href="#Linear-Embedding" class="headerlink" title="Linear Embedding"></a>Linear Embedding</h3><p>即一个全连接层，将每个大小为48的token映射到设定的维度C，此时，每张图片的输入变为了$\frac{H}{4} \times \frac{W}{4} \times C$，然后输入Swin Transformer Block。</p>
<h3 id="Swin-Transformer-Block"><a href="#Swin-Transformer-Block" class="headerlink" title="Swin Transformer Block"></a>Swin Transformer Block</h3><p>对于Transformers中使用的全局自注意力机制，需要计算每个token与其它所有tokens间的关系，计算复杂度为<strong>token数的平方</strong>。不适用于对大量tokens进行密集预测或表示高分辨率图像等视觉问题。</p>
<h4 id="W-MSA"><a href="#W-MSA" class="headerlink" title="W-MSA"></a>W-MSA</h4><p>Swin-T通过<strong>在局部窗口中计算自注意力</strong>，将计算复杂度降低为token数的线性关系，设每个非重叠局部窗口中包含$M \times M$个tokens。</p>
<ul>
<li>MSA：有$hw$个tokens，每个token在全局计算$hw$次；</li>
<li>W-MSA：有$hw$个tokens，每个token在全局计算$M^2$次。</li>
</ul>
<h4 id="SW-MSA"><a href="#SW-MSA" class="headerlink" title="SW-MSA"></a>SW-MSA</h4><p>W-MSA限制了跨窗口token间的交流与联系，从而限制了建模表征能力。作者提出了一种<strong>移位窗口划分</strong>方法SW-MSA，在模型中交替使用两种MSA方法（因此每个stage中Swin Transformer Block的数量都为偶数）。</p>
<p>所谓的移动窗口即将窗口循环位移，如下图所示：</p>
<img src="https://img-blog.csdnimg.cn/20210908164652285.png" alt="img" style="zoom:100%; margin:auto;">

<p>但直接移位得到的窗口大小是不规则的，不利于并行计算，同时9个窗口也提升了计算成本。为了解决这个问题，将重新划分后的窗口进行拼接，如下图所示，得到4个窗口。</p>
<img src="https://img-blog.csdnimg.cn/2021090920295156.png" alt="img" style="zoom:130%; margin:auto;">

<p>4个窗口中来自不同初始位置的patch不应进行自注意计算，因此使用mask机制，将不需要的注意力图置0。</p>
<h4 id="相对位置偏置"><a href="#相对位置偏置" class="headerlink" title="相对位置偏置"></a>相对位置偏置</h4><p>在计算自注意力时，在计算相似度的过程中对每个head加入相对位置偏置，如下所示：</p>
<p><img src="https://img-blog.csdnimg.cn/20210911231011149.png" alt="img"></p>
<ul>
<li>对于预训练中学到的相对位置偏置，可以通过双三次插值初始化具有不同窗口大小的微调模型。</li>
</ul>
<h3 id="Patch-Merging"><a href="#Patch-Merging" class="headerlink" title="Patch Merging"></a>Patch Merging</h3><p>Patch Merging层的功能是产生一个层次化表示，通过<strong>合并相邻的tokens</strong>，减少tokens的数目。</p>
<p>对于Stage1和Stage2间的Patch Merging层，将原维度为C的token合并为大小为4C的token，再使用一个线性层将输出维度降低为2C，token的数目降低为$\frac{H}{8} \times \frac{W}{8}$。</p>
<p>在之后的每个stage中，都会改变张量的维度，从而形成一种层次化的特征。</p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2>]]></content>
      <tags>
        <tag>经典模型</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Model Behavior Preserving for Class-Incremental Learning</title>
    <url>/2022/08/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AModel-Behavior-Preserving-for-Class-Incremental-Learning/</url>
    <content><![CDATA[<ul>
<li>发表时间：2022</li>
<li>研究内容：类增量学习，探讨在增量学习中应保留旧模型的哪些功能性属性。</li>
<li>研究现状：现有的增量学习方法忽略了CNN模型响应间的内部结构，KD的硬约束导致新模型出现混沌行为。</li>
<li>创新点：<ol>
<li><p>Feature Space：设计了一个INP Loss保持成对实例在旧模型上的<strong>相似性顺序</strong>（反映实例集间的相邻关系）；</p>
<p>INP用于惩罚新模型在学习过程中每个实例相邻关系的变化。</p>
<img src="/2022/08/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AModel-Behavior-Preserving-for-Class-Incremental-Learning/image-20220826093609961.png" alt="image-20220826093609961" style="zoom:67%;margin:auto;">

<p>a. 旧实例A在特征空间中与其它实例的相邻关系；</p>
<p>b. 采用传统的KD，引入新实例G后，绝对位置的微小变化被严格限制；</p>
<p>c. 采用INP Loss当相对位置没变时，就不会限制更新。</p>
</li>
<li><p>Label Space：设计了一个LPP Loss在输出空间的实例标签概率向量中保留<strong>标签排名列表</strong>（反映实例属于每一类的排名）；</p>
</li>
<li><p>介绍了一种可导的排名计算方法用于计算上述Loss。</p>
</li>
</ol>
</li>
</ul>
]]></content>
      <tags>
        <tag>组内文章</tag>
        <tag>连续学习</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：PhysFormer: Facial Video-based Physiological Measurement with Temporal Difference Transformer</title>
    <url>/2022/09/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9APhysFormer-Facial-Video-based-Physiological-Measurement-with-Temporal-Difference-Transformer/</url>
    <content><![CDATA[<ul>
<li><p>发表时间：2021</p>
</li>
<li><p>研究对象：rPPG，使用多波长 RGB 相机检测人体皮肤表面脉冲引起的细微颜色变化，实现测量心脏活动和其它生理信号。</p>
</li>
<li><p>研究意义：传统的检测方法会造成discomfort，并且长期检测不方便</p>
</li>
<li><p>Code：<a href="https://github.com/ZitongYu/PhysFormer">https://github.com/ZitongYu/PhysFormer</a></p>
</li>
<li><p>rPPG研究历史：</p>
<ol>
<li>早期使用经典的信号处理方法检测面部细微的颜色变化；</li>
<li>使用非端到端方法，首先生成预处理的信号特征，然后模型从这些特征图中捕捉rPPG特征（对预处理要求严格，忽略了全局特征）；</li>
<li>端到端的基于深度学习的方法（容易被复杂的背景信息影响）。</li>
</ol>
</li>
<li><p>研究现状：</p>
<p>​		现有的基于卷积神经网络的模型在时间和空间上的感受野受限，忽略了长期的时间和空间上的互动与感知。</p>
</li>
<li><p>PhysFormer网络架构：</p>
<img src="/2022/09/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9APhysFormer-Facial-Video-based-Physiological-Measurement-with-Temporal-Difference-Transformer/image-20220919092041506.png" alt="image-20220919092041506" style="zoom:80%;">

<ol>
<li><strong>Stem</strong>：提取粗糙的局部时空特征</li>
<li><strong>Tube Tokens</strong>：将stem输出划分为若干个时空tube token，将时空邻近语义聚合在一起，并减少后续transformer的计算量</li>
<li><strong>Temporal Difference Multi-head Self-attention</strong>：与传统的自注意力机制不同，使用TDC计算距离，可以捕捉局部细粒度的时间差异特征</li>
<li><strong>Spatio-temporal Feed-forward</strong>：</li>
</ol>
</li>
<li><p>创新点：PhysFormer，一种端到端的视频transformer，联合使用了局部的和全局的时空特征。</p>
<ol>
<li>使用<strong>时差引导全局注意力机制</strong>，强化rPPG的周期性特征，针对干扰完善局部时空特征；</li>
<li>使用受label distribution learning和curriculum learning启发的频域动态约束，为PhysFormer提供详细的监督，缓解过拟合。</li>
<li>PhysFormer不需要像其它transformer网络那样在大规模数据集上预训练，仅在rPPG数据集训练即可。</li>
</ol>
</li>
<li><p><strong>Label Distribution Learning</strong>：对于面部的rPPG信号，心率相近的视频会有相似的周期性特征。为了使得模型学习到这种特征，将心率估计问题看作一个多分类问题，有多少个心率就有多少类别，类别概率向量由高斯分布组成。</p>
</li>
<li><p><strong>Curriculum Learning Guided Dynamic Loss</strong>：课程式学习是指模型从容易样本开始学习，逐步学习困难样本。在该任务中，从时域和频域两个方面限制模型学习，时域的限制更直接更容易学习，频域的限制较难学习，因此，使用动态的loss函数，逐步提高频域loss的比例。</p>
</li>
<li><p>实验：不同模型的对比实验、消融实验</p>
</li>
<li><p>Others：注意力图可视化</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>Transformer</tag>
        <tag>组内文章</tag>
        <tag>PPG</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Rethinking the ST-GCNs for 3D skeleton-based human action recognition</title>
    <url>/2022/09/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARethinking-the-ST-GCNs-for-3D-skeleton-based-human-action-recognition/</url>
    <content><![CDATA[<ul>
<li>发表时间：2021</li>
<li>ST-GCN：Spatial-Temporal Graph Convolutional Network，用于解决骨骼数据的动作识别问题。</li>
<li>研究内容：<ol>
<li>证明了在ST-GCN中很多操作对于人体动作识别是没必要的</li>
<li>提出了一个简单有效的策略捕捉全局图的相关性，对输入序列进行有效建模，同时将输入图序列降入欧几里得空间，可以使用多尺度时域滤波器捕捉动态信息。</li>
</ol>
</li>
<li>研究现状：<ol>
<li>骨骼数据成为人体动作识别的主流输入（与传统的RGB视频数据相比，信息更完整）</li>
<li>直接将结构化的数据重新排列，使得tensor适应基础的神经网络（由于骨骼数据中没有天然的局部性概念，深度学习的能力受到限制）</li>
<li>设计一种适应结构化数据的自定义神经网络（ST-GCN）</li>
</ol>
</li>
</ul>
<p>TBC：GCN好难，看不懂</p>
]]></content>
      <tags>
        <tag>组内文章</tag>
        <tag>人体动作识别</tag>
        <tag>GCN</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Structural Knowledge Organization and Transfer for Class-Incremental Learning</title>
    <url>/2022/08/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AStructural-Knowledge-Organization-and-Transfer-for-Class-Incremental-Learning/</url>
    <content><![CDATA[<ul>
<li><p>发表时间：2021</p>
</li>
<li><p>研究问题：类增量学习</p>
</li>
<li><p>研究现状：</p>
<ol>
<li><p><strong>经典的知识蒸馏方法</strong>忽略了信息点之间的关联，当新数据远多于旧数据时，面临着严重的偏差问题；</p>
<ul>
<li><p>KD：在特征空间中，孤立地限制单个训练样本的位置，样本间的关系可能会被改变，并导致分类错误。</p>
</li>
<li><p>SGKD：保持样本的结构化知识，包括样本的位置和样本间关系，确保蒸馏后样本仍能被正确地分类。</p>
</li>
</ul>
<img src="/2022/08/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AStructural-Knowledge-Organization-and-Transfer-for-Class-Incremental-Learning/image-20220822101131518.png" alt="image-20220822101131518" style="zoom:67%;margin:auto;"></li>
</ol>
</li>
<li><p>创新点：</p>
<ol>
<li><p>使用一个<strong>memory knowledge graph</strong>(MKG)表征历史任务的结构化知识</p>
<ul>
<li>在特征空间中的绝对位置（MKG中用顶点表示已知example间的特征向量）</li>
<li>example间对应关系（边表示，使用余弦距离）</li>
</ul>
</li>
<li><p>使用<strong>图插值机制</strong>丰富知识域、缓解类间样本不平衡问题</p>
<p>通过向MKG中插入假的顶点，扩充和平滑分散的数据集，假顶点通过mix两个真顶点的vector得到。</p>
</li>
<li><p>使用<strong>结构化图知识蒸馏（SGKD）</strong>迁移旧知识</p>
<ul>
<li>顶点蒸馏损失</li>
<li>边蒸馏损失</li>
</ul>
<img src="/2022/08/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AStructural-Knowledge-Organization-and-Transfer-for-Class-Incremental-Learning/image-20220823094630810.png" alt="image-20220823094630810" style="zoom:67%;margin:auto;"></li>
</ol>
</li>
<li><p>人脸识别是怎么实现增加新样本的？</p>
<p>人脸识别网络的本质是一个特征提取器，并不是分类器，识别人脸的时候，通过计算输出特征和人脸库中人脸的距离判断人脸所属对象。未涉及类增量学习。</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>组内文章</tag>
        <tag>连续学习</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Revisiting Pixel-Wise Supervision for Face Anti-Spoofing</title>
    <url>/2022/09/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARevisiting-Pixel-Wise-Supervision-for-Face-Anti-Spoofing/</url>
    <content><![CDATA[<ul>
<li><p>发表时间：2021</p>
</li>
<li><p>研究内容：像素级的人脸识别反欺诈方法</p>
</li>
<li><p>创新点：提出基于<strong>金字塔</strong>的监督方法，模型从多空间尺度上学习局部和全局的语义信息</p>
</li>
<li><p>Presentation Attack Detection研究历史：</p>
<ol>
<li><p>传统算法关注于<strong>活体</strong>和<strong>手工特征</strong>的检测，需要丰富的任务级的先验知识。</p>
<p>活体检测：关注眨眼、面部和头部动作、视线追踪以及远程生理信号（这种方法需要长期的互动，容易被video attacks伪造）。</p>
<p>经典的handcrafted descriptors：从多种色彩空间中提取有效的欺诈模式，这种PA方法可以通过训练分类器捕捉，但在遇到未见过的场景或未知的PAs时就失效了。</p>
</li>
<li></li>
<li></li>
</ol>
</li>
<li></li>
</ul>
]]></content>
      <tags>
        <tag>组内文章</tag>
        <tag>人脸识别</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Social Distancing Alert with Smartwatches</title>
    <url>/2022/09/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ASocial-Distancing-Alert-with-Smartwatches/</url>
    <content><![CDATA[<ul>
<li><p>发表时间：2022</p>
</li>
<li><p>研究内容：基于智能手表的社交距离警报系统SoDA，SoDA使用加速器和陀螺仪的数据和简单有效的视觉Transformer模型，识别违反社交距离的活动。</p>
</li>
<li><p>code：<a href="https://github.com/aiotgroup/SoDA">https://github.com/aiotgroup/SoDA</a></p>
</li>
<li><p>创新点：</p>
<ol>
<li>应用价值</li>
<li>创建了一个数据集</li>
<li>证明了ViT是一种有效的方法？</li>
</ol>
</li>
<li><p>模型结构：</p>
<img src="/2022/09/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ASocial-Distancing-Alert-with-Smartwatches/image-20220920104544756.png" alt="image-20220920104544756" style="zoom:80%;"></li>
</ul>
]]></content>
      <tags>
        <tag>Transformer</tag>
        <tag>组内文章</tag>
      </tags>
  </entry>
</search>
