<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>HelloWorld!</title>
    <url>/2020/08/29/HelloWorld/</url>
    <content><![CDATA[<h2 id="A-new-world"><a href="#A-new-world" class="headerlink" title="A new world!"></a>A new world!</h2><p>哈喽！历经一个下午博客终于搭建好了，原来是那么容易的一件事情，大一的时候想的很复杂，迟迟没能动手，现在也终于有了自己的小博客啦~</p>
<p>未来灌水的文章还是会首先发在CSDN上，这里会分享一些重大的经历~已经大三了！要更努力学习！不要被些奇奇怪怪的事情干扰，奥里给！</p>
]]></content>
  </entry>
  <entry>
    <title>经典模型：Vision Transformer</title>
    <url>/2022/09/26/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AVision-Transformer/</url>
    <content><![CDATA[<h2 id="模型介绍"><a href="#模型介绍" class="headerlink" title="模型介绍"></a>模型介绍</h2><ul>
<li>可参考博客：<a href="https://blog.csdn.net/qq_39478403/article/details/118704747">https://blog.csdn.net/qq_39478403/article/details/118704747</a></li>
</ul>
<h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>ViT主要使用Transformer的encoder部分</p>
<img src="/2022/09/26/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AVision-Transformer/image-20220926083713750.png" alt="image-20220926083713750" style="zoom: 67%; margin:auto;">

<ul>
<li>将一张图像分成若干个大小固定且相同的patch，将每个patch投影到线性空间中再加上位置编码</li>
<li>除了每个patch作为一个token外，在序列中添加一个额外的classification token</li>
</ul>
<p><strong>位置编码：（看下代码咋实现的）</strong></p>
<ul>
<li>使用可学习的一维位置编码（作者发现使用更高维的位置编码并没有带来显著的精度提升）</li>
</ul>
<p><strong>混合结构：</strong></p>
<ul>
<li>可以将原始图像使用CNN进行特征提取，将特征图按patch划分送入Transformer中。</li>
</ul>
<h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><p><strong>ViT使用：</strong></p>
<ul>
<li>ViT与Bert类似，先在大数据集上<strong>训练</strong>，再在downstream任务上<strong>微调</strong>。在微调时，将预训练用的预测头换成一个用0初始化的DxK的前馈层，其中K代表下游任务总的类别数。</li>
<li>在微调时，使用更高分辨率的图像可以获得更好的结果。（patch大小不变，输入序列变成，不影响网络结构），这样会导致之前训练得到的<strong>位置编码</strong>无意义，因此在原来的位置编码上进行一个2D的插值。</li>
</ul>
<p><strong>实验结果：</strong></p>
<ul>
<li>当<strong>考虑预训练的训练代价</strong>时，ViT以更低的代价达到了SOTA水平</li>
<li>在<strong>自监督</strong>问题上，ViT很有应用前景</li>
</ul>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><ul>
<li>Official Code：<a href="https://github.com/google-research/vision_transformer">https://github.com/google-research/vision_transformer</a></li>
<li>timm code：<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py">https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py</a></li>
</ul>
<h3 id="Patch-Embeddings"><a href="#Patch-Embeddings" class="headerlink" title="Patch Embeddings"></a>Patch Embeddings</h3><p>实现功能：将输入图像划分为若干个patch，并将每个patch拉平投影到D维。通过一个二维的卷积操作即可实现。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/patch_embed.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 2D Image to Patch Embedding</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">            self,</span></span><br><span class="line"><span class="params">            img_size=<span class="number">224</span>,</span></span><br><span class="line"><span class="params">            patch_size=<span class="number">16</span>,<span class="comment"># 每个patch大小为16x16x3</span></span></span><br><span class="line"><span class="params">            in_chans=<span class="number">3</span>,</span></span><br><span class="line"><span class="params">            embed_dim=<span class="number">768</span>,<span class="comment"># 将patch映射到768维</span></span></span><br><span class="line"><span class="params">            norm_layer=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">            flatten=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">            bias=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        img_size = to_2tuple(img_size)</span><br><span class="line">        patch_size = to_2tuple(patch_size)</span><br><span class="line">        self.img_size = img_size</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.grid_size = (img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>], img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>])</span><br><span class="line">        self.num_patches = self.grid_size[<span class="number">0</span>] * self.grid_size[<span class="number">1</span>]<span class="comment"># 每张图像对应patch个数</span></span><br><span class="line">        self.flatten = flatten</span><br><span class="line">        </span><br><span class="line">		<span class="comment"># 通过一步卷积操作实现嵌入</span></span><br><span class="line">        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias)</span><br><span class="line">        self.norm = norm_layer(embed_dim) <span class="keyword">if</span> norm_layer <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, C, H, W = x.shape</span><br><span class="line">        _<span class="keyword">assert</span>(H == self.img_size[<span class="number">0</span>], <span class="string">f&quot;Input image height (<span class="subst">&#123;H&#125;</span>) doesn&#x27;t match model (<span class="subst">&#123;self.img_size[<span class="number">0</span>]&#125;</span>).&quot;</span>)</span><br><span class="line">        _<span class="keyword">assert</span>(W == self.img_size[<span class="number">1</span>], <span class="string">f&quot;Input image width (<span class="subst">&#123;W&#125;</span>) doesn&#x27;t match model (<span class="subst">&#123;self.img_size[<span class="number">1</span>]&#125;</span>).&quot;</span>)</span><br><span class="line">        x = self.proj(x)<span class="comment"># 投影</span></span><br><span class="line">        <span class="keyword">if</span> self.flatten:</span><br><span class="line">            x = x.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># BCHW -&gt; BNC</span></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h3 id="可学习的嵌入"><a href="#可学习的嵌入" class="headerlink" title="可学习的嵌入"></a>可学习的嵌入</h3><ul>
<li>cls_token：为了与Bert保持一致，设置可学习的嵌入向量作为用于分类的类别向量。</li>
<li>pos_embed：由于自注意力机制具有<strong>扰动不变性</strong>（打乱tokens中的顺序并不会改变结果），因此需要位置编码标识位置信息。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cls_token</span></span><br><span class="line">self.cls_token = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, embed_dim)) <span class="keyword">if</span> class_token <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"><span class="comment"># pos_embed</span></span><br><span class="line">embed_len = num_patches <span class="keyword">if</span> no_embed_class <span class="keyword">else</span> num_patches + self.num_prefix_tokens</span><br><span class="line">self.pos_embed = nn.Parameter(torch.randn(<span class="number">1</span>, embed_len, embed_dim) * <span class="number">.02</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_pos_embed</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="keyword">if</span> self.no_embed_class:</span><br><span class="line">        <span class="comment"># 先加位置编码再拼接cls_token</span></span><br><span class="line">        <span class="comment"># position embedding does not overlap with class token, add then concat</span></span><br><span class="line">        x = x + self.pos_embed</span><br><span class="line">        <span class="keyword">if</span> self.cls_token <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = torch.cat((self.cls_token.expand(x.shape[<span class="number">0</span>], -<span class="number">1</span>, -<span class="number">1</span>), x), dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 先拼接cls_token再加位置编码</span></span><br><span class="line">        <span class="comment"># pos_embed has entry for class token, concat then add</span></span><br><span class="line">        <span class="keyword">if</span> self.cls_token <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = torch.cat((self.cls_token.expand(x.shape[<span class="number">0</span>], -<span class="number">1</span>, -<span class="number">1</span>), x), dim=<span class="number">1</span>)</span><br><span class="line">        x = x + self.pos_embed</span><br><span class="line">    <span class="keyword">return</span> self.pos_drop(x)</span><br></pre></td></tr></table></figure>

<h3 id="多头注意力机制"><a href="#多头注意力机制" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, num_heads=<span class="number">8</span>, qkv_bias=<span class="literal">False</span>, attn_drop=<span class="number">0.</span>, proj_drop=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">assert</span> dim % num_heads == <span class="number">0</span>, <span class="string">&#x27;dim should be divisible by num_heads&#x27;</span></span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        head_dim = dim // num_heads</span><br><span class="line">        self.scale = head_dim ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        self.qkv = nn.Linear(dim, dim * <span class="number">3</span>, bias=qkv_bias)</span><br><span class="line">        self.attn_drop = nn.Dropout(attn_drop)</span><br><span class="line">        self.proj = nn.Linear(dim, dim)</span><br><span class="line">        self.proj_drop = nn.Dropout(proj_drop)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, N, C = x.shape</span><br><span class="line">        qkv = self.qkv(x).reshape(B, N, <span class="number">3</span>, self.num_heads, C // self.num_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        q, k, v = qkv.unbind(<span class="number">0</span>)   <span class="comment"># make torchscript happy (cannot use tensor as tuple)</span></span><br><span class="line"></span><br><span class="line">        attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) * self.scale<span class="comment"># QK</span></span><br><span class="line">        attn = attn.softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        attn = self.attn_drop(attn)</span><br><span class="line"></span><br><span class="line">        x = (attn @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(B, N, C)<span class="comment"># 乘以权重</span></span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        x = self.proj_drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h3 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Mlp</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; MLP as used in Vision Transformer, MLP-Mixer and related networks</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_features, hidden_features=<span class="literal">None</span>, out_features=<span class="literal">None</span>, act_layer=nn.GELU, bias=<span class="literal">True</span>, drop=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        out_features = out_features <span class="keyword">or</span> in_features</span><br><span class="line">        hidden_features = hidden_features <span class="keyword">or</span> in_features</span><br><span class="line">        bias = to_2tuple(bias)</span><br><span class="line">        drop_probs = to_2tuple(drop)</span><br><span class="line"></span><br><span class="line">        self.fc1 = nn.Linear(in_features, hidden_features, bias=bias[<span class="number">0</span>])</span><br><span class="line">        self.act = act_layer()</span><br><span class="line">        self.drop1 = nn.Dropout(drop_probs[<span class="number">0</span>])</span><br><span class="line">        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias[<span class="number">1</span>])</span><br><span class="line">        self.drop2 = nn.Dropout(drop_probs[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.act(x)</span><br><span class="line">        x = self.drop1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.drop2(x)</span><br><span class="line">        <span class="keyword">return</span> </span><br></pre></td></tr></table></figure>

<h3 id="Block"><a href="#Block" class="headerlink" title="Block"></a>Block</h3><p>实现功能：ViT的每个Block包括一层Attention和一层MLP。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Block</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">            self,</span></span><br><span class="line"><span class="params">            dim,</span></span><br><span class="line"><span class="params">            num_heads,</span></span><br><span class="line"><span class="params">            mlp_ratio=<span class="number">4.</span>,</span></span><br><span class="line"><span class="params">            qkv_bias=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">            drop=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">            attn_drop=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">            init_values=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">            drop_path=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">            act_layer=nn.GELU,</span></span><br><span class="line"><span class="params">            norm_layer=nn.LayerNorm</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.norm1 = norm_layer(dim)</span><br><span class="line">        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)</span><br><span class="line">        self.ls1 = LayerScale(dim, init_values=init_values) <span class="keyword">if</span> init_values <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        <span class="comment"># <span class="doctag">NOTE:</span> drop path for stochastic depth, we shall see if this is better than dropout here</span></span><br><span class="line">        self.drop_path1 = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">        self.norm2 = norm_layer(dim)</span><br><span class="line">        self.mlp = Mlp(in_features=dim, hidden_features=<span class="built_in">int</span>(dim * mlp_ratio), act_layer=act_layer, drop=drop)</span><br><span class="line">        self.ls2 = LayerScale(dim, init_values=init_values) <span class="keyword">if</span> init_values <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        self.drop_path2 = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))</span><br><span class="line">        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>







]]></content>
      <tags>
        <tag>经典模型</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：IDPT, Interconnected Dual Pyramid Transformer for Face Super-Resolution</title>
    <url>/2022/08/14/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AIDPT/</url>
    <content><![CDATA[<ul>
<li><p>研究问题：人脸超分辨率技术</p>
</li>
<li><p>FSR：关注于恢复重要的面部结构</p>
</li>
<li><p>创新点：提出了一个新的、有效的基于Transformer的人脸超分辨率架构</p>
<ol>
<li>设计了金字塔结构的encode&#x2F;decoder的Transformer架构：分别提取粗糙纹理和精细纹理。</li>
<li>通过一个底部的金字塔特征提取器，将双重金字塔Transformer建立起联系。</li>
<li>在每个spatial layer插入一个新的融合调制模块：使用粗糙纹理完善对应的精细纹理，融合浅层的粗糙纹理和对应的深层的精细纹理。</li>
</ol>
</li>
<li><p>FSR研究现状</p>
<ol>
<li>现有技术在解决超低分辨率问题上表现很差</li>
<li>卷积难以描述不同域间的关联和捕捉远域间的依赖</li>
</ol>
</li>
<li><p>网络结构： </p>
<img src="/2022/08/14/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AIDPT/1.png" alt="image-20220821153651772" style="zoom:80%;"></li>
</ul>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>FSR</tag>
        <tag>组内文章</tag>
      </tags>
  </entry>
  <entry>
    <title>经典模型：Swin Transformer</title>
    <url>/2022/09/26/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9ASwin-Transformer/</url>
    <content><![CDATA[<h2 id="模型结构与代码实现">模型结构与代码实现</h2>
<ul>
<li>参考博客：<a href="https://blog.csdn.net/qq_39478403/article/details/120042232?spm=1001.2014.3001.5506">https://blog.csdn.net/qq_39478403/article/details/120042232?spm=1001.2014.3001.5506</a></li>
<li>code：<a href="https://github.com/microsoft/Swin-Transformer">https://github.com/microsoft/Swin-Transformer</a></li>
</ul>
<p>Swin（即Shifted Windows） Transformer可以作为CV的一种通用主干，用在分类、检测、语义分割等多种视觉任务上。</p>
<p>Swin Transformer的提出解决了ViT具有的以下两个问题：</p>
<ol>
<li>ViT中，由于每个token的size大小相同，难以捕捉<strong>多尺度</strong>信息。</li>
<li>ViT的自注意力<strong>计算复杂度</strong>是图像大小的二次方。</li>
</ol>
<p>Swin-T构造了层次化特征图，并将自注意力的计算复杂度降为线性相关。</p>
<h3 id="整体结构">整体结构</h3>
<p>Swin-T的整体架构如下图所示：</p>
<img src="https://img-blog.csdnimg.cn/20210908164930810.png" alt="img" style="zoom:100%; margin:auto;">
<h3 id="Patch-Partition">Patch Partition</h3>
<p>对于每个为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>×</mo><mi>W</mi><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">H \times W \times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span>的输入，划分为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>×</mo><mn>4</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">4 \times 4 \times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span>大小的patch，每张图像被拆分为个patches，将每个patch展平作为一个token。</p>
<h3 id="Linear-Embedding">Linear Embedding</h3>
<p>即一个全连接层，将每个大小为48的token映射到设定的维度C，此时，每张图片的输入变为了<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>H</mi><mn>4</mn></mfrac><mo>×</mo><mfrac><mi>W</mi><mn>4</mn></mfrac><mo>×</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">\frac{H}{4} \times \frac{W}{4} \times C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>，然后输入Swin Transformer Block。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Patch Partition + Linear Embedding</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Image to Patch Embedding</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        img_size (int): Image size.  Default: 224.</span></span><br><span class="line"><span class="string">        patch_size (int): Patch token size. Default: 4.</span></span><br><span class="line"><span class="string">        in_chans (int): Number of input image channels. Default: 3.</span></span><br><span class="line"><span class="string">        embed_dim (int): Number of linear projection output channels. Default: 96.</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module, optional): Normalization layer. Default: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">4</span>, in_chans=<span class="number">3</span>, embed_dim=<span class="number">96</span>, norm_layer=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        img_size = to_2tuple(img_size)</span><br><span class="line">        patch_size = to_2tuple(patch_size)</span><br><span class="line">        patches_resolution = [img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>], img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>]]</span><br><span class="line">        self.img_size = img_size</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.patches_resolution = patches_resolution</span><br><span class="line">        self.num_patches = patches_resolution[<span class="number">0</span>] * patches_resolution[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        self.in_chans = in_chans</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">		<span class="comment"># 通过一个2维卷积实现patch partition与linear embedding</span></span><br><span class="line">        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br><span class="line">        <span class="keyword">if</span> norm_layer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.norm = norm_layer(embed_dim)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.norm = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, C, H, W = x.shape</span><br><span class="line">        <span class="comment"># FIXME look at relaxing size constraints</span></span><br><span class="line">        <span class="keyword">assert</span> H == self.img_size[<span class="number">0</span>] <span class="keyword">and</span> W == self.img_size[<span class="number">1</span>], \</span><br><span class="line">            <span class="string">f&quot;Input image size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) doesn&#x27;t match model (<span class="subst">&#123;self.img_size[<span class="number">0</span>]&#125;</span>*<span class="subst">&#123;self.img_size[<span class="number">1</span>]&#125;</span>).&quot;</span></span><br><span class="line">        x = self.proj(x).flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># 先划分patch与投影，再展平与交换</span></span><br><span class="line">        <span class="keyword">if</span> self.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = self.norm(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="Swin-Transformer-Block">Swin Transformer Block</h3>
<p>对于Transformers中使用的全局自注意力机制，需要计算每个token与其它所有tokens间的关系，计算复杂度为<strong>token数的平方</strong>。不适用于对大量tokens进行密集预测或表示高分辨率图像等视觉问题。</p>
<h4 id="W-MSA">W-MSA</h4>
<p>Swin-T通过<strong>在局部窗口中计算自注意力</strong>，将计算复杂度降低为token数的线性关系，设每个非重叠局部窗口中包含<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>×</mo><mi>M</mi></mrow><annotation encoding="application/x-tex">M\times M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span>个tokens。</p>
<ul>
<li>MSA：有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">hw</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">h</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span>个tokens，每个token在全局计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">hw</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">h</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span>次；</li>
<li>W-MSA：有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">hw</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">h</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span>个tokens，每个token在全局计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>M</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">M^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>次。</li>
</ul>
<p>首先将输入划分为若干个大小为MxM的窗口。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">window_partition</span>(<span class="params">x, window_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        x: (B, H, W, C)</span></span><br><span class="line"><span class="string">        window_size (int): window size</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        windows: (num_windows*B, window_size, window_size, C)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    B, H, W, C = x.shape</span><br><span class="line">    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)</span><br><span class="line">    windows = x.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>).contiguous().view(-<span class="number">1</span>, window_size, window_size, C)</span><br><span class="line">    <span class="keyword">return</span> windows</span><br></pre></td></tr></table></figure>
<h4 id="SW-MSA">SW-MSA</h4>
<p>W-MSA限制了跨窗口token间的交流与联系，从而限制了建模表征能力。作者提出了一种<strong>移位窗口划分</strong>方法SW-MSA，在模型中交替使用两种MSA方法（因此每个stage中Swin Transformer Block的数量都为偶数）。</p>
<p>所谓的移动窗口即将窗口循环位移，如下图所示：</p>
<img src="https://img-blog.csdnimg.cn/20210908164652285.png" alt="img" style="zoom:100%; margin:auto;">
<p>但直接移位得到的窗口大小是不规则的，不利于并行计算，同时9个窗口也提升了计算成本。为了解决这个问题，将重新划分后的窗口进行拼接，如下图所示，得到4个窗口。</p>
<img src="https://img-blog.csdnimg.cn/2021090920295156.png" alt="img" style="zoom:130%; margin:auto;">
<p>4个窗口中来自不同初始位置的patch不应进行自注意计算，因此使用mask机制，将不需要的注意力图置0。</p>
<p>W-MSA和SW-MSA公用一块代码实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">WindowAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Window based multi-head self attention (W-MSA) module with relative position bias.</span></span><br><span class="line"><span class="string">    It supports both of shifted and non-shifted window.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        window_size (tuple[int]): The height and width of the window.</span></span><br><span class="line"><span class="string">        num_heads (int): Number of attention heads.</span></span><br><span class="line"><span class="string">        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True</span></span><br><span class="line"><span class="string">        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set</span></span><br><span class="line"><span class="string">        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0</span></span><br><span class="line"><span class="string">        proj_drop (float, optional): Dropout ratio of output. Default: 0.0</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, window_size, num_heads, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">None</span>, attn_drop=<span class="number">0.</span>, proj_drop=<span class="number">0.</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.window_size = window_size  <span class="comment"># Wh, Ww</span></span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        head_dim = dim // num_heads</span><br><span class="line">        self.scale = qk_scale <span class="keyword">or</span> head_dim ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># define a parameter table of relative position bias</span></span><br><span class="line">        self.relative_position_bias_table = nn.Parameter(</span><br><span class="line">            torch.zeros((<span class="number">2</span> * window_size[<span class="number">0</span>] - <span class="number">1</span>) * (<span class="number">2</span> * window_size[<span class="number">1</span>] - <span class="number">1</span>), num_heads))  <span class="comment"># 2*Wh-1 * 2*Ww-1, nH</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># get pair-wise relative position index for each token inside the window</span></span><br><span class="line">        coords_h = torch.arange(self.window_size[<span class="number">0</span>])</span><br><span class="line">        coords_w = torch.arange(self.window_size[<span class="number">1</span>])</span><br><span class="line">        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  <span class="comment"># 2, Wh, Ww</span></span><br><span class="line">        coords_flatten = torch.flatten(coords, <span class="number">1</span>)  <span class="comment"># 2, Wh*Ww</span></span><br><span class="line">        relative_coords = coords_flatten[:, :, <span class="literal">None</span>] - coords_flatten[:, <span class="literal">None</span>, :]  <span class="comment"># 2, Wh*Ww, Wh*Ww</span></span><br><span class="line">        relative_coords = relative_coords.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).contiguous()  <span class="comment"># Wh*Ww, Wh*Ww, 2</span></span><br><span class="line">        relative_coords[:, :, <span class="number">0</span>] += self.window_size[<span class="number">0</span>] - <span class="number">1</span>  <span class="comment"># shift to start from 0</span></span><br><span class="line">        relative_coords[:, :, <span class="number">1</span>] += self.window_size[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">        relative_coords[:, :, <span class="number">0</span>] *= <span class="number">2</span> * self.window_size[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">        relative_position_index = relative_coords.<span class="built_in">sum</span>(-<span class="number">1</span>)  <span class="comment"># Wh*Ww, Wh*Ww</span></span><br><span class="line">        self.register_buffer(<span class="string">&quot;relative_position_index&quot;</span>, relative_position_index)</span><br><span class="line"></span><br><span class="line">        self.qkv = nn.Linear(dim, dim * <span class="number">3</span>, bias=qkv_bias)</span><br><span class="line">        self.attn_drop = nn.Dropout(attn_drop)</span><br><span class="line">        self.proj = nn.Linear(dim, dim)</span><br><span class="line">        self.proj_drop = nn.Dropout(proj_drop)</span><br><span class="line"></span><br><span class="line">        trunc_normal_(self.relative_position_bias_table, std=<span class="number">.02</span>)</span><br><span class="line">        self.softmax = nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: input features with shape of (num_windows*B, N, C)</span></span><br><span class="line"><span class="string">            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        B_, N, C = x.shape</span><br><span class="line">        qkv = self.qkv(x).reshape(B_, N, <span class="number">3</span>, self.num_heads, C // self.num_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        q, k, v = qkv[<span class="number">0</span>], qkv[<span class="number">1</span>], qkv[<span class="number">2</span>]  <span class="comment"># make torchscript happy (cannot use tensor as tuple)</span></span><br><span class="line"></span><br><span class="line">        q = q * self.scale</span><br><span class="line">        attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">		<span class="comment"># 相对位置偏移</span></span><br><span class="line">        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-<span class="number">1</span>)].view(</span><br><span class="line">            self.window_size[<span class="number">0</span>] * self.window_size[<span class="number">1</span>], self.window_size[<span class="number">0</span>] * self.window_size[<span class="number">1</span>], -<span class="number">1</span>)  <span class="comment"># Wh*Ww,Wh*Ww,nH</span></span><br><span class="line">        relative_position_bias = relative_position_bias.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>).contiguous()  <span class="comment"># nH, Wh*Ww, Wh*Ww</span></span><br><span class="line">        attn = attn + relative_position_bias.unsqueeze(<span class="number">0</span>)</span><br><span class="line">		<span class="comment"># 判断是否需要mask</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            nW = mask.shape[<span class="number">0</span>]</span><br><span class="line">            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">            attn = attn.view(-<span class="number">1</span>, self.num_heads, N, N)</span><br><span class="line">            attn = self.softmax(attn)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            attn = self.softmax(attn)</span><br><span class="line"></span><br><span class="line">        attn = self.attn_drop(attn)</span><br><span class="line"></span><br><span class="line">        x = (attn @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(B_, N, C)</span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        x = self.proj_drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h4 id="相对位置偏置">相对位置偏置</h4>
<p>在计算自注意力时，在计算相似度的过程中对每个head加入相对位置偏置，如下所示：</p>
<p><img src="https://img-blog.csdnimg.cn/20210911231011149.png" alt="img"></p>
<ul>
<li>对于预训练中学到的相对位置偏置，可以通过双三次插值初始化具有不同窗口大小的微调模型。</li>
</ul>
<h3 id="Patch-Merging">Patch Merging</h3>
<p>Patch Merging层的功能是产生一个层次化表示，通过<strong>合并相邻的tokens</strong>，减少tokens的数目。</p>
<img src="https://img-blog.csdnimg.cn/20210913201833609.png" alt="img" style="zoom:120%; margin:auto; ">
<p>对于Stage1和Stage2间的Patch Merging层，将原维度为C的token合并为大小为4C的token，再使用一个线性层将输出维度降低为2C，token的数目降低为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>H</mi><mn>8</mn></mfrac><mo>×</mo><mfrac><mi>W</mi><mn>8</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{H}{8} \times \frac{W}{8}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">8</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">8</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>。</p>
<p>在之后的每个stage中，都会改变张量的维度，从而形成一种层次化的特征。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchMerging</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Patch Merging Layer.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        input_resolution (tuple[int]): Resolution of input feature.</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_resolution, dim, norm_layer=nn.LayerNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.input_resolution = input_resolution</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.reduction = nn.Linear(<span class="number">4</span> * dim, <span class="number">2</span> * dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.norm = norm_layer(<span class="number">4</span> * dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        x: B, H*W, C</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        H, W = self.input_resolution</span><br><span class="line">        B, L, C = x.shape</span><br><span class="line">        <span class="keyword">assert</span> L == H * W, <span class="string">&quot;input feature has wrong size&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> H % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> W % <span class="number">2</span> == <span class="number">0</span>, <span class="string">f&quot;x size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) are not even.&quot;</span></span><br><span class="line"></span><br><span class="line">        x = x.view(B, H, W, C)<span class="comment"># 把输入整形为BHWC</span></span><br><span class="line"></span><br><span class="line">        x0 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x1 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x2 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x3 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x = torch.cat([x0, x1, x2, x3], -<span class="number">1</span>)  <span class="comment"># B H/2 W/2 4*C</span></span><br><span class="line">        x = x.view(B, -<span class="number">1</span>, <span class="number">4</span> * C)  <span class="comment"># B H/2*W/2 4*C</span></span><br><span class="line"></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        x = self.reduction(x)<span class="comment"># 降低维度为原来的1/2</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>经典模型</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Model Behavior Preserving for Class-Incremental Learning</title>
    <url>/2022/08/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AModel-Behavior-Preserving-for-Class-Incremental-Learning/</url>
    <content><![CDATA[<ul>
<li>发表时间：2022</li>
<li>研究内容：类增量学习，探讨在增量学习中应保留旧模型的哪些功能性属性。</li>
<li>研究现状：现有的增量学习方法忽略了CNN模型响应间的内部结构，KD的硬约束导致新模型出现混沌行为。</li>
<li>创新点：<ol>
<li><p>Feature Space：设计了一个INP Loss保持成对实例在旧模型上的<strong>相似性顺序</strong>（反映实例集间的相邻关系）；</p>
<p>INP用于惩罚新模型在学习过程中每个实例相邻关系的变化。</p>
<img src="/2022/08/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AModel-Behavior-Preserving-for-Class-Incremental-Learning/image-20220826093609961.png" alt="image-20220826093609961" style="zoom:67%;margin:auto;">

<p>a. 旧实例A在特征空间中与其它实例的相邻关系；</p>
<p>b. 采用传统的KD，引入新实例G后，绝对位置的微小变化被严格限制；</p>
<p>c. 采用INP Loss当相对位置没变时，就不会限制更新。</p>
</li>
<li><p>Label Space：设计了一个LPP Loss在输出空间的实例标签概率向量中保留<strong>标签排名列表</strong>（反映实例属于每一类的排名）；</p>
</li>
<li><p>介绍了一种可导的排名计算方法用于计算上述Loss。</p>
</li>
</ol>
</li>
</ul>
]]></content>
      <tags>
        <tag>组内文章</tag>
        <tag>连续学习</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：PhysFormer: Facial Video-based Physiological Measurement with Temporal Difference Transformer</title>
    <url>/2022/09/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9APhysFormer-Facial-Video-based-Physiological-Measurement-with-Temporal-Difference-Transformer/</url>
    <content><![CDATA[<ul>
<li><p>发表时间：2021</p>
</li>
<li><p>研究对象：rPPG，使用多波长 RGB 相机检测人体皮肤表面脉冲引起的细微颜色变化，实现测量心脏活动和其它生理信号。</p>
</li>
<li><p>研究意义：传统的检测方法会造成discomfort，并且长期检测不方便</p>
</li>
<li><p>Code：<a href="https://github.com/ZitongYu/PhysFormer">https://github.com/ZitongYu/PhysFormer</a></p>
</li>
<li><p>rPPG研究历史：</p>
<ol>
<li>早期使用经典的信号处理方法检测面部细微的颜色变化；</li>
<li>使用非端到端方法，首先生成预处理的信号特征，然后模型从这些特征图中捕捉rPPG特征（对预处理要求严格，忽略了全局特征）；</li>
<li>端到端的基于深度学习的方法（容易被复杂的背景信息影响）。</li>
</ol>
</li>
<li><p>研究现状：</p>
<p>​		现有的基于卷积神经网络的模型在时间和空间上的感受野受限，忽略了长期的时间和空间上的互动与感知。</p>
</li>
<li><p>PhysFormer网络架构：</p>
<img src="/2022/09/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9APhysFormer-Facial-Video-based-Physiological-Measurement-with-Temporal-Difference-Transformer/image-20220919092041506.png" alt="image-20220919092041506" style="zoom:80%;">

<ol>
<li><p><strong>Stem</strong>：由三个卷积块组成，用于提取粗糙的局部时空特征</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># INPUT:[B,3,T,H,W]</span></span><br><span class="line"><span class="comment"># OUTPUT:[B,D,T,H/8,W/8]</span></span><br><span class="line"><span class="comment"># use</span></span><br><span class="line">x = self.Stem0(x)</span><br><span class="line">x = self.Stem1(x)</span><br><span class="line">x = self.Stem2(x)  <span class="comment"># [B, 64, 160, 64, 64]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># implement</span></span><br><span class="line">self.Stem0 = nn.Sequential(</span><br><span class="line">    nn.Conv3d(<span class="number">3</span>, dim//<span class="number">4</span>, [<span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>], stride=<span class="number">1</span>, padding=[<span class="number">0</span>,<span class="number">2</span>,<span class="number">2</span>]),</span><br><span class="line">    nn.BatchNorm3d(dim//<span class="number">4</span>),</span><br><span class="line">    nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">    nn.MaxPool3d((<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>), stride=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">self.Stem1 = nn.Sequential(</span><br><span class="line">    nn.Conv3d(dim//<span class="number">4</span>, dim//<span class="number">2</span>, [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.BatchNorm3d(dim//<span class="number">2</span>),</span><br><span class="line">    nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">    nn.MaxPool3d((<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>), stride=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">self.Stem2 = nn.Sequential(</span><br><span class="line">    nn.Conv3d(dim//<span class="number">2</span>, dim, [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.BatchNorm3d(dim),</span><br><span class="line">    nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">    nn.MaxPool3d((<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>), stride=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Tube Tokens</strong>：将stem输出划分为若干个时空tube token，将时空邻近语义聚合在一起，并减少后续transformer的计算量。通过一个3D卷积实现。</p>
<p>tip：在嵌入后没有额外加上位置编码，因为在stem里已经捕捉了相关的时空信息。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># INPUT: # [B, 64, 160, 64, 64] ([B,D,T,H/8,W/8])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># use</span></span><br><span class="line">x = self.patch_embedding(x)  <span class="comment"># [B, 64, 40, 4, 4]</span></span><br><span class="line">x = x.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># [B, 40*4*4, 64](B,N,D)</span></span><br><span class="line"><span class="comment"># implement</span></span><br><span class="line"><span class="comment"># Patch embedding    [4x16x16]conv</span></span><br><span class="line">self.patch_embedding = nn.Conv3d(dim, dim, kernel_size=(ft, fh, fw), stride=(ft, fh, fw))</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Temporal Difference Multi-head Self-attention</strong>：</p>
<p>与传统的自注意力机制不同，使用TDC（Temporal Difference Convolution）查询Q和K的投影，可以捕捉局部细粒度的时间差异特征。</p>
<img src="/C:/Users/zxkj/AppData/Roaming/Typora/typora-user-images/image-20221002170008240.png" alt="image-20221002170008240" style="zoom:80%; margin:auto;">

<img src="/C:/Users/zxkj/AppData/Roaming/Typora/typora-user-images/image-20221002170042081.png" alt="image-20221002170042081" style="zoom: 67%; margin:auto;">

<img src="/C:/Users/zxkj/AppData/Roaming/Typora/typora-user-images/image-20221002170158926.png" alt="image-20221002170158926" style="zoom:67%; margin:auto;">

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 多头自注意力机制实现</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadedSelfAttention_TDC_gra_sharp</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Multi-Headed Dot Product Attention with depth-wise Conv3d&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, num_heads, dropout, theta</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        self.proj_q = nn.Sequential(</span><br><span class="line">            CDC_T(dim, dim, <span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">False</span>, theta=theta),  </span><br><span class="line">            nn.BatchNorm3d(dim),</span><br><span class="line">            <span class="comment">#nn.ELU(),</span></span><br><span class="line">        )</span><br><span class="line">        self.proj_k = nn.Sequential(</span><br><span class="line">            CDC_T(dim, dim, <span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">False</span>, theta=theta),  </span><br><span class="line">            nn.BatchNorm3d(dim),</span><br><span class="line">            <span class="comment">#nn.ELU(),</span></span><br><span class="line">        )</span><br><span class="line">        self.proj_v = nn.Sequential(</span><br><span class="line">            nn.Conv3d(dim, dim, <span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, groups=<span class="number">1</span>, bias=<span class="literal">False</span>),  </span><br><span class="line">            <span class="comment">#nn.BatchNorm3d(dim),</span></span><br><span class="line">            <span class="comment">#nn.ELU(),</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.drop = nn.Dropout(dropout)</span><br><span class="line">        self.n_heads = num_heads</span><br><span class="line">        self.scores = <span class="literal">None</span> <span class="comment"># for visualization</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, gra_sharp</span>):    <span class="comment"># [B, 4*4*40, 128]</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        x, q(query), k(key), v(value) : (B(batch_size), S(seq_len), D(dim))</span></span><br><span class="line"><span class="string">        mask : (B(batch_size) x S(seq_len))</span></span><br><span class="line"><span class="string">        * split D(dim) into (H(n_heads), W(width of head)) ; D = H * W</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># (B, S, D) -proj-&gt; (B, S, D) -split-&gt; (B, S, H, W) -trans-&gt; (B, H, S, W)</span></span><br><span class="line">        </span><br><span class="line">        [B, P, C]=x.shape</span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).view(B, C, P//<span class="number">16</span>, <span class="number">4</span>, <span class="number">4</span>)      <span class="comment"># [B, dim, 40, 4, 4]</span></span><br><span class="line">        q, k, v = self.proj_q(x), self.proj_k(x), self.proj_v(x)<span class="comment"># 由CDC计算得到Q和K，3D卷积得到V</span></span><br><span class="line">        q = q.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># [B, 4*4*40, dim]</span></span><br><span class="line">        k = k.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># [B, 4*4*40, dim]</span></span><br><span class="line">        v = v.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># [B, 4*4*40, dim]</span></span><br><span class="line">        </span><br><span class="line">        q, k, v = (split_last(x, (self.n_heads, -<span class="number">1</span>)).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="keyword">for</span> x <span class="keyword">in</span> [q, k, v])</span><br><span class="line">        <span class="comment"># (B, H, S, W) @ (B, H, W, S) -&gt; (B, H, S, S) -softmax-&gt; (B, H, S, S)</span></span><br><span class="line">        scores = q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>) / gra_sharp<span class="comment"># 矩阵相乘，计算得分（权重）</span></span><br><span class="line">		<span class="comment"># tip：在python里，“@”表示数学上的矩阵相乘，“*”表示矩阵对应位置两元素相乘</span></span><br><span class="line">        </span><br><span class="line">        scores = self.drop(F.softmax(scores, dim=-<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># (B, H, S, S) @ (B, H, S, W) -&gt; (B, H, S, W) -trans-&gt; (B, S, H, W)</span></span><br><span class="line">        h = (scores @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous()<span class="comment"># 计算结果</span></span><br><span class="line">        <span class="comment"># -merge-&gt; (B, S, D)</span></span><br><span class="line">        h = merge_last(h, <span class="number">2</span>)</span><br><span class="line">        self.scores = scores</span><br><span class="line">        <span class="keyword">return</span> h, scores</span><br><span class="line"><span class="comment"># TDC</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CDC_T</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 padding=<span class="number">1</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">False</span>, theta=<span class="number">0.6</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(CDC_T, self).__init__()</span><br><span class="line">        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,                                   padding=padding, dilation=dilation, groups=groups, bias=bias)</span><br><span class="line">        self.theta = theta</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out_normal = self.conv(x)<span class="comment"># 正常3D卷积输出</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> math.fabs(self.theta - <span class="number">0.0</span>) &lt; <span class="number">1e-8</span>:</span><br><span class="line">            <span class="keyword">return</span> out_normal</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># pdb.set_trace()</span></span><br><span class="line">            [C_out, C_in, t, kernel_size, kernel_size] = self.conv.weight.shape</span><br><span class="line"></span><br><span class="line">            <span class="comment"># only CD works on temporal kernel size&gt;1</span></span><br><span class="line">            <span class="keyword">if</span> self.conv.weight.shape[<span class="number">2</span>] &gt; <span class="number">1</span>:</span><br><span class="line">                kernel_diff = self.conv.weight[:, :, <span class="number">0</span>, :, :].<span class="built_in">sum</span>(<span class="number">2</span>).<span class="built_in">sum</span>(<span class="number">2</span>) + self.conv.weight[:, :,                               <span class="number">2</span>, :, :].<span class="built_in">sum</span>(<span class="number">2</span>).<span class="built_in">sum</span>(<span class="number">2</span>)</span><br><span class="line">                kernel_diff = kernel_diff[:, :, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">                <span class="comment"># 时间差异项</span></span><br><span class="line">                out_diff = F.conv3d(<span class="built_in">input</span>=x, weight=kernel_diff, bias=self.conv.bias,                                                     stride=self.conv.stride, padding=<span class="number">0</span>, dilation=self.conv.dilation,                                     groups=self.conv.groups)</span><br><span class="line">                <span class="keyword">return</span> out_normal - self.theta * out_diff</span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Spatio-temporal Feed-forward</strong>：在常用的两层线性transformation层之间，引入基于深度的3D卷积，ST-FF可以改善局部不一致性和部分噪声特征，同时丰富的局部性为TD-MHSA提供足够的相对位置信息，从而实现性能提升。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># use</span></span><br><span class="line"><span class="comment"># Transformer</span></span><br><span class="line">self.transformer1 = Transformer_ST_TDC_gra_sharp(num_layers=num_layers//<span class="number">3</span>, dim=dim, 		                             num_heads=num_heads, ff_dim=ff_dim, dropout=dropout_rate, theta=theta)</span><br><span class="line"><span class="comment"># Transformer</span></span><br><span class="line">self.transformer2 = Transformer_ST_TDC_gra_sharp(num_layers=num_layers//<span class="number">3</span>, dim=dim,                                       num_heads=num_heads, ff_dim=ff_dim, dropout=dropout_rate, theta=theta)</span><br><span class="line"><span class="comment"># Transformer</span></span><br><span class="line">self.transformer3 = Transformer_ST_TDC_gra_sharp(num_layers=num_layers//<span class="number">3</span>, dim=dim,                                       num_heads=num_heads, ff_dim=ff_dim, dropout=dropout_rate, theta=theta)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer_ST_TDC_gra_sharp</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Transformer with Self-Attentive Blocks&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_layers, dim, num_heads, ff_dim, dropout, theta</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.blocks = nn.ModuleList([</span><br><span class="line">            Block_ST_TDC_gra_sharp(dim, num_heads, ff_dim, dropout, theta) <span class="keyword">for</span> _ <span class="keyword">in</span>                                                      <span class="built_in">range</span>(num_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, gra_sharp</span>):</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self.blocks:</span><br><span class="line">            x, Score = block(x, gra_sharp)</span><br><span class="line">        <span class="keyword">return</span> x, Score</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Block_ST_TDC_gra_sharp</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Transformer Block&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, num_heads, ff_dim, dropout, theta</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.attn = MultiHeadedSelfAttention_TDC_gra_sharp(dim, num_heads, dropout, theta)</span><br><span class="line">        self.proj = nn.Linear(dim, dim)</span><br><span class="line">        self.norm1 = nn.LayerNorm(dim, eps=<span class="number">1e-6</span>)</span><br><span class="line">        self.pwff = PositionWiseFeedForward_ST(dim, ff_dim)<span class="comment"># ST-FF</span></span><br><span class="line">        self.norm2 = nn.LayerNorm(dim, eps=<span class="number">1e-6</span>)</span><br><span class="line">        self.drop = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, gra_sharp</span>):</span><br><span class="line">        Atten, Score = self.attn(self.norm1(x), gra_sharp)</span><br><span class="line">        h = self.drop(self.proj(Atten))</span><br><span class="line">        x = x + h</span><br><span class="line">        h = self.drop(self.pwff(self.norm2(x)))</span><br><span class="line">        x = x + h</span><br><span class="line">        <span class="keyword">return</span> x, Score</span><br><span class="line">    </span><br><span class="line"><span class="comment"># implement</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionWiseFeedForward_ST</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;FeedForward Neural Networks for each position&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, ff_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        self.fc1 = nn.Sequential(</span><br><span class="line">            nn.Conv3d(dim, ff_dim, <span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>),  </span><br><span class="line">            nn.BatchNorm3d(ff_dim),</span><br><span class="line">            nn.ELU(),</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.STConv = nn.Sequential(</span><br><span class="line">            nn.Conv3d(ff_dim, ff_dim, <span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, groups=ff_dim, bias=<span class="literal">False</span>),  </span><br><span class="line">            nn.BatchNorm3d(ff_dim),</span><br><span class="line">            nn.ELU(),</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.fc2 = nn.Sequential(</span><br><span class="line">            nn.Conv3d(ff_dim, dim, <span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>),  </span><br><span class="line">            nn.BatchNorm3d(dim),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):    <span class="comment"># [B, 4*4*40, 128]</span></span><br><span class="line">        [B, P, C]=x.shape</span><br><span class="line">        <span class="comment">#x = x.transpose(1, 2).view(B, C, 40, 4, 4)      # [B, dim, 40, 4, 4]</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).view(B, C, P//<span class="number">16</span>, <span class="number">4</span>, <span class="number">4</span>)      <span class="comment"># [B, dim, 40, 4, 4]</span></span><br><span class="line">        x = self.fc1(x)		              <span class="comment"># x [B, ff_dim, 40, 4, 4]</span></span><br><span class="line">        <span class="comment"># 使用时空卷积</span></span><br><span class="line">        x = self.STConv(x)		          <span class="comment"># x [B, ff_dim, 40, 4, 4]</span></span><br><span class="line">        x = self.fc2(x)		              <span class="comment"># x [B, dim, 40, 4, 4]</span></span><br><span class="line">        x = x.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># [B, 4*4*40, dim]</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (B, S, D) -&gt; (B, S, D_ff) -&gt; (B, S, D)</span></span><br><span class="line">        <span class="comment">#return self.fc2(F.gelu(self.fc1(x)))</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>创新点：PhysFormer，一种端到端的视频transformer，联合使用了局部的和全局的时空特征。</p>
<ol>
<li>使用<strong>时差引导全局注意力机制</strong>，强化rPPG的周期性特征，针对干扰完善局部时空特征；</li>
<li>使用受label distribution learning和curriculum learning启发的频域动态约束，为PhysFormer提供详细的监督，缓解过拟合。</li>
<li>PhysFormer不需要像其它transformer网络那样在大规模数据集上预训练，仅在rPPG数据集训练即可。</li>
</ol>
</li>
<li><p><strong>Label Distribution Learning</strong>：对于面部的rPPG信号，心率相近的视频会有相似的周期性特征。为了使得模型学习到这种特征，将心率估计问题看作一个多分类问题，有多少个心率就有多少类别，类别概率向量由高斯分布组成。</p>
</li>
<li><p><strong>Curriculum Learning Guided Dynamic Loss</strong>：课程式学习是指模型从容易样本开始学习，逐步学习困难样本。在该任务中，从时域和频域两个方面限制模型学习，时域的限制更直接更容易学习，频域的限制较难学习，因此，使用动态的loss函数，逐步提高频域loss的比例。</p>
</li>
<li><p>实验：不同模型的对比实验、消融实验</p>
</li>
<li><p>Others：注意力图可视化</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>Transformer</tag>
        <tag>组内文章</tag>
        <tag>rPPG</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Rethinking the ST-GCNs for 3D skeleton-based human action recognition</title>
    <url>/2022/09/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARethinking-the-ST-GCNs-for-3D-skeleton-based-human-action-recognition/</url>
    <content><![CDATA[<ul>
<li>发表时间：2021</li>
<li>ST-GCN：Spatial-Temporal Graph Convolutional Network，用于解决骨骼数据的动作识别问题。</li>
<li>研究内容：<ol>
<li>证明了在ST-GCN中很多操作对于人体动作识别是没必要的</li>
<li>提出了一个简单有效的策略捕捉全局图的相关性，对输入序列进行有效建模，同时将输入图序列降入欧几里得空间，可以使用多尺度时域滤波器捕捉动态信息。</li>
</ol>
</li>
<li>研究现状：<ol>
<li>骨骼数据成为人体动作识别的主流输入（与传统的RGB视频数据相比，信息更完整）</li>
<li>直接将结构化的数据重新排列，使得tensor适应基础的神经网络（由于骨骼数据中没有天然的局部性概念，深度学习的能力受到限制）</li>
<li>设计一种适应结构化数据的自定义神经网络（ST-GCN）</li>
</ol>
</li>
</ul>
<p>TBC：GCN好难，看不懂</p>
]]></content>
      <tags>
        <tag>组内文章</tag>
        <tag>人体动作识别</tag>
        <tag>GCN</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Structural Knowledge Organization and Transfer for Class-Incremental Learning</title>
    <url>/2022/08/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AStructural-Knowledge-Organization-and-Transfer-for-Class-Incremental-Learning/</url>
    <content><![CDATA[<ul>
<li><p>发表时间：2021</p>
</li>
<li><p>研究问题：类增量学习</p>
</li>
<li><p>研究现状：</p>
<ol>
<li><p><strong>经典的知识蒸馏方法</strong>忽略了信息点之间的关联，当新数据远多于旧数据时，面临着严重的偏差问题；</p>
<ul>
<li><p>KD：在特征空间中，孤立地限制单个训练样本的位置，样本间的关系可能会被改变，并导致分类错误。</p>
</li>
<li><p>SGKD：保持样本的结构化知识，包括样本的位置和样本间关系，确保蒸馏后样本仍能被正确地分类。</p>
</li>
</ul>
<img src="/2022/08/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AStructural-Knowledge-Organization-and-Transfer-for-Class-Incremental-Learning/image-20220822101131518.png" alt="image-20220822101131518" style="zoom:67%;margin:auto;"></li>
</ol>
</li>
<li><p>创新点：</p>
<ol>
<li><p>使用一个<strong>memory knowledge graph</strong>(MKG)表征历史任务的结构化知识</p>
<ul>
<li>在特征空间中的绝对位置（MKG中用顶点表示已知example间的特征向量）</li>
<li>example间对应关系（边表示，使用余弦距离）</li>
</ul>
</li>
<li><p>使用<strong>图插值机制</strong>丰富知识域、缓解类间样本不平衡问题</p>
<p>通过向MKG中插入假的顶点，扩充和平滑分散的数据集，假顶点通过mix两个真顶点的vector得到。</p>
</li>
<li><p>使用<strong>结构化图知识蒸馏（SGKD）</strong>迁移旧知识</p>
<ul>
<li>顶点蒸馏损失</li>
<li>边蒸馏损失</li>
</ul>
<img src="/2022/08/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AStructural-Knowledge-Organization-and-Transfer-for-Class-Incremental-Learning/image-20220823094630810.png" alt="image-20220823094630810" style="zoom:67%;margin:auto;"></li>
</ol>
</li>
<li><p>人脸识别是怎么实现增加新样本的？</p>
<p>人脸识别网络的本质是一个特征提取器，并不是分类器，识别人脸的时候，通过计算输出特征和人脸库中人脸的距离判断人脸所属对象。未涉及类增量学习。</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>组内文章</tag>
        <tag>连续学习</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Revisiting Pixel-Wise Supervision for Face Anti-Spoofing</title>
    <url>/2022/09/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARevisiting-Pixel-Wise-Supervision-for-Face-Anti-Spoofing/</url>
    <content><![CDATA[<ul>
<li><p>发表时间：2021</p>
</li>
<li><p>研究内容：像素级的人脸识别反欺诈方法</p>
</li>
<li><p>创新点：提出基于<strong>金字塔</strong>的监督方法，模型从多空间尺度上学习局部和全局的语义信息</p>
</li>
<li><p>Presentation Attack Detection研究历史：</p>
<ol>
<li><p>传统算法关注于<strong>活体</strong>和<strong>手工特征</strong>的检测，需要丰富的任务级的先验知识。</p>
<p>活体检测：关注眨眼、面部和头部动作、视线追踪以及远程生理信号（这种方法需要长期的互动，容易被video attacks伪造）。</p>
<p>经典的handcrafted descriptors：从多种色彩空间中提取有效的欺诈模式，这种PA方法可以通过训练分类器捕捉，但在遇到未见过的场景或未知的PAs时就失效了。</p>
</li>
<li></li>
<li></li>
</ol>
</li>
<li></li>
</ul>
]]></content>
      <tags>
        <tag>组内文章</tag>
        <tag>人脸识别</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Social Distancing Alert with Smartwatches</title>
    <url>/2022/09/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ASocial-Distancing-Alert-with-Smartwatches/</url>
    <content><![CDATA[<ul>
<li><p>发表时间：2022</p>
</li>
<li><p>研究内容：基于智能手表的社交距离警报系统SoDA，SoDA使用加速器和陀螺仪的数据和简单有效的视觉Transformer模型，识别违反社交距离的活动。</p>
</li>
<li><p>code：<a href="https://github.com/aiotgroup/SoDA">https://github.com/aiotgroup/SoDA</a></p>
</li>
<li><p>创新点：</p>
<ol>
<li>应用价值</li>
<li>创建了一个数据集</li>
<li>证明了ViT是一种有效的方法？</li>
</ol>
</li>
<li><p>模型结构：</p>
<img src="/2022/09/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ASocial-Distancing-Alert-with-Smartwatches/image-20220920104544756.png" alt="image-20220920104544756" style="zoom:80%;"></li>
</ul>
]]></content>
      <tags>
        <tag>Transformer</tag>
        <tag>组内文章</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络基础知识</title>
    <url>/2022/09/27/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<h2 id="3D-CNN"><a href="#3D-CNN" class="headerlink" title="3D CNN"></a>3D CNN</h2><p>参考博客：</p>
<p>​		<a href="https://blog.csdn.net/auto1993/article/details/70948249">https://blog.csdn.net/auto1993/article/details/70948249</a></p>
<p>​		<a href="https://blog.csdn.net/YOULANSHENGMENG/article/details/121328554">https://blog.csdn.net/YOULANSHENGMENG/article/details/121328554</a></p>
<p>使用3D CNN可以捕获视频中的<strong>时间</strong>和<strong>空间</strong>的特征信息。</p>
<h3 id="Conv3D"><a href="#Conv3D" class="headerlink" title="Conv3D"></a>Conv3D</h3><p>3D卷积：对于下面的采用3D卷积核进行的卷积操作，通过堆叠多个连续的帧组成一个立方体，在立方体中运用3D卷积核，卷积层中每一个特征map都与上一层中多个邻近的连续帧相连，以此捕捉运动信息。下图卷积操作的时间维度为3（对连续的三帧图像进行卷积操作）。</p>
<img src="https://img-blog.csdn.net/20170429133650515?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvQVVUTzE5OTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img" style="zoom:80%;">

<p>tip：<strong>3D卷积核只能从cube中提取一种类型的特征</strong>（在整个cube中卷积核的权值是共享的），若要提取多种特征，可以采用多种卷积核。</p>
<p>在pytorch中，同样有Conv3D的实现，使用样例如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample intput | 随机输入</span></span><br><span class="line"><span class="comment"># (batch_size, channel, fram_size, height, width)</span></span><br><span class="line">net_input = torch.randn(<span class="number">32</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 所有维度同一个参数配置</span></span><br><span class="line">conv = nn.Conv3d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">net_output = conv(net_input)</span><br><span class="line"><span class="built_in">print</span>(net_output.shape)  <span class="comment"># shape=[32, 64, 5, 112, 112] | 相当于每一个维度上的卷积核大小都是3，步长都是2，pad都是1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 每一维度不同参数配置</span></span><br><span class="line">conv = nn.Conv3d(<span class="number">3</span>, <span class="number">64</span>, (<span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">net_output = conv(net_input)</span><br><span class="line"><span class="built_in">print</span>(net_output.shape) <span class="comment"># shape=[32, 64, 9, 112, 112]</span></span><br></pre></td></tr></table></figure>

<h3 id="3D-CNN-1"><a href="#3D-CNN-1" class="headerlink" title="3D CNN"></a>3D CNN</h3>]]></content>
      <tags>
        <tag>神经网络基础知识</tag>
      </tags>
  </entry>
  <entry>
    <title>经典模型：Video Swin Transformer</title>
    <url>/2022/09/28/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AVideo-Swin-Transformer/</url>
    <content><![CDATA[<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>Video Swin Transformer的结构整体如下图所示，与Swin-T相比多了时间维度。</p>
<img src="/2022/09/28/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AVideo-Swin-Transformer/image-20220928191836623.png" alt="image-20220928191836623" style="zoom:80%; margin:auto;">

<h3 id="3D-Patch-Partition"><a href="#3D-Patch-Partition" class="headerlink" title="3D Patch Partition"></a>3D Patch Partition</h3><p>在初始时，设置每个token的大小为2x4x4x3，因此，每个视频被划分为$\frac{T}{2}\times \frac{H}{4} \times \frac{W}{4}$个tokens，每个token的维度为96。</p>
<h3 id="Linear-Embedding"><a href="#Linear-Embedding" class="headerlink" title="Linear Embedding"></a>Linear Embedding</h3><p>将每一个token投影到指定的维度C。</p>
<h3 id="Video-Swin-Transformer-Block"><a href="#Video-Swin-Transformer-Block" class="headerlink" title="Video Swin Transformer Block"></a>Video Swin Transformer Block</h3><p>对于两个相邻的Video Swin Transformer Block，仍采用和Swin Transformer相同的处理方法，每个3D W-MSA后接一个3D SW-MSA。</p>
<img src="/2022/09/28/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AVideo-Swin-Transformer/image-20220928200155711.png" alt="image-20220928200155711" style="zoom:67%; margin:auto;">

<h4 id="3D-W-MSA"><a href="#3D-W-MSA" class="headerlink" title="3D W-MSA"></a>3D W-MSA</h4><p>3D W-MSA与W-MSA相似，对于大小为8x8x8的输入，使用4x4x4的窗口大小，在stage1中窗口的数目为2x2x2，在每个窗口内部进行自注意力计算。</p>
<img src="/2022/09/28/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AVideo-Swin-Transformer/image-20220928210505074.png" alt="image-20220928210505074" style="zoom:67%; margin:auto;">

<h4 id="3D-SW-MSA"><a href="#3D-SW-MSA" class="headerlink" title="3D SW-MSA"></a>3D SW-MSA</h4><p>3D SW-MSA与SW-MSA相同，使用移动窗口补充计算不同窗口中token间的自注意力，为了降低移动窗口后增加的窗口数，同样使用拼接与mask技术，保持窗口数的恒定。</p>
<h3 id="Patch-Merging"><a href="#Patch-Merging" class="headerlink" title="Patch Merging"></a>Patch Merging</h3><p>在进行patch合并时，不从时间维度进行下采样，而是从空间维度对2x2的patch进行合并，合并之后使用一个线性层投影将其维度减半。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><h3 id="不同的时空注意力设计"><a href="#不同的时空注意力设计" class="headerlink" title="不同的时空注意力设计"></a>不同的时空注意力设计</h3><ul>
<li>joint：在每一个3D窗口中联合计算时空注意力。</li>
<li>split：在空间swin transformer的基础上添加了两个时间transformer层</li>
<li>factorized：先是一个空间MSA层，再接一个时间MSA层。</li>
</ul>
<p>实验证明，在视频分类任务中，综合考虑速度与精度，joint模式达到了最佳，作者认为空间域的局部性减少了joint的计算量，同时保持了有效性。</p>
<h3 id="3D-token的时间维度与窗口的时间维度"><a href="#3D-token的时间维度与窗口的时间维度" class="headerlink" title="3D token的时间维度与窗口的时间维度"></a>3D token的时间维度与窗口的时间维度</h3><p>总的来说，3D token的时间维度与窗口的时间维度越大，精度越高，相应的计算成本也越高。</p>
]]></content>
      <tags>
        <tag>经典模型</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Deep Learning Methods for Remote Heart Rate Measurement: A Review and Future Research Agenda</title>
    <url>/2022/09/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeep-Learning-Methods-for-Remote-Heart-Rate-Measurement-A-Review-and-Future-Research-Agenda/</url>
    <content><![CDATA[<p>参考博客：<a href="https://blog.csdn.net/m0_46792836/article/details/121222265">https://blog.csdn.net/m0_46792836/article/details/121222265</a></p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><strong>rPPG</strong>，即remote photoplethysmography，远程光电容积脉搏波描记法，通过摄像头捕捉皮肤细微的亮度变化监测心率。</p>
<p>PPG是一种最常用的测量心率的方法，使用一个光源和光电探测器测量皮下血管体积的变化，当光源照在组织上时，光探测器可以捕捉到血液流动反射或透射光强度的微小变化，产生所谓的PPG信号，光的吸收遵循Beer–Lambert定律，该定律指出，血液吸收的光与光进入皮肤的渗透力和血液中血红蛋白的浓度成正比。在心动周期中，血红蛋白浓度的微小变化引起血管吸收的光量的32个波动，导致33个皮肤强度值的变化。</p>
<p>但对于穿戴式或接触式的监测设备，不适用于检测新生儿或皮肤脆弱的患者，长期检测可能会导致患者不舒服或皮肤感染。</p>
<p>在rPPG中，使用相机作为光探测器捕捉皮肤微小的颜色变化，自然光作为光源。其DRM（色反射）模型如下图所示，可以看到相机同时捕捉到皮肤表面产生的镜面反射和身体产生的漫反射，其中镜面反射并不包含有意义的生理信息，因此需要对捕捉到的信号进行精细处理。</p>
<p><img src="/2022/09/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeep-Learning-Methods-for-Remote-Heart-Rate-Measurement-A-Review-and-Future-Research-Agenda/image-20220929213602840.png" alt="image-20220929213602840" style="zoom:67%; margin:auto;"></p>
<p>传统的远程心率测量方法如下图所示，首先对视频进行人脸检测，接着从人脸上选择感兴趣的区域（ROI），以此获得包含强信号的区域，此后从ROI内的像素提取rPPG信号，最后对rPPG信号进一步处理（如频率分析和峰值检测等）获取心率信息。</p>
<p><img src="/2022/09/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeep-Learning-Methods-for-Remote-Heart-Rate-Measurement-A-Review-and-Future-Research-Agenda/image-20220930082244032.png" alt="image-20220930082244032" style="zoom: 67%; margin:auto;"></p>
<p>基于深度学习的远程心率检测方法可以分为<strong>端到端</strong>和<strong>混合深度学习</strong>的方法。</p>
<h2 id="端到端的深度学习方法"><a href="#端到端的深度学习方法" class="headerlink" title="端到端的深度学习方法"></a>端到端的深度学习方法</h2><p>所谓的端到端的方法，即模型直接将视频作为输入，产生心率或rPPG信号输出。这种方法需要大量的训练数据，同时训练结果难以验证，我们需要做更多的工作对模型进行解释。</p>
<p>端到端的心率检测方法又可以分为如下两种：</p>
<p><img src="/2022/09/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeep-Learning-Methods-for-Remote-Heart-Rate-Measurement-A-Review-and-Future-Research-Agenda/image-20220930085223073.png" alt="image-20220930085223073" style="zoom: 67%; margin: auto;"></p>
<h3 id="2D-CNN"><a href="#2D-CNN" class="headerlink" title="2D CNN"></a>2D CNN</h3><p>2D CNN只考虑了视频帧的空间信息。</p>
<ul>
<li>HR-CNN（2018）：包含提取器和HR估计器的两步CNN，提取器从视频帧序列中提取rPPG信号，使其信噪比最大化。HR-CNN解决了<strong>视频压缩伪影</strong>的问题。</li>
<li>DeepPhys（2018）：同时训练一个运动模型和外观模型，运动模型将相邻视频帧间的归一化差作为模型的输入表示，对帧中的运动和颜色变化进行建模；外观模型通过注意力机制引导运动模型学习运动表征。该模型可以更好地<strong>捕捉不同光照条件下的生理信号</strong>，对光照变化和被试运动更有鲁棒性。</li>
<li>MTTS-CAN（2020）：DeepPhys的改进，引入时间移位模块（TSM）捕获时间信息，TSM允许相邻帧之间的信息交换，避免昂贵的3D卷积操作。</li>
</ul>
<h3 id="时空网络——3D-CNN"><a href="#时空网络——3D-CNN" class="headerlink" title="时空网络——3D CNN"></a>时空网络——3D CNN</h3><p>3D CNN可以利用视频中包含的时间信息，时空网络（STNs）有效地表示视频流中生理信号的时空信息。</p>
<ul>
<li>3D CNN PhysNet：旨在定位每个个体心跳的峰值，以准确估计被试的HR和HRV。</li>
<li>两阶段STN：包括一个时空视频增强网络（3D STVEN）和一个时空3D CNN（rPPGNet），压缩的面部视频通过3D STVEN以提高视频质量，同时保留尽可能多的信息；增强后的视频输入rPPGNet以提取rPPG信号，rPPGNet使用注意力机制从皮肤区域获取主导的rPPG特征。</li>
<li>AutoHR：使用神经结构搜索（NAS）自动找到最适合的主干3D CNN，使用一种三维卷积操作时域差分卷积（TDC）帮助跟踪感兴趣区域。</li>
</ul>
<h3 id="时空网络——2D-CNN-RNN"><a href="#时空网络——2D-CNN-RNN" class="headerlink" title="时空网络——2D CNN+RNN"></a>时空网络——2D CNN+RNN</h3><p>使用2D CNN提取空间信息，用RNN提取时间前后信息并结合。</p>
<ul>
<li>基于RNN的PhysNet：首先将输入信息输入到2D CNN中提取RGB视频帧的空间特征，然后利用RNN在时域内传播这些空间特征。但研究证明，基于3D CNN的PhysNet比基于RNN的PhysNet获得了更好的性能。</li>
</ul>
<p><img src="/2022/09/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeep-Learning-Methods-for-Remote-Heart-Rate-Measurement-A-Review-and-Future-Research-Agenda/image-20220930105928378.png" alt="image-20220930105928378" style="zoom: 67%; margin:auto;"></p>
<h2 id="混合深度学习方法"><a href="#混合深度学习方法" class="headerlink" title="混合深度学习方法"></a>混合深度学习方法</h2><p>所谓的混合深度学习方法，是指深度学习技术只应用在检测过程中的某些部分，如信号优化、信号提取或心率估计。</p>
<h3 id="用于信号优化的深度学习方法"><a href="#用于信号优化的深度学习方法" class="headerlink" title="用于信号优化的深度学习方法"></a>用于信号优化的深度学习方法</h3><p>所谓的信号优化就是使用人脸检测或皮肤分割，以<strong>忽略不相关的背景信息</strong>。</p>
<ul>
<li>创建一个用于皮肤检测的2D CNN，用于分割出皮肤所在区域，对检测到的皮肤区域进行常规rPPG算法。但这种方法利用了人脸的所有皮肤区域提取rPPG信号，可能包含不必要的噪声。</li>
<li>Deep-HR：采用接受域块（RFB）网络对感兴趣区域进行目标检测，该方法设计了GAN增强检测到的ROI，对检测到的ROI进行再生，将这个高质量的ROI（这个过程也可以看作后面的信号提取的过程）用于后续的信号提取。</li>
</ul>
<h3 id="用于信号提取的深度学习"><a href="#用于信号提取的深度学习" class="headerlink" title="用于信号提取的深度学习"></a>用于信号提取的深度学习</h3><p>信号提取的目标是从视频中<strong>提取高质量的rPPG信号</strong>进行HR估计（感觉是指去噪这个过程）。</p>
<ul>
<li><p>LSTM：使用LSTM网络对噪声污染的rPPG信号进行滤波，得到无噪声的rPPG信号。由于数据不足的问题，在训练时可以首先在合成数据上进行训练，然后在真实数据集上微调。</p>
</li>
<li><p>2D CNN MetaPhys：使用预训练的2D CNN TS-CAN用于信号提取，并提出元学习方法，利用模型不可知元学习作为个性化参数更新模式，可以在只有少量训练样本的情况下快速适应。作者认为该方法可以减少由于肤色造成的偏差。</p>
</li>
<li><p>3D CNN Siamese-rPPG：基于Siamese 3D CNN框架，作者认为不同的面部区域应反映相同的rPPG特征，建立额头分支和脸颊分支进行特征提取，将两个分支的输出通过运算融合，得到最终的预测的rPPG信号。</p>
<p><img src="/2022/09/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeep-Learning-Methods-for-Remote-Heart-Rate-Measurement-A-Review-and-Future-Research-Agenda/image-20220930142543663.png" alt="image-20220930142543663" style="zoom: 67%; margin: auto;"></p>
</li>
<li><p>3D CNN HeartTrack：利用带有注意力机制的3D CNN进行信号提取，在三维时空注意网络中，利用硬注意力机制忽略不相关的背景信息，利用软注意力机制过滤所覆盖的区域。再将提取到的信号送入1D CNN进行时间序列分析。</p>
</li>
</ul>
<h3 id="用于心率估计的深度学习"><a href="#用于心率估计的深度学习" class="headerlink" title="用于心率估计的深度学习"></a>用于心率估计的深度学习</h3><p>对于提取到的rPPG信号，传统的方法是使用带通滤波器滤波，然后进行频率分析或峰值检测来估计心率。对于深度学习方法，<strong>将心率估计看作回归问题求解</strong>。</p>
<p>HR信号有两种表示：</p>
<ol>
<li>频谱图像：对提取的rPPG信号进行短时傅里叶变换和带通滤波，得到频域表示，将频域表示和时域信号结合，形成频谱图像。</li>
<li>时空图：将ROI像素RGB通道的颜色信息串接在时间序列中，成行排列，形成时空地图。这种信号表示方法可以抑制与HR信号无关的信息。</li>
</ol>
<h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><ul>
<li>流行病控制</li>
<li>防伪：通过捕捉异常生物信号检测深度造假的视频</li>
<li>远程医疗</li>
<li>增强生物识别技术的安全性</li>
<li>驾驶状态检测</li>
<li>？从自然灾害中寻找幸存者</li>
<li>新生儿检测</li>
<li>健康跟踪</li>
</ul>
<h2 id="研究缺口"><a href="#研究缺口" class="headerlink" title="研究缺口"></a>研究缺口</h2><ul>
<li>影响因素：基于rPPG的远程HR测量收到光照变化、运动伪影、肤色变化和视频压缩等诸多因素的影响。<strong>新方法应提供如何从技术和生物物理角度处理这些挑战的见解</strong></li>
<li>测量其他生命体征</li>
<li>数据集：目前数据集抓鱼用于解决运动伪影、照明变化，对于肤色变化、多人检测、远距离、新生儿检测也需要克服。</li>
<li>在不同HR范围上的表现</li>
<li>对基于深度学习方法上的理解</li>
</ul>
]]></content>
      <tags>
        <tag>rPPG</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：MAE</title>
    <url>/2022/10/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AMAE/</url>
    <content><![CDATA[<p>参考：<a href="https://www.bilibili.com/video/BV1sq4y1q77t/">https://www.bilibili.com/video/BV1sq4y1q77t/</a></p>
<p>代码：<a href="https://github.com/facebookresearch/mae">https://github.com/facebookresearch/mae</a></p>
<h2 id="模型概述"><a href="#模型概述" class="headerlink" title="模型概述"></a>模型概述</h2><p>MAE即Masked Autoencoders（其auto是指“自“的意思，而不是自动，指label也来自图片本身），可以看作CV版的Bert，基于ViT方法。</p>
<p>MAE的整体思路是将图片中的一些patches进行mask，然后对masked掉的patches进行预测。如下图所示：</p>
<img src="/2022/10/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AMAE/image-20221003114506533.png" alt="image-20221003114506533" style="zoom:80%; margin:auto;">

<h2 id="NLP与CV"><a href="#NLP与CV" class="headerlink" title="NLP与CV"></a>NLP与CV</h2><p>作者认为造成NLP与CV任务中masked autoencoding的不同，主要来自以下几个方面：</p>
<ol>
<li>文本与图像的信息密度不同，文本是人为生成的，具有<strong>高语义和信息密度</strong>，而图像则具有大量的冗余信息</li>
<li>对于自编码器的解码器，CV任务需要还原的是语义层次比较低的像素信息（解码器更加复杂），而NLP任务需要还原的是语义层次比较高的文本信息（解码器简单）</li>
</ol>
<h2 id="MAE具体实现"><a href="#MAE具体实现" class="headerlink" title="MAE具体实现"></a>MAE具体实现</h2><ul>
<li><p>masking：将一张图片分成若干个patches，然后随机选取部分patches进行mask，仅保留少量的patches，以减少冗余信息。</p>
</li>
<li><p>MAE Encoder：采用完全和ViT相同的方法，对于被masked掉的patches，直接不输入Encoder。</p>
</li>
<li><p>MAE Decoder：解码器同样使用一个Transformer，对于所有patches，通过一个共享的、可以学到的向量来表示，对于每一个块要加上位置编码信息，解码器只有在预训练的时候需要使用。</p>
</li>
<li><p>Reconstruction target：对于解码器的输出通过一个线形层投影到对应的输出shape，使用MSE对被masked的patches计算loss。</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Swin Transformer V2: Scaling Up Capacity and Resolution</title>
    <url>/2022/10/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ASwin-Transformer-V2-Scaling-Up-Capacity-and-Resolution/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>论文阅读：Benchmarking Joint Face Spoofing and Forgery Detection with Visual and Physiological Cues</title>
    <url>/2022/10/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ABenchmarking-Joint-Face-Spoofing-and-Forgery-Detection-with-Visual-and-Physiological-Cues/</url>
    <content><![CDATA[<ul>
<li>发表时间：2022</li>
<li>研究内容：基于视觉和生理线索的联合人脸欺骗和伪造检测标杆</li>
<li>FAS：Face Anti-Spoofing，活体检测</li>
<li>Face Forgery Detection：人脸伪造检测（Deepfake检测）</li>
</ul>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>目前大多数的<strong>活体检测</strong>和<strong>人脸伪造检测</strong>方法关注以下两个方面：</p>
<ol>
<li>单模态的视觉外观或生理信号（rPPG信号）</li>
<li>分离FAS或人脸伪造检测的特征表示</li>
</ol>
<p>但这两种方面存在如下问题：</p>
<ol>
<li>单模态的外观和rPPG特征对于高保真的3D面罩或视频回放袭击来说相对脆弱。</li>
<li>对于FAS和人脸欺诈识别任务，有丰富的常用特征，可用于以多任务学习的方式设计一个FAS和人脸欺诈识别联合系统。</li>
</ol>
<p>本文<strong>贡献</strong>：</p>
<ol>
<li>建立了第一个活体检测和伪造识别的benchmark，该benchmark同时使用视觉外观和生理rPPG信号。</li>
<li>设计了一个双分支的生理网络，同时使用面部时空的rPPG信号和其对应的连续小波变换作为输入。加强了rPPG周期性的区别。</li>
<li>在进行多模态融合前，对视觉外观和rPPG特征实施a weighted batch and layer normalization，以消除模态偏差，提高融合效果。</li>
<li>研究发现，无论单模态还是多模态模型，通过在活体检测和人脸伪造检测两个任务上进行联合训练可以显著提高模型泛化能力。</li>
</ol>
<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>基于rPPG或外观+rPPG的活体检测与人脸伪造识别技术还不成熟。主要包括如下两个原因：</p>
<ol>
<li>由于外部干扰，时域rPPG信号的周期性识别受到限制（无法区分是真实的还是伪造的）。</li>
<li>由于模态偏差，外观与rPPG信号的直接融合会导致负效果。</li>
</ol>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p><strong>Physiological rPPG based FAS：</strong>使用rPPG信号进行活体检测。主要包括如下几种做法：</p>
<ul>
<li>考虑真实的面部和打印出的面部的心跳区别；</li>
<li>通过比较面部的rPPG信号和背景噪音判断是否为活体；</li>
<li>使用transformer架构提取全局的周期性的信息用于活体检测；</li>
<li>通过评分级融合结合外观和rPPG预测实现活体检测</li>
</ul>
<p><strong>Physiological rPPG based face forgery detection：</strong>使用rPPG信号进行伪造人脸识别，主要包括如下几种做法：</p>
<ul>
<li>与活体检测类似，通过判断心率是否包含固定周期性模式；</li>
<li>从时间域和功率谱域提取rPPG信号的特征，描述时间一致性和空间相干性；</li>
</ul>
<h2 id="实现方法"><a href="#实现方法" class="headerlink" title="实现方法"></a>实现方法</h2><p><strong>Joint Face Spoofing and Forgery Detection：</strong>联合训练两种任务</p>
<ul>
<li><p>联合训练架构：在多任务训练中有如下三种设置方式：</p>
<img src="/2022/10/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ABenchmarking-Joint-Face-Spoofing-and-Forgery-Detection-with-Visual-and-Physiological-Cues/image-20221005201731187.png" alt="image-20221005201731187" style="zoom:67%; margin:auto;">
</li>
<li><p>联合训练采样策略</p>
<ul>
<li>随机采样：每个batch中的数据从两任务的混合数据中随机采样（比例不定）</li>
<li>同步采样：每个batch中的数据来自两任务的数据各占一半</li>
<li>交替采样：每个SGD step轮流学习每个任务（即一个batch学习FAS，一个batch学习伪造人脸识别）</li>
<li>按任务采样：先学习一个任务（全部数据），再学习另一个任务。</li>
</ul>
</li>
</ul>
<p><strong>Two-branch Physiological Network：</strong>同时使用面部时空的rPPG信号和其对应的连续小波变换作为输入。</p>
<p>文章使用的两分支的生理网络如下图所示，使用两个结构相同但参数不同的编码器获得时域、时频域特征，将两种特征拼接形成通用的rPPG特征，然后接一个二分类头进行预测，同时为了训练的稳定性，对于两个分支添加额外的二分类头，模型的Loss由上述三个头的loss组成。</p>
<img src="/2022/10/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ABenchmarking-Joint-Face-Spoofing-and-Forgery-Detection-with-Visual-and-Physiological-Cues/image-20221005204141328.png" alt="image-20221005204141328" style="zoom:67%; margin:auto;">

<ul>
<li>MSTmap（Multi-scale Spatial-temporal map）：多尺度时空图，考虑局部和全局的生理信号。</li>
<li>WaveletMap：基于连续小波变换的时频图</li>
</ul>
<p><strong>Weighted Normalization for Appearance and rPPG Fusion：</strong>多模态融合</p>
<p>对于每个模态使用batch normalization和layer normalization，然后再将两个模态进行连接。如下所示：</p>
<img src="/2022/10/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ABenchmarking-Joint-Face-Spoofing-and-Forgery-Detection-with-Visual-and-Physiological-Cues/image-20221005212654204.png" alt="image-20221005212654204" style="zoom: 50%; margin:auto;">



<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>未来研究方向：</p>
<ol>
<li>探索多任务学习和多模态融合的策略</li>
<li>除了基于rPPG的颜色变化，利用面部运动的语义线索和上下文动态进行活体检测和伪造人脸识别。</li>
</ol>
]]></content>
      <tags>
        <tag>rPPG</tag>
        <tag>FAS</tag>
      </tags>
  </entry>
</search>
