<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>HelloWorld!</title>
    <url>/2020/08/29/HelloWorld/</url>
    <content><![CDATA[<h2 id="A-new-world"><a href="#A-new-world" class="headerlink" title="A new world!"></a>A new world!</h2><p>哈喽！历经一个下午博客终于搭建好了，原来是那么容易的一件事情，大一的时候想的很复杂，迟迟没能动手，现在也终于有了自己的小博客啦~</p>
<p>未来灌水的文章还是会首先发在CSDN上，这里会分享一些重大的经历~已经大三了！要更努力学习！不要被些奇奇怪怪的事情干扰，奥里给！</p>
]]></content>
  </entry>
  <entry>
    <title>经典模型：Vision Transformer</title>
    <url>/2022/09/26/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AVision-Transformer/</url>
    <content><![CDATA[<h2 id="模型介绍"><a href="#模型介绍" class="headerlink" title="模型介绍"></a>模型介绍</h2><ul>
<li>可参考博客：<a href="https://blog.csdn.net/qq_39478403/article/details/118704747">https://blog.csdn.net/qq_39478403/article/details/118704747</a></li>
</ul>
<h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>ViT主要使用Transformer的encoder部分</p>
<img src="/2022/09/26/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AVision-Transformer/image-20220926083713750.png" alt="image-20220926083713750" style="zoom: 67%; margin:auto;">

<ul>
<li>将一张图像分成若干个大小固定且相同的patch，将每个patch投影到线性空间中再加上位置编码</li>
<li>除了每个patch作为一个token外，在序列中添加一个额外的classification token</li>
</ul>
<p><strong>位置编码：（看下代码咋实现的）</strong></p>
<ul>
<li>使用可学习的一维位置编码（作者发现使用更高维的位置编码并没有带来显著的精度提升）</li>
</ul>
<p><strong>混合结构：</strong></p>
<ul>
<li>可以将原始图像使用CNN进行特征提取，将特征图按patch划分送入Transformer中。</li>
</ul>
<h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><p><strong>ViT使用：</strong></p>
<ul>
<li>ViT与Bert类似，先在大数据集上<strong>训练</strong>，再在downstream任务上<strong>微调</strong>。在微调时，将预训练用的预测头换成一个用0初始化的DxK的前馈层，其中K代表下游任务总的类别数。</li>
<li>在微调时，使用更高分辨率的图像可以获得更好的结果。（patch大小不变，输入序列变成，不影响网络结构），这样会导致之前训练得到的<strong>位置编码</strong>无意义，因此在原来的位置编码上进行一个2D的插值。</li>
</ul>
<p><strong>实验结果：</strong></p>
<ul>
<li>当<strong>考虑预训练的训练代价</strong>时，ViT以更低的代价达到了SOTA水平</li>
<li>在<strong>自监督</strong>问题上，ViT很有应用前景</li>
</ul>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><ul>
<li>Official Code：<a href="https://github.com/google-research/vision_transformer">https://github.com/google-research/vision_transformer</a></li>
<li>timm code：<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py">https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py</a></li>
</ul>
<h3 id="Patch-Embeddings"><a href="#Patch-Embeddings" class="headerlink" title="Patch Embeddings"></a>Patch Embeddings</h3><p>实现功能：将输入图像划分为若干个patch，并将每个patch拉平投影到D维。通过一个二维的卷积操作即可实现。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/patch_embed.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 2D Image to Patch Embedding</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">            self,</span></span><br><span class="line"><span class="params">            img_size=<span class="number">224</span>,</span></span><br><span class="line"><span class="params">            patch_size=<span class="number">16</span>,<span class="comment"># 每个patch大小为16x16x3</span></span></span><br><span class="line"><span class="params">            in_chans=<span class="number">3</span>,</span></span><br><span class="line"><span class="params">            embed_dim=<span class="number">768</span>,<span class="comment"># 将patch映射到768维</span></span></span><br><span class="line"><span class="params">            norm_layer=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">            flatten=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">            bias=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        img_size = to_2tuple(img_size)</span><br><span class="line">        patch_size = to_2tuple(patch_size)</span><br><span class="line">        self.img_size = img_size</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.grid_size = (img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>], img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>])</span><br><span class="line">        self.num_patches = self.grid_size[<span class="number">0</span>] * self.grid_size[<span class="number">1</span>]<span class="comment"># 每张图像对应patch个数</span></span><br><span class="line">        self.flatten = flatten</span><br><span class="line">        </span><br><span class="line">		<span class="comment"># 通过一步卷积操作实现嵌入</span></span><br><span class="line">        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias)</span><br><span class="line">        self.norm = norm_layer(embed_dim) <span class="keyword">if</span> norm_layer <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, C, H, W = x.shape</span><br><span class="line">        _<span class="keyword">assert</span>(H == self.img_size[<span class="number">0</span>], <span class="string">f&quot;Input image height (<span class="subst">&#123;H&#125;</span>) doesn&#x27;t match model (<span class="subst">&#123;self.img_size[<span class="number">0</span>]&#125;</span>).&quot;</span>)</span><br><span class="line">        _<span class="keyword">assert</span>(W == self.img_size[<span class="number">1</span>], <span class="string">f&quot;Input image width (<span class="subst">&#123;W&#125;</span>) doesn&#x27;t match model (<span class="subst">&#123;self.img_size[<span class="number">1</span>]&#125;</span>).&quot;</span>)</span><br><span class="line">        x = self.proj(x)<span class="comment"># 投影</span></span><br><span class="line">        <span class="keyword">if</span> self.flatten:</span><br><span class="line">            x = x.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># BCHW -&gt; BNC</span></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h3 id="可学习的嵌入"><a href="#可学习的嵌入" class="headerlink" title="可学习的嵌入"></a>可学习的嵌入</h3><ul>
<li>cls_token：为了与Bert保持一致，设置可学习的嵌入向量作为用于分类的类别向量。</li>
<li>pos_embed：由于自注意力机制具有<strong>扰动不变性</strong>（打乱tokens中的顺序并不会改变结果），因此需要位置编码标识位置信息。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cls_token</span></span><br><span class="line">self.cls_token = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, embed_dim)) <span class="keyword">if</span> class_token <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"><span class="comment"># pos_embed</span></span><br><span class="line">embed_len = num_patches <span class="keyword">if</span> no_embed_class <span class="keyword">else</span> num_patches + self.num_prefix_tokens</span><br><span class="line">self.pos_embed = nn.Parameter(torch.randn(<span class="number">1</span>, embed_len, embed_dim) * <span class="number">.02</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_pos_embed</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="keyword">if</span> self.no_embed_class:</span><br><span class="line">        <span class="comment"># 先加位置编码再拼接cls_token</span></span><br><span class="line">        <span class="comment"># position embedding does not overlap with class token, add then concat</span></span><br><span class="line">        x = x + self.pos_embed</span><br><span class="line">        <span class="keyword">if</span> self.cls_token <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = torch.cat((self.cls_token.expand(x.shape[<span class="number">0</span>], -<span class="number">1</span>, -<span class="number">1</span>), x), dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 先拼接cls_token再加位置编码</span></span><br><span class="line">        <span class="comment"># pos_embed has entry for class token, concat then add</span></span><br><span class="line">        <span class="keyword">if</span> self.cls_token <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = torch.cat((self.cls_token.expand(x.shape[<span class="number">0</span>], -<span class="number">1</span>, -<span class="number">1</span>), x), dim=<span class="number">1</span>)</span><br><span class="line">        x = x + self.pos_embed</span><br><span class="line">    <span class="keyword">return</span> self.pos_drop(x)</span><br></pre></td></tr></table></figure>

<h3 id="多头注意力机制"><a href="#多头注意力机制" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, num_heads=<span class="number">8</span>, qkv_bias=<span class="literal">False</span>, attn_drop=<span class="number">0.</span>, proj_drop=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">assert</span> dim % num_heads == <span class="number">0</span>, <span class="string">&#x27;dim should be divisible by num_heads&#x27;</span></span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        head_dim = dim // num_heads</span><br><span class="line">        self.scale = head_dim ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        self.qkv = nn.Linear(dim, dim * <span class="number">3</span>, bias=qkv_bias)</span><br><span class="line">        self.attn_drop = nn.Dropout(attn_drop)</span><br><span class="line">        self.proj = nn.Linear(dim, dim)</span><br><span class="line">        self.proj_drop = nn.Dropout(proj_drop)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, N, C = x.shape</span><br><span class="line">        qkv = self.qkv(x).reshape(B, N, <span class="number">3</span>, self.num_heads, C // self.num_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        q, k, v = qkv.unbind(<span class="number">0</span>)   <span class="comment"># make torchscript happy (cannot use tensor as tuple)</span></span><br><span class="line"></span><br><span class="line">        attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) * self.scale<span class="comment"># QK</span></span><br><span class="line">        attn = attn.softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        attn = self.attn_drop(attn)</span><br><span class="line"></span><br><span class="line">        x = (attn @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(B, N, C)<span class="comment"># 乘以权重</span></span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        x = self.proj_drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h3 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Mlp</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; MLP as used in Vision Transformer, MLP-Mixer and related networks</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_features, hidden_features=<span class="literal">None</span>, out_features=<span class="literal">None</span>, act_layer=nn.GELU, bias=<span class="literal">True</span>, drop=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        out_features = out_features <span class="keyword">or</span> in_features</span><br><span class="line">        hidden_features = hidden_features <span class="keyword">or</span> in_features</span><br><span class="line">        bias = to_2tuple(bias)</span><br><span class="line">        drop_probs = to_2tuple(drop)</span><br><span class="line"></span><br><span class="line">        self.fc1 = nn.Linear(in_features, hidden_features, bias=bias[<span class="number">0</span>])</span><br><span class="line">        self.act = act_layer()</span><br><span class="line">        self.drop1 = nn.Dropout(drop_probs[<span class="number">0</span>])</span><br><span class="line">        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias[<span class="number">1</span>])</span><br><span class="line">        self.drop2 = nn.Dropout(drop_probs[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.act(x)</span><br><span class="line">        x = self.drop1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.drop2(x)</span><br><span class="line">        <span class="keyword">return</span> </span><br></pre></td></tr></table></figure>

<h3 id="Block"><a href="#Block" class="headerlink" title="Block"></a>Block</h3><p>实现功能：ViT的每个Block包括一层Attention和一层MLP。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Block</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">            self,</span></span><br><span class="line"><span class="params">            dim,</span></span><br><span class="line"><span class="params">            num_heads,</span></span><br><span class="line"><span class="params">            mlp_ratio=<span class="number">4.</span>,</span></span><br><span class="line"><span class="params">            qkv_bias=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">            drop=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">            attn_drop=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">            init_values=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">            drop_path=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">            act_layer=nn.GELU,</span></span><br><span class="line"><span class="params">            norm_layer=nn.LayerNorm</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.norm1 = norm_layer(dim)</span><br><span class="line">        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)</span><br><span class="line">        self.ls1 = LayerScale(dim, init_values=init_values) <span class="keyword">if</span> init_values <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        <span class="comment"># <span class="doctag">NOTE:</span> drop path for stochastic depth, we shall see if this is better than dropout here</span></span><br><span class="line">        self.drop_path1 = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">        self.norm2 = norm_layer(dim)</span><br><span class="line">        self.mlp = Mlp(in_features=dim, hidden_features=<span class="built_in">int</span>(dim * mlp_ratio), act_layer=act_layer, drop=drop)</span><br><span class="line">        self.ls2 = LayerScale(dim, init_values=init_values) <span class="keyword">if</span> init_values <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        self.drop_path2 = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))</span><br><span class="line">        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>







]]></content>
      <tags>
        <tag>经典模型</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：IDPT, Interconnected Dual Pyramid Transformer for Face Super-Resolution</title>
    <url>/2022/08/14/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AIDPT/</url>
    <content><![CDATA[<ul>
<li><p>研究问题：人脸超分辨率技术</p>
</li>
<li><p>FSR：关注于恢复重要的面部结构</p>
</li>
<li><p>创新点：提出了一个新的、有效的基于Transformer的人脸超分辨率架构</p>
<ol>
<li>设计了金字塔结构的encode&#x2F;decoder的Transformer架构：分别提取粗糙纹理和精细纹理。</li>
<li>通过一个底部的金字塔特征提取器，将双重金字塔Transformer建立起联系。</li>
<li>在每个spatial layer插入一个新的融合调制模块：使用粗糙纹理完善对应的精细纹理，融合浅层的粗糙纹理和对应的深层的精细纹理。</li>
</ol>
</li>
<li><p>FSR研究现状</p>
<ol>
<li>现有技术在解决超低分辨率问题上表现很差</li>
<li>卷积难以描述不同域间的关联和捕捉远域间的依赖</li>
</ol>
</li>
<li><p>网络结构： </p>
<img src="/2022/08/14/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AIDPT/1.png" alt="image-20220821153651772" style="zoom:80%;"></li>
</ul>
]]></content>
      <tags>
        <tag>FSR</tag>
        <tag>组内文章</tag>
      </tags>
  </entry>
  <entry>
    <title>经典模型：Swin Transformer</title>
    <url>/2022/09/26/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9ASwin-Transformer/</url>
    <content><![CDATA[<h2 id="模型结构与代码实现">模型结构与代码实现</h2>
<ul>
<li>参考博客：<a href="https://blog.csdn.net/qq_39478403/article/details/120042232?spm=1001.2014.3001.5506">https://blog.csdn.net/qq_39478403/article/details/120042232?spm=1001.2014.3001.5506</a></li>
<li>code：<a href="https://github.com/microsoft/Swin-Transformer">https://github.com/microsoft/Swin-Transformer</a></li>
</ul>
<p>Swin（即Shifted Windows） Transformer可以作为CV的一种通用主干，用在分类、检测、语义分割等多种视觉任务上。</p>
<p>Swin Transformer的提出解决了ViT具有的以下两个问题：</p>
<ol>
<li>ViT中，由于每个token的size大小相同，难以捕捉<strong>多尺度</strong>信息。</li>
<li>ViT的自注意力<strong>计算复杂度</strong>是图像大小的二次方。</li>
</ol>
<p>Swin-T构造了层次化特征图，并将自注意力的计算复杂度降为线性相关。</p>
<h3 id="整体结构">整体结构</h3>
<p>Swin-T的整体架构如下图所示：</p>
<img src="https://img-blog.csdnimg.cn/20210908164930810.png" alt="img" style="zoom:100%; margin:auto;">
<h3 id="Patch-Partition">Patch Partition</h3>
<p>对于每个为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>×</mo><mi>W</mi><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">H \times W \times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span>的输入，划分为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>×</mo><mn>4</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">4 \times 4 \times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span>大小的patch，每张图像被拆分为个patches，将每个patch展平作为一个token。</p>
<h3 id="Linear-Embedding">Linear Embedding</h3>
<p>即一个全连接层，将每个大小为48的token映射到设定的维度C，此时，每张图片的输入变为了<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>H</mi><mn>4</mn></mfrac><mo>×</mo><mfrac><mi>W</mi><mn>4</mn></mfrac><mo>×</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">\frac{H}{4} \times \frac{W}{4} \times C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>，然后输入Swin Transformer Block。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Patch Partition + Linear Embedding</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Image to Patch Embedding</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        img_size (int): Image size.  Default: 224.</span></span><br><span class="line"><span class="string">        patch_size (int): Patch token size. Default: 4.</span></span><br><span class="line"><span class="string">        in_chans (int): Number of input image channels. Default: 3.</span></span><br><span class="line"><span class="string">        embed_dim (int): Number of linear projection output channels. Default: 96.</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module, optional): Normalization layer. Default: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">4</span>, in_chans=<span class="number">3</span>, embed_dim=<span class="number">96</span>, norm_layer=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        img_size = to_2tuple(img_size)</span><br><span class="line">        patch_size = to_2tuple(patch_size)</span><br><span class="line">        patches_resolution = [img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>], img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>]]</span><br><span class="line">        self.img_size = img_size</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.patches_resolution = patches_resolution</span><br><span class="line">        self.num_patches = patches_resolution[<span class="number">0</span>] * patches_resolution[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        self.in_chans = in_chans</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">		<span class="comment"># 通过一个2维卷积实现patch partition与linear embedding</span></span><br><span class="line">        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br><span class="line">        <span class="keyword">if</span> norm_layer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.norm = norm_layer(embed_dim)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.norm = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, C, H, W = x.shape</span><br><span class="line">        <span class="comment"># FIXME look at relaxing size constraints</span></span><br><span class="line">        <span class="keyword">assert</span> H == self.img_size[<span class="number">0</span>] <span class="keyword">and</span> W == self.img_size[<span class="number">1</span>], \</span><br><span class="line">            <span class="string">f&quot;Input image size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) doesn&#x27;t match model (<span class="subst">&#123;self.img_size[<span class="number">0</span>]&#125;</span>*<span class="subst">&#123;self.img_size[<span class="number">1</span>]&#125;</span>).&quot;</span></span><br><span class="line">        x = self.proj(x).flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># 先划分patch与投影，再展平与交换</span></span><br><span class="line">        <span class="keyword">if</span> self.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = self.norm(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="Swin-Transformer-Block">Swin Transformer Block</h3>
<p>对于Transformers中使用的全局自注意力机制，需要计算每个token与其它所有tokens间的关系，计算复杂度为<strong>token数的平方</strong>。不适用于对大量tokens进行密集预测或表示高分辨率图像等视觉问题。</p>
<h4 id="W-MSA">W-MSA</h4>
<p>Swin-T通过<strong>在局部窗口中计算自注意力</strong>，将计算复杂度降低为token数的线性关系，设每个非重叠局部窗口中包含<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>×</mo><mi>M</mi></mrow><annotation encoding="application/x-tex">M\times M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span>个tokens。</p>
<ul>
<li>MSA：有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">hw</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">h</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span>个tokens，每个token在全局计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">hw</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">h</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span>次；</li>
<li>W-MSA：有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">hw</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">h</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span>个tokens，每个token在全局计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>M</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">M^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>次。</li>
</ul>
<p>首先将输入划分为若干个大小为MxM的窗口。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">window_partition</span>(<span class="params">x, window_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        x: (B, H, W, C)</span></span><br><span class="line"><span class="string">        window_size (int): window size</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        windows: (num_windows*B, window_size, window_size, C)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    B, H, W, C = x.shape</span><br><span class="line">    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)</span><br><span class="line">    windows = x.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>).contiguous().view(-<span class="number">1</span>, window_size, window_size, C)</span><br><span class="line">    <span class="keyword">return</span> windows</span><br></pre></td></tr></table></figure>
<h4 id="SW-MSA">SW-MSA</h4>
<p>W-MSA限制了跨窗口token间的交流与联系，从而限制了建模表征能力。作者提出了一种<strong>移位窗口划分</strong>方法SW-MSA，在模型中交替使用两种MSA方法（因此每个stage中Swin Transformer Block的数量都为偶数）。</p>
<p>所谓的移动窗口即将窗口循环位移，如下图所示：</p>
<img src="https://img-blog.csdnimg.cn/20210908164652285.png" alt="img" style="zoom:100%; margin:auto;">
<p>但直接移位得到的窗口大小是不规则的，不利于并行计算，同时9个窗口也提升了计算成本。为了解决这个问题，将重新划分后的窗口进行拼接，如下图所示，得到4个窗口。</p>
<img src="https://img-blog.csdnimg.cn/2021090920295156.png" alt="img" style="zoom:130%; margin:auto;">
<p>4个窗口中来自不同初始位置的patch不应进行自注意计算，因此使用mask机制，将不需要的注意力图置0。</p>
<p>W-MSA和SW-MSA公用一块代码实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">WindowAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Window based multi-head self attention (W-MSA) module with relative position bias.</span></span><br><span class="line"><span class="string">    It supports both of shifted and non-shifted window.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        window_size (tuple[int]): The height and width of the window.</span></span><br><span class="line"><span class="string">        num_heads (int): Number of attention heads.</span></span><br><span class="line"><span class="string">        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True</span></span><br><span class="line"><span class="string">        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set</span></span><br><span class="line"><span class="string">        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0</span></span><br><span class="line"><span class="string">        proj_drop (float, optional): Dropout ratio of output. Default: 0.0</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, window_size, num_heads, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">None</span>, attn_drop=<span class="number">0.</span>, proj_drop=<span class="number">0.</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.window_size = window_size  <span class="comment"># Wh, Ww</span></span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        head_dim = dim // num_heads</span><br><span class="line">        self.scale = qk_scale <span class="keyword">or</span> head_dim ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># define a parameter table of relative position bias</span></span><br><span class="line">        self.relative_position_bias_table = nn.Parameter(</span><br><span class="line">            torch.zeros((<span class="number">2</span> * window_size[<span class="number">0</span>] - <span class="number">1</span>) * (<span class="number">2</span> * window_size[<span class="number">1</span>] - <span class="number">1</span>), num_heads))  <span class="comment"># 2*Wh-1 * 2*Ww-1, nH</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># get pair-wise relative position index for each token inside the window</span></span><br><span class="line">        coords_h = torch.arange(self.window_size[<span class="number">0</span>])</span><br><span class="line">        coords_w = torch.arange(self.window_size[<span class="number">1</span>])</span><br><span class="line">        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  <span class="comment"># 2, Wh, Ww</span></span><br><span class="line">        coords_flatten = torch.flatten(coords, <span class="number">1</span>)  <span class="comment"># 2, Wh*Ww</span></span><br><span class="line">        relative_coords = coords_flatten[:, :, <span class="literal">None</span>] - coords_flatten[:, <span class="literal">None</span>, :]  <span class="comment"># 2, Wh*Ww, Wh*Ww</span></span><br><span class="line">        relative_coords = relative_coords.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).contiguous()  <span class="comment"># Wh*Ww, Wh*Ww, 2</span></span><br><span class="line">        relative_coords[:, :, <span class="number">0</span>] += self.window_size[<span class="number">0</span>] - <span class="number">1</span>  <span class="comment"># shift to start from 0</span></span><br><span class="line">        relative_coords[:, :, <span class="number">1</span>] += self.window_size[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">        relative_coords[:, :, <span class="number">0</span>] *= <span class="number">2</span> * self.window_size[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">        relative_position_index = relative_coords.<span class="built_in">sum</span>(-<span class="number">1</span>)  <span class="comment"># Wh*Ww, Wh*Ww</span></span><br><span class="line">        self.register_buffer(<span class="string">&quot;relative_position_index&quot;</span>, relative_position_index)</span><br><span class="line"></span><br><span class="line">        self.qkv = nn.Linear(dim, dim * <span class="number">3</span>, bias=qkv_bias)</span><br><span class="line">        self.attn_drop = nn.Dropout(attn_drop)</span><br><span class="line">        self.proj = nn.Linear(dim, dim)</span><br><span class="line">        self.proj_drop = nn.Dropout(proj_drop)</span><br><span class="line"></span><br><span class="line">        trunc_normal_(self.relative_position_bias_table, std=<span class="number">.02</span>)</span><br><span class="line">        self.softmax = nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: input features with shape of (num_windows*B, N, C)</span></span><br><span class="line"><span class="string">            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        B_, N, C = x.shape</span><br><span class="line">        qkv = self.qkv(x).reshape(B_, N, <span class="number">3</span>, self.num_heads, C // self.num_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        q, k, v = qkv[<span class="number">0</span>], qkv[<span class="number">1</span>], qkv[<span class="number">2</span>]  <span class="comment"># make torchscript happy (cannot use tensor as tuple)</span></span><br><span class="line"></span><br><span class="line">        q = q * self.scale</span><br><span class="line">        attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">		<span class="comment"># 相对位置偏移</span></span><br><span class="line">        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-<span class="number">1</span>)].view(</span><br><span class="line">            self.window_size[<span class="number">0</span>] * self.window_size[<span class="number">1</span>], self.window_size[<span class="number">0</span>] * self.window_size[<span class="number">1</span>], -<span class="number">1</span>)  <span class="comment"># Wh*Ww,Wh*Ww,nH</span></span><br><span class="line">        relative_position_bias = relative_position_bias.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>).contiguous()  <span class="comment"># nH, Wh*Ww, Wh*Ww</span></span><br><span class="line">        attn = attn + relative_position_bias.unsqueeze(<span class="number">0</span>)</span><br><span class="line">		<span class="comment"># 判断是否需要mask</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            nW = mask.shape[<span class="number">0</span>]</span><br><span class="line">            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">            attn = attn.view(-<span class="number">1</span>, self.num_heads, N, N)</span><br><span class="line">            attn = self.softmax(attn)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            attn = self.softmax(attn)</span><br><span class="line"></span><br><span class="line">        attn = self.attn_drop(attn)</span><br><span class="line"></span><br><span class="line">        x = (attn @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(B_, N, C)</span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        x = self.proj_drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h4 id="相对位置偏置">相对位置偏置</h4>
<p>在计算自注意力时，在计算相似度的过程中对每个head加入相对位置偏置，如下所示：</p>
<p><img src="https://img-blog.csdnimg.cn/20210911231011149.png" alt="img"></p>
<ul>
<li>对于预训练中学到的相对位置偏置，可以通过双三次插值初始化具有不同窗口大小的微调模型。</li>
</ul>
<h3 id="Patch-Merging">Patch Merging</h3>
<p>Patch Merging层的功能是产生一个层次化表示，通过<strong>合并相邻的tokens</strong>，减少tokens的数目。</p>
<img src="https://img-blog.csdnimg.cn/20210913201833609.png" alt="img" style="zoom:120%; margin:auto; ">
<p>对于Stage1和Stage2间的Patch Merging层，将原维度为C的token合并为大小为4C的token，再使用一个线性层将输出维度降低为2C，token的数目降低为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>H</mi><mn>8</mn></mfrac><mo>×</mo><mfrac><mi>W</mi><mn>8</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{H}{8} \times \frac{W}{8}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">8</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">8</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>。</p>
<p>在之后的每个stage中，都会改变张量的维度，从而形成一种层次化的特征。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchMerging</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Patch Merging Layer.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        input_resolution (tuple[int]): Resolution of input feature.</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_resolution, dim, norm_layer=nn.LayerNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.input_resolution = input_resolution</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.reduction = nn.Linear(<span class="number">4</span> * dim, <span class="number">2</span> * dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.norm = norm_layer(<span class="number">4</span> * dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        x: B, H*W, C</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        H, W = self.input_resolution</span><br><span class="line">        B, L, C = x.shape</span><br><span class="line">        <span class="keyword">assert</span> L == H * W, <span class="string">&quot;input feature has wrong size&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> H % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> W % <span class="number">2</span> == <span class="number">0</span>, <span class="string">f&quot;x size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) are not even.&quot;</span></span><br><span class="line"></span><br><span class="line">        x = x.view(B, H, W, C)<span class="comment"># 把输入整形为BHWC</span></span><br><span class="line"></span><br><span class="line">        x0 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x1 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x2 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x3 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x = torch.cat([x0, x1, x2, x3], -<span class="number">1</span>)  <span class="comment"># B H/2 W/2 4*C</span></span><br><span class="line">        x = x.view(B, -<span class="number">1</span>, <span class="number">4</span> * C)  <span class="comment"># B H/2*W/2 4*C</span></span><br><span class="line"></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        x = self.reduction(x)<span class="comment"># 降低维度为原来的1/2</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>经典模型</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Model Behavior Preserving for Class-Incremental Learning</title>
    <url>/2022/08/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AModel-Behavior-Preserving-for-Class-Incremental-Learning/</url>
    <content><![CDATA[<ul>
<li>发表时间：2022</li>
<li>研究内容：类增量学习，探讨在增量学习中应保留旧模型的哪些功能性属性。</li>
<li>研究现状：现有的增量学习方法忽略了CNN模型响应间的内部结构，KD的硬约束导致新模型出现混沌行为。</li>
<li>创新点：<ol>
<li><p>Feature Space：设计了一个INP Loss保持成对实例在旧模型上的<strong>相似性顺序</strong>（反映实例集间的相邻关系）；</p>
<p>INP用于惩罚新模型在学习过程中每个实例相邻关系的变化。</p>
<img src="/2022/08/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AModel-Behavior-Preserving-for-Class-Incremental-Learning/image-20220826093609961.png" alt="image-20220826093609961" style="zoom:67%;margin:auto;">

<p>a. 旧实例A在特征空间中与其它实例的相邻关系；</p>
<p>b. 采用传统的KD，引入新实例G后，绝对位置的微小变化被严格限制；</p>
<p>c. 采用INP Loss当相对位置没变时，就不会限制更新。</p>
</li>
<li><p>Label Space：设计了一个LPP Loss在输出空间的实例标签概率向量中保留<strong>标签排名列表</strong>（反映实例属于每一类的排名）；</p>
</li>
<li><p>介绍了一种可导的排名计算方法用于计算上述Loss。</p>
</li>
</ol>
</li>
</ul>
]]></content>
      <tags>
        <tag>组内文章</tag>
        <tag>连续学习</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：PhysFormer: Facial Video-based Physiological Measurement with Temporal Difference Transformer</title>
    <url>/2022/09/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9APhysFormer-Facial-Video-based-Physiological-Measurement-with-Temporal-Difference-Transformer/</url>
    <content><![CDATA[<ul>
<li><p>发表时间：2021</p>
</li>
<li><p>研究对象：rPPG，使用多波长 RGB 相机检测人体皮肤表面脉冲引起的细微颜色变化，实现测量心脏活动和其它生理信号。</p>
</li>
<li><p>研究意义：传统的检测方法会造成discomfort，并且长期检测不方便</p>
</li>
<li><p>Code：<a href="https://github.com/ZitongYu/PhysFormer">https://github.com/ZitongYu/PhysFormer</a></p>
</li>
<li><p>rPPG研究历史：</p>
<ol>
<li>早期使用经典的信号处理方法检测面部细微的颜色变化；</li>
<li>使用非端到端方法，首先生成预处理的信号特征，然后模型从这些特征图中捕捉rPPG特征（对预处理要求严格，忽略了全局特征）；</li>
<li>端到端的基于深度学习的方法（容易被复杂的背景信息影响）。</li>
</ol>
</li>
<li><p>研究现状：</p>
<p>​        现有的基于卷积神经网络的模型在时间和空间上的感受野受限，忽略了长期的时间和空间上的互动与感知。</p>
</li>
<li><p>PhysFormer网络架构：</p>
<p><img src="/2022/09/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9APhysFormer-Facial-Video-based-Physiological-Measurement-with-Temporal-Difference-Transformer/image-20220919092041506.png" alt="image-20220919092041506" style="zoom:80%;"></p>
<ol>
<li><p><strong>Stem</strong>：由三个卷积块组成，用于提取粗糙的局部时空特征</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># INPUT:[B,3,T,H,W]</span></span><br><span class="line"><span class="comment"># OUTPUT:[B,D,T,H/8,W/8]</span></span><br><span class="line"><span class="comment"># use</span></span><br><span class="line">x = self.Stem0(x)</span><br><span class="line">x = self.Stem1(x)</span><br><span class="line">x = self.Stem2(x)  <span class="comment"># [B, 64, 160, 64, 64]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># implement</span></span><br><span class="line">self.Stem0 = nn.Sequential(</span><br><span class="line">    nn.Conv3d(<span class="number">3</span>, dim//<span class="number">4</span>, [<span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>], stride=<span class="number">1</span>, padding=[<span class="number">0</span>,<span class="number">2</span>,<span class="number">2</span>]),</span><br><span class="line">    nn.BatchNorm3d(dim//<span class="number">4</span>),</span><br><span class="line">    nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">    nn.MaxPool3d((<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>), stride=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">self.Stem1 = nn.Sequential(</span><br><span class="line">    nn.Conv3d(dim//<span class="number">4</span>, dim//<span class="number">2</span>, [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.BatchNorm3d(dim//<span class="number">2</span>),</span><br><span class="line">    nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">    nn.MaxPool3d((<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>), stride=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">self.Stem2 = nn.Sequential(</span><br><span class="line">    nn.Conv3d(dim//<span class="number">2</span>, dim, [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.BatchNorm3d(dim),</span><br><span class="line">    nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">    nn.MaxPool3d((<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>), stride=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Tube Tokens</strong>：将stem输出划分为若干个时空tube token，将时空邻近语义聚合在一起，并减少后续transformer的计算量。通过一个3D卷积实现。</p>
<p>tip：在嵌入后没有额外加上位置编码，因为在stem里已经捕捉了相关的时空信息。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># INPUT: # [B, 64, 160, 64, 64] ([B,D,T,H/8,W/8])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># use</span></span><br><span class="line">x = self.patch_embedding(x)  <span class="comment"># [B, 64, 40, 4, 4]</span></span><br><span class="line">x = x.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># [B, 40*4*4, 64](B,N,D)</span></span><br><span class="line"><span class="comment"># implement</span></span><br><span class="line"><span class="comment"># Patch embedding    [4x16x16]conv</span></span><br><span class="line">self.patch_embedding = nn.Conv3d(dim, dim, kernel_size=(ft, fh, fw), stride=(ft, fh, fw))</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Temporal Difference Multi-head Self-attention</strong>：</p>
<p>与传统的自注意力机制不同，使用TDC（Temporal Difference Convolution）查询Q和K的投影，可以捕捉局部细粒度的时间差异特征。</p>
<p><img src="/2022/09/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9APhysFormer-Facial-Video-based-Physiological-Measurement-with-Temporal-Difference-Transformer/image-20221002170008240.png" alt="image-20221002170008240" style="zoom:80%; margin:auto;"></p>
<p><img src="/2022/09/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9APhysFormer-Facial-Video-based-Physiological-Measurement-with-Temporal-Difference-Transformer/image-20221002170042081.png" alt="image-20221002170042081" style="zoom: 67%; margin:auto;"></p>
<p><img src="/2022/09/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9APhysFormer-Facial-Video-based-Physiological-Measurement-with-Temporal-Difference-Transformer/image-20221002170158926.png" alt="image-20221002170158926" style="zoom:67%; margin:auto;"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 多头自注意力机制实现</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadedSelfAttention_TDC_gra_sharp</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Multi-Headed Dot Product Attention with depth-wise Conv3d&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, num_heads, dropout, theta</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        self.proj_q = nn.Sequential(</span><br><span class="line">            CDC_T(dim, dim, <span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">False</span>, theta=theta),  </span><br><span class="line">            nn.BatchNorm3d(dim),</span><br><span class="line">            <span class="comment">#nn.ELU(),</span></span><br><span class="line">        )</span><br><span class="line">        self.proj_k = nn.Sequential(</span><br><span class="line">            CDC_T(dim, dim, <span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">False</span>, theta=theta),  </span><br><span class="line">            nn.BatchNorm3d(dim),</span><br><span class="line">            <span class="comment">#nn.ELU(),</span></span><br><span class="line">        )</span><br><span class="line">        self.proj_v = nn.Sequential(</span><br><span class="line">            nn.Conv3d(dim, dim, <span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, groups=<span class="number">1</span>, bias=<span class="literal">False</span>),  </span><br><span class="line">            <span class="comment">#nn.BatchNorm3d(dim),</span></span><br><span class="line">            <span class="comment">#nn.ELU(),</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.drop = nn.Dropout(dropout)</span><br><span class="line">        self.n_heads = num_heads</span><br><span class="line">        self.scores = <span class="literal">None</span> <span class="comment"># for visualization</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, gra_sharp</span>):    <span class="comment"># [B, 4*4*40, 128]</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        x, q(query), k(key), v(value) : (B(batch_size), S(seq_len), D(dim))</span></span><br><span class="line"><span class="string">        mask : (B(batch_size) x S(seq_len))</span></span><br><span class="line"><span class="string">        * split D(dim) into (H(n_heads), W(width of head)) ; D = H * W</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># (B, S, D) -proj-&gt; (B, S, D) -split-&gt; (B, S, H, W) -trans-&gt; (B, H, S, W)</span></span><br><span class="line">        </span><br><span class="line">        [B, P, C]=x.shape</span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).view(B, C, P//<span class="number">16</span>, <span class="number">4</span>, <span class="number">4</span>)      <span class="comment"># [B, dim, 40, 4, 4]</span></span><br><span class="line">        q, k, v = self.proj_q(x), self.proj_k(x), self.proj_v(x)<span class="comment"># 由CDC计算得到Q和K，3D卷积得到V</span></span><br><span class="line">        q = q.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># [B, 4*4*40, dim]</span></span><br><span class="line">        k = k.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># [B, 4*4*40, dim]</span></span><br><span class="line">        v = v.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># [B, 4*4*40, dim]</span></span><br><span class="line">        </span><br><span class="line">        q, k, v = (split_last(x, (self.n_heads, -<span class="number">1</span>)).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="keyword">for</span> x <span class="keyword">in</span> [q, k, v])</span><br><span class="line">        <span class="comment"># (B, H, S, W) @ (B, H, W, S) -&gt; (B, H, S, S) -softmax-&gt; (B, H, S, S)</span></span><br><span class="line">        scores = q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>) / gra_sharp<span class="comment"># 矩阵相乘，计算得分（权重）</span></span><br><span class="line">		<span class="comment"># tip：在python里，“@”表示数学上的矩阵相乘，“*”表示矩阵对应位置两元素相乘</span></span><br><span class="line">        </span><br><span class="line">        scores = self.drop(F.softmax(scores, dim=-<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># (B, H, S, S) @ (B, H, S, W) -&gt; (B, H, S, W) -trans-&gt; (B, S, H, W)</span></span><br><span class="line">        h = (scores @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous()<span class="comment"># 计算结果</span></span><br><span class="line">        <span class="comment"># -merge-&gt; (B, S, D)</span></span><br><span class="line">        h = merge_last(h, <span class="number">2</span>)</span><br><span class="line">        self.scores = scores</span><br><span class="line">        <span class="keyword">return</span> h, scores</span><br><span class="line"><span class="comment"># TDC</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CDC_T</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 padding=<span class="number">1</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">False</span>, theta=<span class="number">0.6</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(CDC_T, self).__init__()</span><br><span class="line">        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,                                   padding=padding, dilation=dilation, groups=groups, bias=bias)</span><br><span class="line">        self.theta = theta</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out_normal = self.conv(x)<span class="comment"># 正常3D卷积输出</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> math.fabs(self.theta - <span class="number">0.0</span>) &lt; <span class="number">1e-8</span>:</span><br><span class="line">            <span class="keyword">return</span> out_normal</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># pdb.set_trace()</span></span><br><span class="line">            [C_out, C_in, t, kernel_size, kernel_size] = self.conv.weight.shape</span><br><span class="line"></span><br><span class="line">            <span class="comment"># only CD works on temporal kernel size&gt;1</span></span><br><span class="line">            <span class="keyword">if</span> self.conv.weight.shape[<span class="number">2</span>] &gt; <span class="number">1</span>:</span><br><span class="line">                kernel_diff = self.conv.weight[:, :, <span class="number">0</span>, :, :].<span class="built_in">sum</span>(<span class="number">2</span>).<span class="built_in">sum</span>(<span class="number">2</span>) + self.conv.weight[:, :,                               <span class="number">2</span>, :, :].<span class="built_in">sum</span>(<span class="number">2</span>).<span class="built_in">sum</span>(<span class="number">2</span>)</span><br><span class="line">                kernel_diff = kernel_diff[:, :, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">                <span class="comment"># 时间差异项</span></span><br><span class="line">                out_diff = F.conv3d(<span class="built_in">input</span>=x, weight=kernel_diff, bias=self.conv.bias,                                                     stride=self.conv.stride, padding=<span class="number">0</span>, dilation=self.conv.dilation,                                     groups=self.conv.groups)</span><br><span class="line">                <span class="keyword">return</span> out_normal - self.theta * out_diff</span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Spatio-temporal Feed-forward</strong>：在常用的两层线性transformation层之间，引入基于深度的3D卷积，ST-FF可以改善局部不一致性和部分噪声特征，同时丰富的局部性为TD-MHSA提供足够的相对位置信息，从而实现性能提升。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># use</span></span><br><span class="line"><span class="comment"># Transformer</span></span><br><span class="line">self.transformer1 = Transformer_ST_TDC_gra_sharp(num_layers=num_layers//<span class="number">3</span>, dim=dim, 		                             num_heads=num_heads, ff_dim=ff_dim, dropout=dropout_rate, theta=theta)</span><br><span class="line"><span class="comment"># Transformer</span></span><br><span class="line">self.transformer2 = Transformer_ST_TDC_gra_sharp(num_layers=num_layers//<span class="number">3</span>, dim=dim,                                       num_heads=num_heads, ff_dim=ff_dim, dropout=dropout_rate, theta=theta)</span><br><span class="line"><span class="comment"># Transformer</span></span><br><span class="line">self.transformer3 = Transformer_ST_TDC_gra_sharp(num_layers=num_layers//<span class="number">3</span>, dim=dim,                                       num_heads=num_heads, ff_dim=ff_dim, dropout=dropout_rate, theta=theta)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer_ST_TDC_gra_sharp</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Transformer with Self-Attentive Blocks&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_layers, dim, num_heads, ff_dim, dropout, theta</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.blocks = nn.ModuleList([</span><br><span class="line">            Block_ST_TDC_gra_sharp(dim, num_heads, ff_dim, dropout, theta) <span class="keyword">for</span> _ <span class="keyword">in</span>                                                      <span class="built_in">range</span>(num_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, gra_sharp</span>):</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self.blocks:</span><br><span class="line">            x, Score = block(x, gra_sharp)</span><br><span class="line">        <span class="keyword">return</span> x, Score</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Block_ST_TDC_gra_sharp</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Transformer Block&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, num_heads, ff_dim, dropout, theta</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.attn = MultiHeadedSelfAttention_TDC_gra_sharp(dim, num_heads, dropout, theta)</span><br><span class="line">        self.proj = nn.Linear(dim, dim)</span><br><span class="line">        self.norm1 = nn.LayerNorm(dim, eps=<span class="number">1e-6</span>)</span><br><span class="line">        self.pwff = PositionWiseFeedForward_ST(dim, ff_dim)<span class="comment"># ST-FF</span></span><br><span class="line">        self.norm2 = nn.LayerNorm(dim, eps=<span class="number">1e-6</span>)</span><br><span class="line">        self.drop = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, gra_sharp</span>):</span><br><span class="line">        Atten, Score = self.attn(self.norm1(x), gra_sharp)</span><br><span class="line">        h = self.drop(self.proj(Atten))</span><br><span class="line">        x = x + h</span><br><span class="line">        h = self.drop(self.pwff(self.norm2(x)))</span><br><span class="line">        x = x + h</span><br><span class="line">        <span class="keyword">return</span> x, Score</span><br><span class="line">    </span><br><span class="line"><span class="comment"># implement</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionWiseFeedForward_ST</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;FeedForward Neural Networks for each position&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, ff_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        self.fc1 = nn.Sequential(</span><br><span class="line">            nn.Conv3d(dim, ff_dim, <span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>),  </span><br><span class="line">            nn.BatchNorm3d(ff_dim),</span><br><span class="line">            nn.ELU(),</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.STConv = nn.Sequential(</span><br><span class="line">            nn.Conv3d(ff_dim, ff_dim, <span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, groups=ff_dim, bias=<span class="literal">False</span>),  </span><br><span class="line">            nn.BatchNorm3d(ff_dim),</span><br><span class="line">            nn.ELU(),</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.fc2 = nn.Sequential(</span><br><span class="line">            nn.Conv3d(ff_dim, dim, <span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>),  </span><br><span class="line">            nn.BatchNorm3d(dim),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):    <span class="comment"># [B, 4*4*40, 128]</span></span><br><span class="line">        [B, P, C]=x.shape</span><br><span class="line">        <span class="comment">#x = x.transpose(1, 2).view(B, C, 40, 4, 4)      # [B, dim, 40, 4, 4]</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).view(B, C, P//<span class="number">16</span>, <span class="number">4</span>, <span class="number">4</span>)      <span class="comment"># [B, dim, 40, 4, 4]</span></span><br><span class="line">        x = self.fc1(x)		              <span class="comment"># x [B, ff_dim, 40, 4, 4]</span></span><br><span class="line">        <span class="comment"># 使用时空卷积</span></span><br><span class="line">        x = self.STConv(x)		          <span class="comment"># x [B, ff_dim, 40, 4, 4]</span></span><br><span class="line">        x = self.fc2(x)		              <span class="comment"># x [B, dim, 40, 4, 4]</span></span><br><span class="line">        x = x.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># [B, 4*4*40, dim]</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (B, S, D) -&gt; (B, S, D_ff) -&gt; (B, S, D)</span></span><br><span class="line">        <span class="comment">#return self.fc2(F.gelu(self.fc1(x)))</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ul>
<ul>
<li><p>创新点：PhysFormer，一种端到端的视频transformer，联合使用了局部的和全局的时空特征。</p>
<ol>
<li>使用<strong>时差引导全局注意力机制</strong>，强化rPPG的周期性特征，针对干扰完善局部时空特征；</li>
<li>使用受label distribution learning和curriculum learning启发的频域动态约束，为PhysFormer提供详细的监督，缓解过拟合。</li>
<li>PhysFormer不需要像其它transformer网络那样在大规模数据集上预训练，仅在rPPG数据集训练即可。</li>
</ol>
</li>
<li><p><strong>Label Distribution Learning</strong>：对于面部的rPPG信号，心率相近的视频会有相似的周期性特征。为了使得模型学习到这种特征，将心率估计问题看作一个多分类问题，有多少个心率就有多少类别，类别概率向量由高斯分布组成。</p>
</li>
<li><p><strong>Curriculum Learning Guided Dynamic Loss</strong>：课程式学习是指模型从容易样本开始学习，逐步学习困难样本。在该任务中，从时域和频域两个方面限制模型学习，时域的限制更直接更容易学习，频域的限制较难学习，因此，使用动态的loss函数，逐步提高频域loss的比例。</p>
</li>
<li><p>实验：不同模型的对比实验、消融实验</p>
</li>
<li>Others：注意力图可视化</li>
</ul>
]]></content>
      <tags>
        <tag>Transformer</tag>
        <tag>组内文章</tag>
        <tag>rPPG</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Rethinking the ST-GCNs for 3D skeleton-based human action recognition</title>
    <url>/2022/09/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARethinking-the-ST-GCNs-for-3D-skeleton-based-human-action-recognition/</url>
    <content><![CDATA[<ul>
<li>发表时间：2021</li>
<li>ST-GCN：Spatial-Temporal Graph Convolutional Network，用于解决骨骼数据的动作识别问题。</li>
<li>研究内容：<ol>
<li>证明了在ST-GCN中很多操作对于人体动作识别是没必要的</li>
<li>提出了一个简单有效的策略捕捉全局图的相关性，对输入序列进行有效建模，同时将输入图序列降入欧几里得空间，可以使用多尺度时域滤波器捕捉动态信息。</li>
</ol>
</li>
<li>研究现状：<ol>
<li>骨骼数据成为人体动作识别的主流输入（与传统的RGB视频数据相比，信息更完整）</li>
<li>直接将结构化的数据重新排列，使得tensor适应基础的神经网络（由于骨骼数据中没有天然的局部性概念，深度学习的能力受到限制）</li>
<li>设计一种适应结构化数据的自定义神经网络（ST-GCN）</li>
</ol>
</li>
</ul>
<p>TBC：GCN好难，看不懂</p>
]]></content>
      <tags>
        <tag>组内文章</tag>
        <tag>人体动作识别</tag>
        <tag>GCN</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Structural Knowledge Organization and Transfer for Class-Incremental Learning</title>
    <url>/2022/08/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AStructural-Knowledge-Organization-and-Transfer-for-Class-Incremental-Learning/</url>
    <content><![CDATA[<ul>
<li><p>发表时间：2021</p>
</li>
<li><p>研究问题：类增量学习</p>
</li>
<li><p>研究现状：</p>
<ol>
<li><p><strong>经典的知识蒸馏方法</strong>忽略了信息点之间的关联，当新数据远多于旧数据时，面临着严重的偏差问题；</p>
<ul>
<li><p>KD：在特征空间中，孤立地限制单个训练样本的位置，样本间的关系可能会被改变，并导致分类错误。</p>
</li>
<li><p>SGKD：保持样本的结构化知识，包括样本的位置和样本间关系，确保蒸馏后样本仍能被正确地分类。</p>
</li>
</ul>
<img src="/2022/08/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AStructural-Knowledge-Organization-and-Transfer-for-Class-Incremental-Learning/image-20220822101131518.png" alt="image-20220822101131518" style="zoom:67%;margin:auto;"></li>
</ol>
</li>
<li><p>创新点：</p>
<ol>
<li><p>使用一个<strong>memory knowledge graph</strong>(MKG)表征历史任务的结构化知识</p>
<ul>
<li>在特征空间中的绝对位置（MKG中用顶点表示已知example间的特征向量）</li>
<li>example间对应关系（边表示，使用余弦距离）</li>
</ul>
</li>
<li><p>使用<strong>图插值机制</strong>丰富知识域、缓解类间样本不平衡问题</p>
<p>通过向MKG中插入假的顶点，扩充和平滑分散的数据集，假顶点通过mix两个真顶点的vector得到。</p>
</li>
<li><p>使用<strong>结构化图知识蒸馏（SGKD）</strong>迁移旧知识</p>
<ul>
<li>顶点蒸馏损失</li>
<li>边蒸馏损失</li>
</ul>
<img src="/2022/08/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AStructural-Knowledge-Organization-and-Transfer-for-Class-Incremental-Learning/image-20220823094630810.png" alt="image-20220823094630810" style="zoom:67%;margin:auto;"></li>
</ol>
</li>
<li><p>人脸识别是怎么实现增加新样本的？</p>
<p>人脸识别网络的本质是一个特征提取器，并不是分类器，识别人脸的时候，通过计算输出特征和人脸库中人脸的距离判断人脸所属对象。未涉及类增量学习。</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>连续学习</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Revisiting Pixel-Wise Supervision for Face Anti-Spoofing</title>
    <url>/2022/09/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARevisiting-Pixel-Wise-Supervision-for-Face-Anti-Spoofing/</url>
    <content><![CDATA[<ul>
<li><p>发表时间：2021</p>
</li>
<li><p>研究内容：像素级的人脸识别反欺诈方法</p>
</li>
<li><p>创新点：提出基于<strong>金字塔</strong>的监督方法，模型从多空间尺度上学习局部和全局的语义信息</p>
</li>
<li><p>Presentation Attack Detection研究历史：</p>
<ol>
<li><p>传统算法关注于<strong>活体</strong>和<strong>手工特征</strong>的检测，需要丰富的任务级的先验知识。</p>
<p>活体检测：关注眨眼、面部和头部动作、视线追踪以及远程生理信号（这种方法需要长期的互动，容易被video attacks伪造）。</p>
<p>经典的handcrafted descriptors：从多种色彩空间中提取有效的欺诈模式，这种PA方法可以通过训练分类器捕捉，但在遇到未见过的场景或未知的PAs时就失效了。</p>
</li>
<li></li>
<li></li>
</ol>
</li>
<li></li>
</ul>
]]></content>
      <tags>
        <tag>组内文章</tag>
        <tag>人脸识别</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Social Distancing Alert with Smartwatches</title>
    <url>/2022/09/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ASocial-Distancing-Alert-with-Smartwatches/</url>
    <content><![CDATA[<ul>
<li><p>发表时间：2022</p>
</li>
<li><p>研究内容：基于智能手表的社交距离警报系统SoDA，SoDA使用加速器和陀螺仪的数据和简单有效的视觉Transformer模型，识别违反社交距离的活动。</p>
</li>
<li><p>code：<a href="https://github.com/aiotgroup/SoDA">https://github.com/aiotgroup/SoDA</a></p>
</li>
<li><p>创新点：</p>
<ol>
<li>应用价值</li>
<li>创建了一个数据集</li>
<li>证明了ViT是一种有效的方法？</li>
</ol>
</li>
<li><p>模型结构：</p>
<img src="/2022/09/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ASocial-Distancing-Alert-with-Smartwatches/image-20220920104544756.png" alt="image-20220920104544756" style="zoom:80%;"></li>
</ul>
]]></content>
      <tags>
        <tag>Transformer</tag>
        <tag>组内文章</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络基础知识</title>
    <url>/2022/09/27/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<h2 id="3D-CNN"><a href="#3D-CNN" class="headerlink" title="3D CNN"></a>3D CNN</h2><p>参考博客：</p>
<p>​        <a href="https://blog.csdn.net/auto1993/article/details/70948249">https://blog.csdn.net/auto1993/article/details/70948249</a></p>
<p>​        <a href="https://blog.csdn.net/YOULANSHENGMENG/article/details/121328554">https://blog.csdn.net/YOULANSHENGMENG/article/details/121328554</a></p>
<p>使用3D CNN可以捕获视频中的<strong>时间</strong>和<strong>空间</strong>的特征信息。</p>
<h3 id="Conv3D"><a href="#Conv3D" class="headerlink" title="Conv3D"></a>Conv3D</h3><p>3D卷积：对于下面的采用3D卷积核进行的卷积操作，通过堆叠多个连续的帧组成一个立方体，在立方体中运用3D卷积核，卷积层中每一个特征map都与上一层中多个邻近的连续帧相连，以此捕捉运动信息。下图卷积操作的时间维度为3（对连续的三帧图像进行卷积操作）。</p>
<p><img src="https://img-blog.csdn.net/20170429133650515?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvQVVUTzE5OTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img" style="zoom:80%; margin:auto;"></p>
<p>tip：<strong>3D卷积核只能从cube中提取一种类型的特征</strong>（在整个cube中卷积核的权值是共享的），若要提取多种特征，可以采用多种卷积核。</p>
<p>在pytorch中，同样有Conv3D的实现，使用样例如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample intput | 随机输入</span></span><br><span class="line"><span class="comment"># (batch_size, channel, fram_size, height, width)</span></span><br><span class="line">net_input = torch.randn(<span class="number">32</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 所有维度同一个参数配置</span></span><br><span class="line">conv = nn.Conv3d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">net_output = conv(net_input)</span><br><span class="line"><span class="built_in">print</span>(net_output.shape)  <span class="comment"># shape=[32, 64, 5, 112, 112] | 相当于每一个维度上的卷积核大小都是3，步长都是2，pad都是1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 每一维度不同参数配置</span></span><br><span class="line">conv = nn.Conv3d(<span class="number">3</span>, <span class="number">64</span>, (<span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">net_output = conv(net_input)</span><br><span class="line"><span class="built_in">print</span>(net_output.shape) <span class="comment"># shape=[32, 64, 9, 112, 112]</span></span><br></pre></td></tr></table></figure>
<h2 id="差分卷积（Difference-Convolution）"><a href="#差分卷积（Difference-Convolution）" class="headerlink" title="差分卷积（Difference Convolution）"></a>差分卷积（Difference Convolution）</h2><p>参考博客：<a href="https://zhuanlan.zhihu.com/p/392986663">https://zhuanlan.zhihu.com/p/392986663</a></p>
<pre class="mermaid">graph LR
A[LBP]-->B[CDC]
B-->c[3D-CDC]</pre>

<p>空间差分特征具有如下两个优点：</p>
<ol>
<li>对于光照变化具有鲁棒性；</li>
<li>描述了细粒度的纹理信息。</li>
</ol>
<h3 id="LBP（Local-Binary-Patterns）"><a href="#LBP（Local-Binary-Patterns）" class="headerlink" title="LBP（Local Binary Patterns）"></a>LBP（Local Binary Patterns）</h3><p>LBP即局部二值模式，是一种经典的传统手工特征提取方法。</p>
<p>在3x3邻域，将周围像素点的灰度值与中心像素值进行比较，大于中心像素记为1，小于中心像素记为0。这样将产生8个二进制数，然后转换为十进制的LBP码，用LBP码反映该区域的纹理信息。</p>
<p><img src="https://pic3.zhimg.com/80/v2-8ba548951eb9a627c45e67a654641576_720w.webp" alt="img" style="zoom:100%; margin:auto;"></p>
<p>对于图像中的每个像素点，都对应一个LBP码，LBP码聚合了邻域内的差分信息，对光照变化较为鲁棒，同时描述了细粒度的纹理信息，早期在人脸识别中广泛使用。</p>
<h3 id="中心差分卷积（Central-Difference-Convolution）"><a href="#中心差分卷积（Central-Difference-Convolution）" class="headerlink" title="中心差分卷积（Central Difference Convolution）"></a>中心差分卷积（Central Difference Convolution）</h3><p>Q：为什么vanilla卷积不好使？</p>
<p>A：vanilla卷积是直接聚合局部的亮度级的信息，容易受到光照等因素影响，同时难以表征细粒度的特征。应用到活体检测任务中，受光照等因素影响会导致模型泛化能力变弱，难以表征细粒度的特征则会导致模型难以学到防伪本质的细节信息。根据上述描述，使用空间差分特征可以缓解vanilla卷积存在的问题。</p>
<p>CDC的工作原理如下图所示：</p>
<p><img src="https://pic4.zhimg.com/80/v2-c088862cb3f9790751f558ecac7b49c3_720w.webp" alt="img" style="zoom:100%; margin:auto;"></p>
<p>其数值形式的卷积过程如下所示，用区域内的像素与中心像素作差值，然后再进行卷积操作。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/19293651-b1bbd94eaca4a61b.png?imageMogr2/auto-orient/strip|imageView2/2/w/496/format/webp" alt="img" style="zoom:100%;"></p>
<p><img src="/2022/09/27/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/image-20221015151341198.png" alt="image-20221015151341198" style="zoom:80%; margin:auto;"></p>
<p>θ控制差分卷积的贡献，即gradient-level的信息。</p>
<p>其实现代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># https://github.com/ZitongYu/CDCN/blob/master/CVPR2020_paper_codes/models/CDCNs.py</span></span><br><span class="line"><span class="comment"># vanilla convolution</span></span><br><span class="line">out_normal = self.conv(x)</span><br><span class="line"><span class="comment"># central difference term</span></span><br><span class="line">[C_out,C_in, kernel_size,kernel_size] = self.conv.weight.shape</span><br><span class="line">kernel_diff = self.conv.weight.<span class="built_in">sum</span>(<span class="number">2</span>).<span class="built_in">sum</span>(<span class="number">2</span>)</span><br><span class="line">kernel_diff = kernel_diff[:, :, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">out_diff = F.conv2d(<span class="built_in">input</span>=x, weight=kernel_diff, bias=self.conv.bias, stride=self.conv.stride, padding=<span class="number">0</span>,                       groups=self.conv.groups)</span><br><span class="line"><span class="comment"># CDC OUTPUT</span></span><br><span class="line"><span class="keyword">return</span> out_normal - self.theta * out_diff</span><br></pre></td></tr></table></figure>
<h3 id="时空差分卷积（3D-CDC）"><a href="#时空差分卷积（3D-CDC）" class="headerlink" title="时空差分卷积（3D-CDC）"></a>时空差分卷积（3D-CDC）</h3><p>vanilla 3D的卷积操作难以感知细粒度的时空差异信息。Zitong Yu（这是什么怪物啊）设计了三种3D-CDC，用于不同场景下增强时域特征。</p>
<p><img src="https://pic4.zhimg.com/80/v2-6b6cbc16d5dd7fb0bc044b238e3a948f_720w.webp" alt="img" style="zoom:120%;"></p>
<ul>
<li>3D-CDC-ST：聚合局部时空区域内的所有中心差分信息，<strong>擅长动态纹理表征</strong>。</li>
<li>3D-CDC-T：聚合相邻帧间的局部时空区域内的中心差分信息，<strong>擅长捕捉精细的时域上下文信息</strong>。在PhysFormer里使用的就是这种差分卷积。</li>
<li>3D-CDC-TR：计算差分前采用temporal average pooling融合上下文信息，<strong>抗时域间噪声扰动</strong>。</li>
</ul>
<p>三种差分卷积的公式如下所示：</p>
<p><img src="/2022/09/27/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/image-20221015172029434.png" alt="image-20221015172029434" style="zoom: 80%; margin:auto;"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># https://github.com/ZitongYu/3DCDC-NAS/blob/master/3DCDC.py</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># vanilla</span></span><br><span class="line">self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,</span><br><span class="line">                      dilation=dilation, groups=groups, bias=bias)</span><br><span class="line">out_normal = self.conv(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># CDC_ST</span></span><br><span class="line">kernel_diff = self.conv.weight.<span class="built_in">sum</span>(<span class="number">2</span>).<span class="built_in">sum</span>(<span class="number">2</span>).<span class="built_in">sum</span>(<span class="number">2</span>)</span><br><span class="line">kernel_diff = kernel_diff[:, :, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">out_diff = F.conv3d(<span class="built_in">input</span>=x, weight=kernel_diff, bias=self.conv.bias, stride=self.conv.stride,</span><br><span class="line">                    padding=<span class="number">0</span>, dilation=self.conv.dilation, groups=self.conv.groups)</span><br><span class="line"><span class="keyword">return</span> out_normal - self.theta * out_diff</span><br><span class="line"></span><br><span class="line"><span class="comment"># CDC_T</span></span><br><span class="line">kernel_diff = self.conv.weight[:, :, <span class="number">0</span>, :, :].<span class="built_in">sum</span>(<span class="number">2</span>).<span class="built_in">sum</span>(<span class="number">2</span>) + self.conv.weight[:, :, <span class="number">2</span>, :, :].<span class="built_in">sum</span>(<span class="number">2</span>).<span class="built_in">sum</span>(<span class="number">2</span>)</span><br><span class="line">kernel_diff = kernel_diff[:, :, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">out_diff = F.conv3d(<span class="built_in">input</span>=x, weight=kernel_diff, bias=self.conv.bias, stride=self.conv.stride,</span><br><span class="line">                    padding=<span class="number">0</span>, dilation=self.conv.dilation, groups=self.conv.groups)</span><br><span class="line"><span class="keyword">return</span> out_normal - self.theta * out_diff</span><br><span class="line"></span><br><span class="line"><span class="comment"># CDC_TR</span></span><br><span class="line">local_avg = self.avgpool(x)</span><br><span class="line">kernel_diff = self.conv.weight[:, :, <span class="number">0</span>, :, :].<span class="built_in">sum</span>(<span class="number">2</span>).<span class="built_in">sum</span>(<span class="number">2</span>) + self.conv.weight[:, :, <span class="number">2</span>, :, :].<span class="built_in">sum</span>(<span class="number">2</span>).<span class="built_in">sum</span>(<span class="number">2</span>)</span><br><span class="line">kernel_diff = kernel_diff[:, :, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">out_diff = F.conv3d(<span class="built_in">input</span>=local_avg, weight=kernel_diff, bias=self.conv.bias, stride=self.conv.stride,</span><br><span class="line">                    padding=<span class="number">0</span>, groups=self.conv.groups)</span><br><span class="line"><span class="keyword">return</span> out_normal - self.theta * out_diff</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>神经网络基础知识</tag>
      </tags>
  </entry>
  <entry>
    <title>经典模型：Video Swin Transformer</title>
    <url>/2022/09/28/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AVideo-Swin-Transformer/</url>
    <content><![CDATA[<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>Video Swin Transformer的结构整体如下图所示，与Swin-T相比多了时间维度。</p>
<img src="/2022/09/28/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AVideo-Swin-Transformer/image-20220928191836623.png" alt="image-20220928191836623" style="zoom:80%; margin:auto;">

<h3 id="3D-Patch-Partition"><a href="#3D-Patch-Partition" class="headerlink" title="3D Patch Partition"></a>3D Patch Partition</h3><p>在初始时，设置每个token的大小为2x4x4x3，因此，每个视频被划分为$\frac{T}{2}\times \frac{H}{4} \times \frac{W}{4}$个tokens，每个token的维度为96。</p>
<h3 id="Linear-Embedding"><a href="#Linear-Embedding" class="headerlink" title="Linear Embedding"></a>Linear Embedding</h3><p>将每一个token投影到指定的维度C。</p>
<h3 id="Video-Swin-Transformer-Block"><a href="#Video-Swin-Transformer-Block" class="headerlink" title="Video Swin Transformer Block"></a>Video Swin Transformer Block</h3><p>对于两个相邻的Video Swin Transformer Block，仍采用和Swin Transformer相同的处理方法，每个3D W-MSA后接一个3D SW-MSA。</p>
<img src="/2022/09/28/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AVideo-Swin-Transformer/image-20220928200155711.png" alt="image-20220928200155711" style="zoom:67%; margin:auto;">

<h4 id="3D-W-MSA"><a href="#3D-W-MSA" class="headerlink" title="3D W-MSA"></a>3D W-MSA</h4><p>3D W-MSA与W-MSA相似，对于大小为8x8x8的输入，使用4x4x4的窗口大小，在stage1中窗口的数目为2x2x2，在每个窗口内部进行自注意力计算。</p>
<img src="/2022/09/28/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AVideo-Swin-Transformer/image-20220928210505074.png" alt="image-20220928210505074" style="zoom:67%; margin:auto;">

<h4 id="3D-SW-MSA"><a href="#3D-SW-MSA" class="headerlink" title="3D SW-MSA"></a>3D SW-MSA</h4><p>3D SW-MSA与SW-MSA相同，使用移动窗口补充计算不同窗口中token间的自注意力，为了降低移动窗口后增加的窗口数，同样使用拼接与mask技术，保持窗口数的恒定。</p>
<h3 id="Patch-Merging"><a href="#Patch-Merging" class="headerlink" title="Patch Merging"></a>Patch Merging</h3><p>在进行patch合并时，不从时间维度进行下采样，而是从空间维度对2x2的patch进行合并，合并之后使用一个线性层投影将其维度减半。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><h3 id="不同的时空注意力设计"><a href="#不同的时空注意力设计" class="headerlink" title="不同的时空注意力设计"></a>不同的时空注意力设计</h3><ul>
<li>joint：在每一个3D窗口中联合计算时空注意力。</li>
<li>split：在空间swin transformer的基础上添加了两个时间transformer层</li>
<li>factorized：先是一个空间MSA层，再接一个时间MSA层。</li>
</ul>
<p>实验证明，在视频分类任务中，综合考虑速度与精度，joint模式达到了最佳，作者认为空间域的局部性减少了joint的计算量，同时保持了有效性。</p>
<h3 id="3D-token的时间维度与窗口的时间维度"><a href="#3D-token的时间维度与窗口的时间维度" class="headerlink" title="3D token的时间维度与窗口的时间维度"></a>3D token的时间维度与窗口的时间维度</h3><p>总的来说，3D token的时间维度与窗口的时间维度越大，精度越高，相应的计算成本也越高。</p>
]]></content>
      <tags>
        <tag>经典模型</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Deep Learning Methods for Remote Heart Rate Measurement: A Review and Future Research Agenda</title>
    <url>/2022/09/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeep-Learning-Methods-for-Remote-Heart-Rate-Measurement-A-Review-and-Future-Research-Agenda/</url>
    <content><![CDATA[<p>参考博客：<a href="https://blog.csdn.net/m0_46792836/article/details/121222265">https://blog.csdn.net/m0_46792836/article/details/121222265</a></p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><strong>rPPG</strong>，即remote photoplethysmography，远程光电容积脉搏波描记法，通过摄像头捕捉皮肤细微的亮度变化监测心率。</p>
<p>PPG是一种最常用的测量心率的方法，使用一个光源和光电探测器测量皮下血管体积的变化，当光源照在组织上时，光探测器可以捕捉到血液流动反射或透射光强度的微小变化，产生所谓的PPG信号，光的吸收遵循Beer–Lambert定律，该定律指出，血液吸收的光与光进入皮肤的渗透力和血液中血红蛋白的浓度成正比。在心动周期中，血红蛋白浓度的微小变化引起血管吸收的光量的32个波动，导致33个皮肤强度值的变化。</p>
<p>但对于穿戴式或接触式的监测设备，不适用于检测新生儿或皮肤脆弱的患者，长期检测可能会导致患者不舒服或皮肤感染。</p>
<p>在rPPG中，使用相机作为光探测器捕捉皮肤微小的颜色变化，自然光作为光源。其DRM（色反射）模型如下图所示，可以看到相机同时捕捉到皮肤表面产生的镜面反射和身体产生的漫反射，其中镜面反射并不包含有意义的生理信息，因此需要对捕捉到的信号进行精细处理。</p>
<p><img src="/2022/09/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeep-Learning-Methods-for-Remote-Heart-Rate-Measurement-A-Review-and-Future-Research-Agenda/image-20220929213602840.png" alt="image-20220929213602840" style="zoom:67%; margin:auto;"></p>
<p>传统的远程心率测量方法如下图所示，首先对视频进行人脸检测，接着从人脸上选择感兴趣的区域（ROI），以此获得包含强信号的区域，此后从ROI内的像素提取rPPG信号，最后对rPPG信号进一步处理（如频率分析和峰值检测等）获取心率信息。</p>
<p><img src="/2022/09/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeep-Learning-Methods-for-Remote-Heart-Rate-Measurement-A-Review-and-Future-Research-Agenda/image-20220930082244032.png" alt="image-20220930082244032" style="zoom: 67%; margin:auto;"></p>
<p>基于深度学习的远程心率检测方法可以分为<strong>端到端</strong>和<strong>混合深度学习</strong>的方法。</p>
<h2 id="端到端的深度学习方法"><a href="#端到端的深度学习方法" class="headerlink" title="端到端的深度学习方法"></a>端到端的深度学习方法</h2><p>所谓的端到端的方法，即模型直接将视频作为输入，产生心率或rPPG信号输出。这种方法需要大量的训练数据，同时训练结果难以验证，我们需要做更多的工作对模型进行解释。</p>
<p>端到端的心率检测方法又可以分为如下两种：</p>
<p><img src="/2022/09/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeep-Learning-Methods-for-Remote-Heart-Rate-Measurement-A-Review-and-Future-Research-Agenda/image-20220930085223073.png" alt="image-20220930085223073" style="zoom: 67%; margin: auto;"></p>
<h3 id="2D-CNN"><a href="#2D-CNN" class="headerlink" title="2D CNN"></a>2D CNN</h3><p>2D CNN只考虑了视频帧的空间信息。</p>
<ul>
<li>HR-CNN（2018）：包含提取器和HR估计器的两步CNN，提取器从视频帧序列中提取rPPG信号，使其信噪比最大化。HR-CNN解决了<strong>视频压缩伪影</strong>的问题。</li>
<li>DeepPhys（2018）：同时训练一个运动模型和外观模型，运动模型将相邻视频帧间的归一化差作为模型的输入表示，对帧中的运动和颜色变化进行建模；外观模型通过注意力机制引导运动模型学习运动表征。该模型可以更好地<strong>捕捉不同光照条件下的生理信号</strong>，对光照变化和被试运动更有鲁棒性。</li>
<li>MTTS-CAN（2020）：DeepPhys的改进，引入时间移位模块（TSM）捕获时间信息，TSM允许相邻帧之间的信息交换，避免昂贵的3D卷积操作。</li>
</ul>
<h3 id="时空网络——3D-CNN"><a href="#时空网络——3D-CNN" class="headerlink" title="时空网络——3D CNN"></a>时空网络——3D CNN</h3><p>3D CNN可以利用视频中包含的时间信息，时空网络（STNs）有效地表示视频流中生理信号的时空信息。</p>
<ul>
<li>3D CNN PhysNet：旨在定位每个个体心跳的峰值，以准确估计被试的HR和HRV。</li>
<li>两阶段STN：包括一个时空视频增强网络（3D STVEN）和一个时空3D CNN（rPPGNet），压缩的面部视频通过3D STVEN以提高视频质量，同时保留尽可能多的信息；增强后的视频输入rPPGNet以提取rPPG信号，rPPGNet使用注意力机制从皮肤区域获取主导的rPPG特征。</li>
<li>AutoHR：使用神经结构搜索（NAS）自动找到最适合的主干3D CNN，使用一种三维卷积操作时域差分卷积（TDC）帮助跟踪感兴趣区域。</li>
</ul>
<h3 id="时空网络——2D-CNN-RNN"><a href="#时空网络——2D-CNN-RNN" class="headerlink" title="时空网络——2D CNN+RNN"></a>时空网络——2D CNN+RNN</h3><p>使用2D CNN提取空间信息，用RNN提取时间前后信息并结合。</p>
<ul>
<li>基于RNN的PhysNet：首先将输入信息输入到2D CNN中提取RGB视频帧的空间特征，然后利用RNN在时域内传播这些空间特征。但研究证明，基于3D CNN的PhysNet比基于RNN的PhysNet获得了更好的性能。</li>
</ul>
<p><img src="/2022/09/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeep-Learning-Methods-for-Remote-Heart-Rate-Measurement-A-Review-and-Future-Research-Agenda/image-20220930105928378.png" alt="image-20220930105928378" style="zoom: 67%; margin:auto;"></p>
<h2 id="混合深度学习方法"><a href="#混合深度学习方法" class="headerlink" title="混合深度学习方法"></a>混合深度学习方法</h2><p>所谓的混合深度学习方法，是指深度学习技术只应用在检测过程中的某些部分，如信号优化、信号提取或心率估计。</p>
<h3 id="用于信号优化的深度学习方法"><a href="#用于信号优化的深度学习方法" class="headerlink" title="用于信号优化的深度学习方法"></a>用于信号优化的深度学习方法</h3><p>所谓的信号优化就是使用人脸检测或皮肤分割，以<strong>忽略不相关的背景信息</strong>。</p>
<ul>
<li>创建一个用于皮肤检测的2D CNN，用于分割出皮肤所在区域，对检测到的皮肤区域进行常规rPPG算法。但这种方法利用了人脸的所有皮肤区域提取rPPG信号，可能包含不必要的噪声。</li>
<li>Deep-HR：采用接受域块（RFB）网络对感兴趣区域进行目标检测，该方法设计了GAN增强检测到的ROI，对检测到的ROI进行再生，将这个高质量的ROI（这个过程也可以看作后面的信号提取的过程）用于后续的信号提取。</li>
</ul>
<h3 id="用于信号提取的深度学习"><a href="#用于信号提取的深度学习" class="headerlink" title="用于信号提取的深度学习"></a>用于信号提取的深度学习</h3><p>信号提取的目标是从视频中<strong>提取高质量的rPPG信号</strong>进行HR估计（感觉是指去噪这个过程）。</p>
<ul>
<li><p>LSTM：使用LSTM网络对噪声污染的rPPG信号进行滤波，得到无噪声的rPPG信号。由于数据不足的问题，在训练时可以首先在合成数据上进行训练，然后在真实数据集上微调。</p>
</li>
<li><p>2D CNN MetaPhys：使用预训练的2D CNN TS-CAN用于信号提取，并提出元学习方法，利用模型不可知元学习作为个性化参数更新模式，可以在只有少量训练样本的情况下快速适应。作者认为该方法可以减少由于肤色造成的偏差。</p>
</li>
<li><p>3D CNN Siamese-rPPG：基于Siamese 3D CNN框架，作者认为不同的面部区域应反映相同的rPPG特征，建立额头分支和脸颊分支进行特征提取，将两个分支的输出通过运算融合，得到最终的预测的rPPG信号。</p>
<p><img src="/2022/09/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeep-Learning-Methods-for-Remote-Heart-Rate-Measurement-A-Review-and-Future-Research-Agenda/image-20220930142543663.png" alt="image-20220930142543663" style="zoom: 67%; margin: auto;"></p>
</li>
<li><p>3D CNN HeartTrack：利用带有注意力机制的3D CNN进行信号提取，在三维时空注意网络中，利用硬注意力机制忽略不相关的背景信息，利用软注意力机制过滤所覆盖的区域。再将提取到的信号送入1D CNN进行时间序列分析。</p>
</li>
</ul>
<h3 id="用于心率估计的深度学习"><a href="#用于心率估计的深度学习" class="headerlink" title="用于心率估计的深度学习"></a>用于心率估计的深度学习</h3><p>对于提取到的rPPG信号，传统的方法是使用带通滤波器滤波，然后进行频率分析或峰值检测来估计心率。对于深度学习方法，<strong>将心率估计看作回归问题求解</strong>。</p>
<p>HR信号有两种表示：</p>
<ol>
<li>频谱图像：对提取的rPPG信号进行短时傅里叶变换和带通滤波，得到频域表示，将频域表示和时域信号结合，形成频谱图像。</li>
<li>时空图：将ROI像素RGB通道的颜色信息串接在时间序列中，成行排列，形成时空地图。这种信号表示方法可以抑制与HR信号无关的信息。</li>
</ol>
<h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><ul>
<li>流行病控制</li>
<li>防伪：通过捕捉异常生物信号检测深度造假的视频</li>
<li>远程医疗</li>
<li>增强生物识别技术的安全性</li>
<li>驾驶状态检测</li>
<li>？从自然灾害中寻找幸存者</li>
<li>新生儿检测</li>
<li>健康跟踪</li>
</ul>
<h2 id="研究缺口"><a href="#研究缺口" class="headerlink" title="研究缺口"></a>研究缺口</h2><ul>
<li>影响因素：基于rPPG的远程HR测量收到光照变化、运动伪影、肤色变化和视频压缩等诸多因素的影响。<strong>新方法应提供如何从技术和生物物理角度处理这些挑战的见解</strong></li>
<li>测量其他生命体征</li>
<li>数据集：目前数据集抓鱼用于解决运动伪影、照明变化，对于肤色变化、多人检测、远距离、新生儿检测也需要克服。</li>
<li>在不同HR范围上的表现</li>
<li>对基于深度学习方法上的理解</li>
</ul>
]]></content>
      <tags>
        <tag>rPPG</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：MAE</title>
    <url>/2022/10/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AMAE/</url>
    <content><![CDATA[<p>参考：<a href="https://www.bilibili.com/video/BV1sq4y1q77t/">https://www.bilibili.com/video/BV1sq4y1q77t/</a></p>
<p>代码：<a href="https://github.com/facebookresearch/mae">https://github.com/facebookresearch/mae</a></p>
<h2 id="模型概述"><a href="#模型概述" class="headerlink" title="模型概述"></a>模型概述</h2><p>MAE即Masked Autoencoders（其auto是指“自“的意思，而不是自动，指label也来自图片本身），可以看作CV版的Bert，基于ViT方法。</p>
<p>MAE的整体思路是将图片中的一些patches进行mask，然后对masked掉的patches进行预测。如下图所示：</p>
<img src="/2022/10/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AMAE/image-20221003114506533.png" alt="image-20221003114506533" style="zoom:80%; margin:auto;">

<h2 id="NLP与CV"><a href="#NLP与CV" class="headerlink" title="NLP与CV"></a>NLP与CV</h2><p>作者认为造成NLP与CV任务中masked autoencoding的不同，主要来自以下几个方面：</p>
<ol>
<li>文本与图像的信息密度不同，文本是人为生成的，具有<strong>高语义和信息密度</strong>，而图像则具有大量的冗余信息</li>
<li>对于自编码器的解码器，CV任务需要还原的是语义层次比较低的像素信息（解码器更加复杂），而NLP任务需要还原的是语义层次比较高的文本信息（解码器简单）</li>
</ol>
<h2 id="MAE具体实现"><a href="#MAE具体实现" class="headerlink" title="MAE具体实现"></a>MAE具体实现</h2><ul>
<li><p>masking：将一张图片分成若干个patches，然后随机选取部分patches进行mask，仅保留少量的patches，以减少冗余信息。</p>
</li>
<li><p>MAE Encoder：采用完全和ViT相同的方法，对于被masked掉的patches，直接不输入Encoder。</p>
</li>
<li><p>MAE Decoder：解码器同样使用一个Transformer，对于所有patches，通过一个共享的、可以学到的向量来表示，对于每一个块要加上位置编码信息，解码器只有在预训练的时候需要使用。</p>
</li>
<li><p>Reconstruction target：对于解码器的输出通过一个线形层投影到对应的输出shape，使用MSE对被masked的patches计算loss。</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Benchmarking Joint Face Spoofing and Forgery Detection with Visual and Physiological Cues</title>
    <url>/2022/10/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ABenchmarking-Joint-Face-Spoofing-and-Forgery-Detection-with-Visual-and-Physiological-Cues/</url>
    <content><![CDATA[<ul>
<li>发表时间：2022</li>
<li>研究内容：基于视觉和生理线索的联合人脸欺骗和伪造检测标杆</li>
<li>FAS：Face Anti-Spoofing，活体检测</li>
<li>Face Forgery Detection：人脸伪造检测（Deepfake检测）</li>
</ul>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>目前大多数的<strong>活体检测</strong>和<strong>人脸伪造检测</strong>方法关注以下两个方面：</p>
<ol>
<li>单模态的视觉外观或生理信号（rPPG信号）</li>
<li>分离FAS或人脸伪造检测的特征表示</li>
</ol>
<p>但这两种方面存在如下问题：</p>
<ol>
<li>单模态的外观和rPPG特征对于高保真的3D面罩或视频回放袭击来说相对脆弱。</li>
<li>对于FAS和人脸欺诈识别任务，有丰富的常用特征，可用于以多任务学习的方式设计一个FAS和人脸欺诈识别联合系统。</li>
</ol>
<p>本文<strong>贡献</strong>：</p>
<ol>
<li>建立了第一个活体检测和伪造识别的benchmark，该benchmark同时使用视觉外观和生理rPPG信号。</li>
<li>设计了一个双分支的生理网络，同时使用面部时空的rPPG信号和其对应的连续小波变换作为输入。加强了rPPG周期性的区别。</li>
<li>在进行多模态融合前，对视觉外观和rPPG特征实施a weighted batch and layer normalization，以消除模态偏差，提高融合效果。</li>
<li>研究发现，无论单模态还是多模态模型，通过在活体检测和人脸伪造检测两个任务上进行联合训练可以显著提高模型泛化能力。</li>
</ol>
<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>基于rPPG或外观+rPPG的活体检测与人脸伪造识别技术还不成熟。主要包括如下两个原因：</p>
<ol>
<li>由于外部干扰，时域rPPG信号的周期性识别受到限制（无法区分是真实的还是伪造的）。</li>
<li>由于模态偏差，外观与rPPG信号的直接融合会导致负效果。</li>
</ol>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p><strong>Physiological rPPG based FAS：</strong>使用rPPG信号进行活体检测。主要包括如下几种做法：</p>
<ul>
<li>考虑真实的面部和打印出的面部的心跳区别；</li>
<li>通过比较面部的rPPG信号和背景噪音判断是否为活体；</li>
<li>使用transformer架构提取全局的周期性的信息用于活体检测；</li>
<li>通过评分级融合结合外观和rPPG预测实现活体检测</li>
</ul>
<p><strong>Physiological rPPG based face forgery detection：</strong>使用rPPG信号进行伪造人脸识别，主要包括如下几种做法：</p>
<ul>
<li>与活体检测类似，通过判断心率是否包含固定周期性模式；</li>
<li>从时间域和功率谱域提取rPPG信号的特征，描述时间一致性和空间相干性；</li>
</ul>
<h2 id="实现方法"><a href="#实现方法" class="headerlink" title="实现方法"></a>实现方法</h2><p><strong>Joint Face Spoofing and Forgery Detection：</strong>联合训练两种任务</p>
<ul>
<li><p>联合训练架构：在多任务训练中有如下三种设置方式：</p>
<img src="/2022/10/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ABenchmarking-Joint-Face-Spoofing-and-Forgery-Detection-with-Visual-and-Physiological-Cues/image-20221005201731187.png" alt="image-20221005201731187" style="zoom:67%; margin:auto;">
</li>
<li><p>联合训练采样策略</p>
<ul>
<li>随机采样：每个batch中的数据从两任务的混合数据中随机采样（比例不定）</li>
<li>同步采样：每个batch中的数据来自两任务的数据各占一半</li>
<li>交替采样：每个SGD step轮流学习每个任务（即一个batch学习FAS，一个batch学习伪造人脸识别）</li>
<li>按任务采样：先学习一个任务（全部数据），再学习另一个任务。</li>
</ul>
</li>
</ul>
<p><strong>Two-branch Physiological Network：</strong>同时使用面部时空的rPPG信号和其对应的连续小波变换作为输入。</p>
<p>文章使用的两分支的生理网络如下图所示，使用两个结构相同但参数不同的编码器获得时域、时频域特征，将两种特征拼接形成通用的rPPG特征，然后接一个二分类头进行预测，同时为了训练的稳定性，对于两个分支添加额外的二分类头，模型的Loss由上述三个头的loss组成。</p>
<img src="/2022/10/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ABenchmarking-Joint-Face-Spoofing-and-Forgery-Detection-with-Visual-and-Physiological-Cues/image-20221005204141328.png" alt="image-20221005204141328" style="zoom:67%; margin:auto;">

<ul>
<li>MSTmap（Multi-scale Spatial-temporal map）：多尺度时空图，考虑局部和全局的生理信号。</li>
<li>WaveletMap：基于连续小波变换的时频图</li>
</ul>
<p><strong>Weighted Normalization for Appearance and rPPG Fusion：</strong>多模态融合</p>
<p>对于每个模态使用batch normalization和layer normalization，然后再将两个模态进行连接。如下所示：</p>
<img src="/2022/10/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ABenchmarking-Joint-Face-Spoofing-and-Forgery-Detection-with-Visual-and-Physiological-Cues/image-20221005212654204.png" alt="image-20221005212654204" style="zoom: 50%; margin:auto;">



<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>未来研究方向：</p>
<ol>
<li>探索多任务学习和多模态融合的策略</li>
<li>除了基于rPPG的颜色变化，利用面部运动的语义线索和上下文动态进行活体检测和伪造人脸识别。</li>
</ol>
]]></content>
      <tags>
        <tag>rPPG</tag>
        <tag>FAS</tag>
      </tags>
  </entry>
  <entry>
    <title>论文技巧</title>
    <url>/2022/10/06/%E8%AE%BA%E6%96%87%E6%8A%80%E5%B7%A7/</url>
    <content><![CDATA[<h2 id="如何找研究想法"><a href="#如何找研究想法" class="headerlink" title="如何找研究想法"></a>如何找研究想法</h2><ul>
<li>参考：<a href="https://www.bilibili.com/video/BV1qq4y1z7F2/">https://www.bilibili.com/video/BV1qq4y1z7F2/</a></li>
</ul>
<ol>
<li>打补丁法：针对新发表的论文中存在的问题打补丁（注意：若一篇文章已经是打补丁的文章，最好不要继续在其上继续打补丁。）</li>
</ol>
<h2 id="如何判断研究工作的价值"><a href="#如何判断研究工作的价值" class="headerlink" title="如何判断研究工作的价值"></a>如何判断研究工作的价值</h2><ul>
<li>参考：<a href="https://www.bilibili.com/video/BV1oL411c7Us/">https://www.bilibili.com/video/BV1oL411c7Us/</a></li>
</ul>
<p>好的研究工作应该：用有<strong>新意</strong>的方法，<strong>有效</strong>的解决一个<strong>研究</strong>问题</p>
<ul>
<li>研究问题：与之相对的是工程问题</li>
</ul>
<p>量化地来看，研究价值&#x3D;新意度x有效性x问题大小</p>
<h2 id="论文写作"><a href="#论文写作" class="headerlink" title="论文写作"></a>论文写作</h2><ul>
<li>参考：<a href="https://www.bilibili.com/video/BV1hY411T7vy/">https://www.bilibili.com/video/BV1hY411T7vy/</a></li>
</ul>
<h3 id="Research-Researchers-and-Readers"><a href="#Research-Researchers-and-Readers" class="headerlink" title="Research, Researchers, and Readers"></a>Research, Researchers, and Readers</h3><ul>
<li>论文可以看作想象中的对话，根据读者的知识储备决定写作内容与风格</li>
</ul>
<h3 id="Asking-Questions-Finding-Answers"><a href="#Asking-Questions-Finding-Answers" class="headerlink" title="Asking Questions, Finding Answers"></a>Asking Questions, Finding Answers</h3><ul>
<li>so what</li>
</ul>
]]></content>
      <tags>
        <tag>论文技巧</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Remote Photoplethysmograph Signal Measurement from Facial Videos Using Spatio-Temporal Networks</title>
    <url>/2022/10/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARemote-Photoplethysmograph-Signal-Measurement-from-Facial-Videos-Using-Spatio-Temporal-Networks/</url>
    <content><![CDATA[<ul>
<li>发表时间：2019</li>
<li>code：<a href="https://github.com/ZitongYu/PhysNet">https://github.com/ZitongYu/PhysNet</a></li>
<li>HRV：Heart Rate Variability，心率变异性，从更精细的角度描述心脏活动</li>
<li>AF：atrial fibrillatio，心房震颤</li>
<li>时空网络：主流的时空网络有两种，第一种是基于3D卷积的时空网络，第二种是基于RNN的神经网络（如LSTM等）。</li>
</ul>
<h2 id="简介">简介</h2>
<p>文章提出了一个深度时空网络PhysNet，用于从原始面部视频中重建精确的rPPG信号，包括每个时间位置与其对应的脉冲峰值，可以获得平均HR，以及HRV和IBIs信息，用于AF检测与情绪识别中。</p>
<h3 id="研究现状">研究现状</h3>
<ol>
<li>早期使用两阶段的方法，阶段1检测或跟踪面部以提取rPPG信号，阶段2通过频率分析估计对应的平均心率。但这种方法有两个缺点，1是自定义的面部区域是根据经验知识得到的，可能并非最佳的区域。2是该方法需要手工特征或过滤器，可能泛化能力弱并且丢失与心率相关的重要信息；</li>
<li>对于基于深度学习的心率检测方法，通常有如下几个缺点，1是将心率估计问题看作简单的一阶段回归问题，得到平均心率，而丢失了单个脉冲峰值信息，从而限制了在医学上的应用；2是使用的方法并非端到端的系统，仍然需要预处理或后处理步骤，引入了手工特征；3是使用的方法基于2D空间神经网络，没有考虑对rPPG测量重要的时间特征。</li>
</ol>
<h3 id="模型简介">模型简介</h3>
<p>文章提出的rPPG信号测量方法的框架如下图所示：</p>
<img src="/2022/10/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARemote-Photoplethysmograph-Signal-Measurement-from-Facial-Videos-Using-Spatio-Temporal-Networks/image-20221008100615766.png" alt="image-20221008100615766" style="zoom:80%; margin:auto;">
<p>文章的主要贡献包括如下几点：</p>
<ol>
<li>提出了第一个基于端到端的rPPG信号测量网络PhysNet，并将之前忽略的时间信息考虑进去；</li>
<li>PhysNet在测量平均心率和HRV特征上都达到了很好的性能；</li>
<li>PhysNet具有很好的泛化性</li>
</ol>
<h2 id="PhysNet">PhysNet</h2>
<p>PhysNet的结构如下图所示：</p>
<img src="/2022/10/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARemote-Photoplethysmograph-Signal-Measurement-from-Facial-Videos-Using-Spatio-Temporal-Networks/image-20221008105124878.png" alt="image-20221008105124878" style="zoom:80%; margin:auto;">
<p>主流的时空网络有基于3DCNN和基于RNN两种，因此作者也提出了两种PhyNet。</p>
<h3 id="3DCNN-based-PhysNet">3DCNN based PhysNet</h3>
<ul>
<li>采用一个3x3x3的卷积在空间域和时间域同时提取rPPG特征。</li>
<li>采用基于时间的encoder-decoder结构PhysNet-3DCNN-ED，可以更有效地利用时间信息，减少冗余与噪音。</li>
</ul>
<h3 id="RNN-based-PhysNet">RNN based PhysNet</h3>
<ul>
<li>首先使用2DCNN提取空间特征，然后使用基于RNN的模块实现空间特征在时域的传播</li>
</ul>
<h3 id="损失函数">损失函数</h3>
<p>使用负皮尔森相关系数作为损失函数，以最大化趋势相似度，最小化峰值定位误差。</p>
<img src="/2022/10/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARemote-Photoplethysmograph-Signal-Measurement-from-Facial-Videos-Using-Spatio-Temporal-Networks/image-20221008111446331.png" alt="image-20221008111446331" style="zoom: 50%; margin:auto;">
<h2 id="实验设置">实验设置</h2>
<h3 id="数据集">数据集</h3>
<p>文章使用了两个数据集，使用OBF数据集进行训练与测试，使用MAHNOB-HCI数据集进行模型泛化能力的交叉验证。</p>
<ol>
<li>OBF数据集：采集自100个健康的成人和6个患有心房震颤的病人，每个人有两个时长为5分钟的视频，每个视频包括其对应的ECG、呼吸、ppg信号，每个视频的分辨率为1920x2080，帧率为60fps。</li>
<li>MAHNOB-HCI数据集：采集自27个人的共527个视频，每个视频的分辨率为780x580，帧率为61fps。</li>
</ol>
<h3 id="训练设置">训练设置</h3>
<p>对于每一个视频，进行如下操作：</p>
<ul>
<li>在第一帧中使用Viola-Jones面部检测器切割出面部区域，并在之后的帧中固定这个区域；</li>
<li>将面部图片正则化到128x128</li>
<li>将视频和对应的真实信号进行下采样，视频下采样至30fps，信号下采样至30HZ</li>
<li>使用Adam，lr=1e-4，epoch=15</li>
</ul>
<h3 id="测试设置">测试设置</h3>
<p>对于HR与HRV的评估，使用如下几个性能指标：</p>
<ol>
<li>SD（standard deviation）：标准差</li>
<li>RMSE（root mean square error）：均方根误差</li>
<li>R（Pearson’s correlation coefficien）：皮尔森相关系数</li>
<li>MAE（mean absolute error）：平均绝对误差</li>
</ol>
<h2 id="代码实现（UBFC版）">代码实现（UBFC版）</h2>
<h3 id="数据集UBFC-rPPG">数据集UBFC-rPPG</h3>
<p><strong>UBFC-rPPG</strong>：出自论文《Unsupervised skin tissue segmentation for remote photoplethysmography》</p>
<ul>
<li>
<p>video：43个</p>
</li>
<li>
<p>time：2min</p>
</li>
<li>
<p>帧率：30fps</p>
</li>
<li>
<p>分辨率：640x480</p>
</li>
<li>
<p>ground truth：与视频同步的心率信号，对于ground truth，第一行是gtTrace，第二行是gtHR，第三行是gtTime。</p>
<img src="/C:/Users/zxkj/AppData/Roaming/Typora/typora-user-images/image-20221010183747585.png" alt="image-20221010183747585" style="zoom:80%;">
</li>
</ul>
<h3 id="K-交叉验证（但好像没用到）">K-交叉验证（但好像没用到）</h3>
<p>K-交叉验证是一种<strong>模型评估</strong>方法，指将原始数据均分成k组，轮流将每个子集数据分别作为验证集，其余k-1组子集数据作为训练集，这样将得到k个模型，用k个模型的平均性能作为模型的评价指标。</p>
<h3 id="DataLoader">DataLoader</h3>
<p>PhysNet实现中，训练集和测试集是通过如下代码手动划分的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (test):</span><br><span class="line">    self.vdPath_list = os.listdir(<span class="string">&quot;/data/maoguanhui/UBFC/&quot;</span>)[<span class="number">30</span>:<span class="number">42</span>]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    self.vdPath_list = os.listdir(<span class="string">&quot;/data/maoguanhui/UBFC/&quot;</span>)[:<span class="number">30</span>]</span><br></pre></td></tr></table></figure>
<p>之后，将每个视频划分为7个片段，每个片段包括160帧</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clip = idx % <span class="number">7</span>  <span class="comment"># 第几个剪辑片段</span></span><br><span class="line">idx = <span class="built_in">int</span>(idx / <span class="number">7</span>)  <span class="comment"># 第几个视频</span></span><br><span class="line">start_frame = <span class="number">160</span> * clip</span><br></pre></td></tr></table></figure>
<p>然后从ground truth中获取计算loss需要的信息，首先获取平均HR，用于计算师兄添加的fre_loss</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clhr = <span class="built_in">list</span>(data[<span class="number">1</span>].split())</span><br><span class="line">clhr = [<span class="built_in">str</span>.replace(<span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;E&#x27;</span>) <span class="keyword">for</span> <span class="built_in">str</span> <span class="keyword">in</span> clhr]</span><br><span class="line">sumHR = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">for</span> kj <span class="keyword">in</span> <span class="built_in">range</span>(start_frame, start_frame + <span class="number">160</span>):</span><br><span class="line">    sumHR += <span class="built_in">float</span>(clhr[kj])</span><br><span class="line">clip_average_HR = sumHR / <span class="number">160</span></span><br></pre></td></tr></table></figure>
<p>再获取PPG信号，用于计算皮尔森系数对应的rPPG loss。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Trace = []</span><br><span class="line">data = <span class="built_in">list</span>(data[<span class="number">0</span>].split())</span><br><span class="line">data = [<span class="built_in">str</span>.replace(<span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;E&#x27;</span>) <span class="keyword">for</span> <span class="built_in">str</span> <span class="keyword">in</span> data]</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data)):</span><br><span class="line">    Trace.append(<span class="built_in">float</span>(data[j]))</span><br></pre></td></tr></table></figure>
<h3 id="网络结构">网络结构</h3>
<p>在PhysNet的网络实现中，有多种网络，文章中选择使用PhysNet_padding_ED_peak</p>
]]></content>
      <tags>
        <tag>rPPG</tag>
      </tags>
  </entry>
  <entry>
    <title>视频理解</title>
    <url>/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<h2 id="双流网络"><a href="#双流网络" class="headerlink" title="双流网络"></a>双流网络</h2><ul>
<li>论文：《Two-stream convolutional networks for action recognition in videos》</li>
</ul>
<p>提出的具有两个分支的神经网络，将两个分支的输出融合后进行分类。</p>
<img src="/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/image-20221009101114213.png" alt="image-20221009101114213" style="zoom:80%; margin:auto;">

<ul>
<li>空间流神经网络：捕捉apperance信息，将视频帧作为处理对象，就是普通的图像处理网络，可以使用imagenet上的预训练网络。</li>
<li>时间流神经网络：捕捉两帧间光流的运动信息，一般将光流的运动信息分解为水平方向和竖直方向，如下图所示。对于L帧的视频，最终将得到L-1个光流图</li>
</ul>
<img src="/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/image-20221009100000394.png" alt="image-20221009100000394" style="zoom:80%; margin:auto;">

<h3 id="光流图"><a href="#光流图" class="headerlink" title="光流图"></a>光流图</h3><p>对于从视频帧中得到的光流图，要利用光流图间的信息，作者提出了两种光流图叠加方式：</p>
<ol>
<li>optical flow stacking：直接将光流图对应位置叠加在一起，简单但没有充分利用光流信息</li>
<li>trajectory stacking：沿光流轨迹进行数值叠加（理论上有效，但实验效果不如1）</li>
</ol>
<img src="/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/image-20221009100819605.png" alt="image-20221009100819605" style="zoom:80%; margin:auto;">

<p><strong>Bi-directional optical flow</strong>：</p>
<p>​	双向光流，在一个帧的前向和后向区间进行光流计算，达到双向传递的效果。</p>
<h2 id="I3D"><a href="#I3D" class="headerlink" title="I3D"></a>I3D</h2><ul>
<li>论文《Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset》</li>
<li>参考：<a href="https://www.bilibili.com/video/BV1tY4y1p7hq">https://www.bilibili.com/video/BV1tY4y1p7hq</a></li>
</ul>
<p>这篇论文主要有两个贡献：</p>
<ol>
<li>提出了I3D</li>
<li>提出了一个数据集Kinetics（类别均衡、规模适中、可玩性强，可作为视频任务的预训练数据集，地位相当于图片任务中的ImageNet）</li>
</ol>
<h3 id="Inflated-3D-Network"><a href="#Inflated-3D-Network" class="headerlink" title="Inflated 3D Network"></a>Inflated 3D Network</h3><p>把一个2D模型直接扩张到3D，使用2D模型的参数去初始化3D模型。现在常说的I3D网络一般是指基于ResNet的3D网络。</p>
<p>对于视频动作识别任务，网络发展主要经过如下a,b,c,d四个阶段。其中e为作者在Kinetics数据集上提出的I3D网络。</p>
<img src="/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/image-20221010205800716.png" alt="image-20221010205800716" style="zoom:80%; margin:auto;">

<p>作者提出这种网络的根据是，随着训练集规模的增大，使用3D CNN可以取得比2D CNN更好的效果，同时，使用光流信息，可以帮助模型取得更好的结果。</p>
<p><strong>Inflating 2D ConvNets into 3D</strong></p>
<p><em>2D Network &gt;&gt;&gt; 3D Network</em></p>
<p>保持2D网络的架构不变，直接将2D的卷积层、池化层等膨胀到3D（？实现了后向兼容），得到对应的3D神经网络架构。</p>
<p>tip：作者以及后人经过实验发现，最好不要做<strong>时间维度上的下采样</strong>。</p>
<p><strong>Bootstrapping 3D Filters from 2D  Filters</strong></p>
<p><em>2D Model &gt;&gt;&gt; 3D Model</em></p>
<p>将2D模型的参数在时间维度上进行复制，即得到对应的3D模型。通过代码的实现如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">temporal_dim = weights3d[key3d].shape[<span class="number">2</span>]</span><br><span class="line">inflated_2d = nd.broadcast_to(temporal_2d, shape=[<span class="number">0</span>, <span class="number">0</span>, temporal_dim, <span class="number">0</span>, <span class="number">0</span>]) / temporal_dim</span><br></pre></td></tr></table></figure>

<p>代码中除以temporal_dim的操作，是为了实现一个rescaling，使得对于分类任务，2D网络和3D网络的输出完全一致。</p>
<h2 id="基于深度学习的视频动作识别综述"><a href="#基于深度学习的视频动作识别综述" class="headerlink" title="基于深度学习的视频动作识别综述"></a>基于深度学习的视频动作识别综述</h2><ul>
<li>论文：《A Comprehensive Study of Deep Video Action Recognition》</li>
<li>参考：<a href="https://www.bilibili.com/video/BV1fL4y157yA">https://www.bilibili.com/video/BV1fL4y157yA</a></li>
</ul>
<p>截至Video Transformer提出前，视频动作识别模型的发展历程如下图所示：</p>
<img src="/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/image-20221011160226907.png" alt="image-20221011160226907" style="zoom:67%; margin:auto;">

<h3 id="Hand-crafted–-gt-CNN"><a href="#Hand-crafted–-gt-CNN" class="headerlink" title="Hand-crafted–&gt;CNN"></a>Hand-crafted–&gt;CNN</h3><p>DeepVideo：没啥意思，通过一系列2D卷积操作，对视频进行分类。</p>
<h3 id="双流网络-1"><a href="#双流网络-1" class="headerlink" title="双流网络"></a>双流网络</h3><p>双流网络是在2D CNN基础上，添加一个处理光流的分支。针对初版双流网络的问题，主要在如下四个方面进行改进。</p>
<ol>
<li>将late fusion改造为一种合适的early fusion；</li>
<li>把双流网络中使用的CNN网络进行变体，如融入之后提出的ResNet等；</li>
<li>双流网络的空间流和时间流分支是直接抽取特征后进行分类，可以换成RNN或LSTM等充分利用时序信息；</li>
<li>双流网络利用的是短时间内的视频信息，而一个动作往往持续时间较长，如何进行长视频的理解也是需要解决的问题。</li>
</ol>
<p>&#x3D;&#x3D;双流网络目前仍存在的一个问题是，抽取光流的过程耗时巨大，在推理时，仍然需要花费较长时间去抽取光流，无法达到实时处理的要求（实时处理一般要求帧率达25fps）；另一方面，光流图的存储占据空间较多。&#x3D;&#x3D;</p>
<h4 id="Beyond-Short-Snippets（引入LSTM）"><a href="#Beyond-Short-Snippets（引入LSTM）" class="headerlink" title="Beyond Short Snippets（引入LSTM）"></a>Beyond Short Snippets（引入LSTM）</h4><p>提高特征提取使用的帧数</p>
<ul>
<li>Conv Pooling：</li>
<li>LSTM：带来的提升有限，可能是因为短时内语义信息没有显著变化，而LSTM只有在语义信息发生较大变化时作用显著。</li>
</ul>
<h4 id="Convolutional-Fusion（使用early-fusion）"><a href="#Convolutional-Fusion（使用early-fusion）" class="headerlink" title="Convolutional Fusion（使用early fusion）"></a>Convolutional Fusion（使用early fusion）</h4><ul>
<li>spatial fusion：在空间层面上对特征图进行fusion</li>
<li>在网络的哪个部分进行fusion</li>
<li>temporal fusion：如何在时间轴维度上进行fusion</li>
</ul>
<h4 id="TSN（Temporal-Segment-Networks，解决长视频理解问题）"><a href="#TSN（Temporal-Segment-Networks，解决长视频理解问题）" class="headerlink" title="TSN（Temporal Segment Networks，解决长视频理解问题）"></a>TSN（Temporal Segment Networks，解决长视频理解问题）</h4><p>将一个视频分成多个段，每个段分别送入双流网络（不同段的双流网络共享一组参数），将不同段在空间流输出的结果进行segmental consensus（共识），同样对时间流的输出结果进行相同计算，然后两个流的输出融合后，得到最终输出结果。</p>
<img src="/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/image-20221012102455303.png" alt="image-20221012102455303" style="zoom:80%; margin:auto;">

<p>除此之外，这篇论文也提出了一些技巧：</p>
<ol>
<li>Cross Modality Pre-training：将空间流和时间流（即光流）输入看作多模态问题，使用ImageNet上训练的模型对光流分支进行初始化，需要对预训练模型进行一下改造（将原来3通道的卷积加权平均，应用到20个通道的光流输入上），这种思想与I3D之后使用的方法类似。</li>
<li>Partial BN：缓解BN在视频理解任务上的过拟合问题，在视频数据集上对预训练模型进行训练时，冻住除第一层外的所有BN层。</li>
<li>corner cropping：强制模型在边角位置进行裁剪</li>
<li>scale- jittering：通过改变输入的长宽比，增加输入的多样性</li>
</ol>
<h3 id="3D-CNN"><a href="#3D-CNN" class="headerlink" title="3D CNN"></a>3D CNN</h3><p>在双流网已经取得很好效果的时候，人们关注于3D CNN网络，旨在提高模型的速度，以达到实时的效果。</p>
<pre class="mermaid">graph LR
A[C3D]-->B[I3D]
B-->C[Non-local]
C-->D[R2+1D]
D-->E[SlowFast]</pre>

<h4 id="C3D（2015）"><a href="#C3D（2015）" class="headerlink" title="C3D（2015）"></a>C3D（2015）</h4><p>C3D提出了类似于VGG的3D卷积神经网络，在sports 1million数据集上进行训练，该模型可以分为特征抽取和分类两部分，作者提供了训练好的特征抽取模型接口，使得C3D在视频理解领域流行起来。</p>
<img src="/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/image-20221012112941810.png" alt="image-20221012112941810" style="zoom:80%; margin:auto;">

<h4 id="I3D（2017）"><a href="#I3D（2017）" class="headerlink" title="I3D（2017）"></a>I3D（2017）</h4><p>I3D的主要贡献包括如下两方面：</p>
<ol>
<li>降低了视频理解模型的训练难度（将2D模型膨胀至3D）</li>
<li>提出了一个很好、很大的数据集</li>
</ol>
<h4 id="Non-local（2018）"><a href="#Non-local（2018）" class="headerlink" title="Non-local（2018）"></a>Non-local（2018）</h4><p>使用自注意力机制替代LSTM，用在视频理解网络中，进行长距离建模。</p>
<p>Non-local模块与自注意力模块相近，只是扩展到了3D上，在<strong>时间和空间</strong>维度上进行自注意力计算，同样具有即插即用的性质。</p>
<img src="/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/image-20221012115426776.png" alt="image-20221012115426776" style="zoom: 67%; margin:auto;">

<h4 id="R-2-1-D（2018）"><a href="#R-2-1-D（2018）" class="headerlink" title="R(2+1)D（2018）"></a>R(2+1)D（2018）</h4><p>作者做了大量的实验，探究视频理解任务中的网络结构。经过实验证明，使用R(2+1)D的结构，把3D卷积拆分成两个卷积，先在空间上做2D的卷积，再在时间上做1D的卷积。</p>
<img src="/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/image-20221014103245936.png" alt="image-20221014103245936" style="zoom:67%; margin:auto;">

<p>R(2+1)D的具体拆分方法如下图所示：其中a表示正常的3D卷积，b表示R(2+1)D，Mi指特征投射操作，进行维度变化，使得R(2+1)D网络参数尽可能与3D网络保持一致，从而证明R(2+1)D网络的优越性。</p>
<img src="/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/image-20221014104229339.png" alt="image-20221014104229339" style="zoom:80%; margin:auto;">

<p>该方法的有效性，可以从以下几个方面解释：</p>
<ol>
<li>relu操作增多，提高了模型的非线性学习能力</li>
<li>1D和2D网络比3D网络更容易训练与优化</li>
</ol>
<h4 id="SlowFast（2019）"><a href="#SlowFast（2019）" class="headerlink" title="SlowFast（2019）"></a>SlowFast（2019）</h4><p>SlowFast借鉴了双流网络的思想，但并没有使用光流。</p>
<p>SlowFast网络的原理来自人体视觉细胞，用80%的细胞去捕捉慢的场景信息，用20%的细胞捕捉快的高频率的运动信息。对应到网络结构上，抽取低帧率图像输入到慢分支中（小输入大网络），抽取高帧率图像输出快分支中（大输入小网络），同时每组卷积层后都跟一个later connection，</p>
<img src="/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/image-20221014105622585.png" alt="image-20221014105622585" style="zoom:67%; margin:auto;">

<h3 id="Video-Transformer"><a href="#Video-Transformer" class="headerlink" title="Video Transformer"></a>Video Transformer</h3><p>将Transformer直接应用到视频理解领域，扩展到在时间和空间两个维度进行自注意力计算，计算成本极高，很难训练起来。因此人们通过对时间和空间注意力<strong>拆分</strong>的方式，降低计算成本的同时，尽可能提升模型性能。</p>
<h4 id="Timesformer（2021）"><a href="#Timesformer（2021）" class="headerlink" title="Timesformer（2021）"></a>Timesformer（2021）</h4><p>Timesformer比较了几种Video Transformer的实现方式。经过实验验证，T+S的实现取得了最好的效果。</p>
<img src="/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/image-20221014113000002.png" alt="image-20221014113000002" style="zoom:80%; margin:auto;">

<ul>
<li>S：即经典的用在图像处理上的ViT</li>
<li>ST：暴力地在时间和空间两个维度计算注意力，但内存占用太大，难以实现</li>
<li>T+S：借鉴了R(2+1)D的思想，先在时间上计算自注意力，再在空间上计算自注意力</li>
<li>L+G：先在局部的小窗口计算自注意力，再在全局计算自注意力</li>
<li>T+W+H：只沿着特定的轴做attention，</li>
</ul>
<p>以上几种方法进行可视化后如下图所示，其中蓝色的色块表示基准点，其它相同颜色的色块则表示用于同基准点计算attention。</p>
<img src="/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/image-20221015084134927.png" alt="image-20221015084134927" style="zoom: 67%; margin: auto;">

<ul>
<li>L+G：其中黄色的色块表示计算local attention的色块，紫色的色块表示计算global attention的色块（为了降低计算量，global也缩小了范围）</li>
</ul>
<h4 id="transformer未来方向"><a href="#transformer未来方向" class="headerlink" title="transformer未来方向"></a>transformer未来方向</h4><p>在视频理解领域，transformer大致有如下几个研究方向：</p>
<ol>
<li>利用transformer长时间建模的能力，进行长视频理解</li>
<li>多模态学习</li>
<li>自监督学习</li>
</ol>
]]></content>
      <tags>
        <tag>视频理解</tag>
        <tag>综述</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Masked Autoencoders Enable Efficient Knowledge Distillers</title>
    <url>/2022/10/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AMasked-Autoencoders-Enable-Efficient-Knowledge-Distillers/</url>
    <content><![CDATA[<ul>
<li>参考：<a href="https://mp.weixin.qq.com/s/q0Bq-R2ZD_lOmxdhW2qy1w">https://mp.weixin.qq.com/s/q0Bq-R2ZD_lOmxdhW2qy1w</a></li>
<li>Code：<a href="https://github.com/UCSC-VLAA/DMAE">https://github.com/UCSC-VLAA/DMAE</a></li>
</ul>
<h2 id="论文简介">论文简介</h2>
<p>作者在MAE框架的基础上加入知识蒸馏技术，在MAE的预训练阶段进行特征对齐蒸馏，提出DMAE，探索了MAE从预训练模型中获取知识的能力。</p>
<p>（感觉这篇文章的意义就是把一个大的MAE，通过知识蒸馏，蒸馏到了一个小的MAE模型上，这个小的MAE模型取得了很好的效果QAQ）</p>
<p>DMAE的整体框架如下图所示：</p>
<img src="https://mmbiz.qpic.cn/mmbiz_png/ibaXaPIy7jV1XPqibfEWTuS2fgQlJhamfNBMNHY1KW1UfgnupzicbFqGe5zrhyqreTHY2ORyBKe8qJ889knzBQjxw/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片" style="zoom:80%;">
]]></content>
      <tags>
        <tag>Transformer</tag>
        <tag>知识蒸馏</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：基于 rPPG 的生理指标测量方法综述</title>
    <url>/2022/10/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E5%9F%BA%E4%BA%8E-rPPG-%E7%9A%84%E7%94%9F%E7%90%86%E6%8C%87%E6%A0%87%E6%B5%8B%E9%87%8F%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/</url>
    <content><![CDATA[<ul>
<li>发表时间：2020</li>
</ul>
<h2 id="心动周期信号传统测量方法"><a href="#心动周期信号传统测量方法" class="headerlink" title="心动周期信号传统测量方法"></a>心动周期信号传统测量方法</h2><p>心动周期：从一次心跳的起始到下一次心跳的起始，心血管系统经历的过程。</p>
<p>测量心动周期信号主要有如下两种方法：</p>
<ol>
<li>心电图（ECG）：接触式测量，通过记录心脏活动产生的生物电信号获得；</li>
<li>光电容积脉搏波描记法（PPG）：接触式测量，实时测量特定身体部位因心脏搏动产生的对不同波长光照吸收&#x2F;反射量的周期性变化，这种信号也叫做血液体积脉冲（BVP，Blood Volume Pulse）信号。</li>
</ol>
<h2 id="rPPG技术的原理"><a href="#rPPG技术的原理" class="headerlink" title="rPPG技术的原理"></a>rPPG技术的原理</h2><pre class="mermaid">graph TD
D[心动周期造成皮肤颜色变化]-->A[摄像头捕捉皮肤颜色周期性变化]
A-->B[根据颜色变化提取血液体积脉冲信号BVP]
B-->C[测量心动周期相关生理指标心率和心跳变异性等]</pre>

<h2 id="如何选取合适的ROI？"><a href="#如何选取合适的ROI？" class="headerlink" title="如何选取合适的ROI？"></a>如何选取合适的ROI？</h2><p>选取合适的ROI可以达到<strong>过滤背景噪音</strong>的效果，ROI的选取大致可以分为如下两种：</p>
<ol>
<li>选择定义好的感兴趣区域；</li>
<li>使用算法自动挑选感兴趣区域。</li>
</ol>
<h2 id="如何提取BVP信号？"><a href="#如何提取BVP信号？" class="headerlink" title="如何提取BVP信号？"></a>如何提取BVP信号？</h2><p>BVP信号的提取即从ROI区域中选取BVP信号，可以分为三种。</p>
<h3 id="基于皮肤光照模型的方法"><a href="#基于皮肤光照模型的方法" class="headerlink" title="基于皮肤光照模型的方法"></a>基于皮肤光照模型的方法</h3><p>通过对皮肤光照反射模型进行建模，依据皮肤的颜色变化，获取毛细血管内的血红蛋白含量变化，从而推断血流量变化。</p>
<p><strong>使用皮肤光照模型的先验知识缓解各种因素带来的信号不稳定。</strong></p>
<p>对皮肤模型的建模方法可分为两种：</p>
<ol>
<li>建模皮肤对不同波长光照的吸收和反射情况，利用皮肤对不同波长光照吸收的差异性和互补性提取和增强BVP信号；</li>
<li>考虑不同头部的姿态和运动状况对光照在皮肤上的吸收和反射情况的影响，从而对BVP信号进行校正与增强。</li>
</ol>
<p>典型算法：</p>
<ul>
<li>CHROM（2013）：基于色度的颜色空间投影分解算法，消除头部运动的影响；</li>
<li>标准朗博体模型：将人脸看作标准朗博体模型，建模表示不同头部运动状态下皮肤对光照的吸收和反射情况，从而消除头部运动的影响。</li>
</ul>
<h3 id="基于BVP信号特性假设的方法"><a href="#基于BVP信号特性假设的方法" class="headerlink" title="基于BVP信号特性假设的方法"></a>基于BVP信号特性假设的方法</h3><p>该方法是利用BVP信号的特性<strong>去除噪声</strong>。关于BVP信号的噪声有如下几种假设：</p>
<ul>
<li>线性混合假设：不同颜色通道或不同ROI内的信号是BVP信号与噪声信号线性组合来的。</li>
<li>流形混合假设：非线性组合，BVP信号是不同颜色通道信号在原始颜色空间的流形子空间上的投影。</li>
<li>信号同步假设：不同ROI内的信号是同步的，因此所有ROI中不同步的部分就是噪声信号。</li>
<li>周期性假设：BVP信号具有周期性，正常心率在相邻几次不容易发生周期变化，在频域上利用这种特性去除噪声。</li>
</ul>
<h3 id="基于数据驱动的方法"><a href="#基于数据驱动的方法" class="headerlink" title="基于数据驱动的方法"></a>基于数据驱动的方法</h3><p>利用纯净的BVP信号，通过机器学习建模<strong>过滤噪声</strong>。其实就是基于深度学习的方法。这种方法受限于数据规模和多样性，同时对于预测结果缺乏可解释性。</p>
<h2 id="如何通过BVP信号计算各种生理指标？"><a href="#如何通过BVP信号计算各种生理指标？" class="headerlink" title="如何通过BVP信号计算各种生理指标？"></a>如何通过BVP信号计算各种生理指标？</h2><p>对于BVP信号，可以从频域和时域进行分析。</p>
<h3 id="频域分析"><a href="#频域分析" class="headerlink" title="频域分析"></a>频域分析</h3><ul>
<li>平均心率：频谱中对应最大频谱强度的周期；</li>
<li>呼吸率：BVP信号最大频谱幅值对应的周期；</li>
</ul>
<h3 id="时域分析"><a href="#时域分析" class="headerlink" title="时域分析"></a>时域分析</h3><ul>
<li>心跳变异性（HRV）：检测BVP信号的峰值点，计算相邻峰值点间的时间间隔，从而计算瞬时心率以及瞬时心率时序变化。</li>
</ul>
<h2 id="rPPG算法评测"><a href="#rPPG算法评测" class="headerlink" title="rPPG算法评测"></a>rPPG算法评测</h2><h3 id="评测任务"><a href="#评测任务" class="headerlink" title="评测任务"></a>评测任务</h3><p>使用BVP信号可以测量的生理指标包括：心率、呼吸率、心跳变异性，大部分文献中只使用平均心率作为算法的评测指标。</p>
<ul>
<li>平均心率：对于长视频（30s）的平均心率估计缺乏应用价值，现提出使用短视频（4s，6s，8s）的平均心率估计作为评价指标。<strong>这种方法不能评价算法是否提取了每一个心动周期的波形变化。</strong></li>
<li>HRV：可以反映算法是否提取了BVP信号中每个心动周期的波形变化。</li>
</ul>
<p><strong>使用HRV和呼吸率可以更好地反映算法的有效性，但数据集中提供两种值的较少。</strong></p>
<h3 id="评测指标"><a href="#评测指标" class="headerlink" title="评测指标"></a>评测指标</h3><ul>
<li>生理指标测量的准确性：评价生理指标测量值与参考值间的误差。包括MAE、RMSE、SD、Mean、MER等。<strong>人类的心率和心跳变异性的个体差异较小，当预测值恒为所有样本的平均结果时，也能取得较低的评测误差</strong>，为了解决这个问题，研究将测量误差小于特定阈值样本占总样本的比例作为测量准确性的评价指标，或计算测量值与参考值间的<strong>皮尔逊相关性系数</strong>，度量测量值与参考值间的一致性。</li>
<li>提取BVP信号的可靠性：评估提取的BVP信号的质量，常用信噪比SNR用于评估，即<strong>预测心率</strong>频率对应的频域能量占整个频域能量的比值，可以反映提取到的BVP信号的周期性。</li>
</ul>
<h3 id="评测协议"><a href="#评测协议" class="headerlink" title="评测协议"></a>评测协议</h3><p>对于数据驱动方法，数据集划分对评测结果影响显著。常用如下几种评测协议：</p>
<ol>
<li>训练&#x2F;测试集固定划分：容易受到训练偏差影响，使用较少。</li>
<li>与人无关的多折测试：同一被试者的数据可同时出现在训练集和测试集，被试者的生理指标在数据录制时不会发生较大变化，应避免采用。</li>
<li>按人划分的多折测试：去除了个体特征的影响，但同一数据集录制环境较为相似，不能作为唯一的评测协议。</li>
<li>跨数据集测试：训练集和测试集来自不同数据集。</li>
</ol>
<h2 id="现存问题"><a href="#现存问题" class="headerlink" title="现存问题"></a>现存问题</h2><ol>
<li>输入视频质量低：视频压缩、图像预处理（手机摄像头为了获得更好的视觉效果对视频进行的自动处理）造成视频质量降低，影响对微弱颜色变化的监测。</li>
<li>被试者头部运动复杂：头部运动（平移、旋转、眨眼、说话）造成人脸皮肤反射光的变化。头部的俯仰和偏航旋转会造成皮肤区域的遮挡和皮肤光照反射的变化，目前还难以消除。</li>
<li>实际环境光照多变：环境光照较暗时，皮肤对光照的反射和吸收量小，会产生较多噪声；在驾驶场景下，环境光照变化会叠加到皮肤的周期性颜色变化上。</li>
<li>数据集规模和多样性不足，多数只包含平均心率值。</li>
</ol>
<h2 id="研究方向"><a href="#研究方向" class="headerlink" title="研究方向"></a>研究方向</h2><ol>
<li>对低质量视频、手机成像视频进行研究；</li>
<li>⭐使用无监督、半监督、弱监督等充分利用无标注样本在内的各种数据；</li>
<li>⭐利用心动周期的生物物理模型和相关知识，引导机器学习模型的设计和训练；</li>
<li>构建包含各种挑战性环境光照场景、各种实际运动场景的人脸视频数据集；</li>
<li>克服头部俯仰和偏航旋转造成的皮肤区域遮挡和皮肤光照反射的变化，对生理指标测量产生的影响；</li>
</ol>
<h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><ol>
<li><p>rPPG信号的测量受环境光照、被试者的头部运动影响显著。</p>
<p>CHROM、朗博体模型、人脸跟踪；</p>
</li>
<li><p>如何把关于BVP信号的先验知识应用到基于深度学习的方法中？</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>rPPG</tag>
      </tags>
  </entry>
  <entry>
    <title>对比学习</title>
    <url>/2022/10/20/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<ul>
<li>参考博客：<a href="https://zhuanlan.zhihu.com/p/367290573">https://zhuanlan.zhihu.com/p/367290573</a></li>
<li>参考视频：<a href="https://www.bilibili.com/video/BV19S4y1M7hm">https://www.bilibili.com/video/BV19S4y1M7hm</a></li>
</ul>
<h2 id="何为对比学习？"><a href="#何为对比学习？" class="headerlink" title="何为对比学习？"></a>何为对比学习？</h2><p>对比学习是无监督或自监督学习方法的一种，其思想与NLP领域中的Bert类似，即从大量的无标签数据中吸取图像的先验知识分布，再在下游任务上微调，从而将预训练学习到的知识，迁移到下游任务上。</p>
<p>对比学习是一种判别式自监督学习方法，对于对比学习，尚不存在明确的定义，但其存在一套固定的指导原则：通过==构造相似实例和不相似实例==，训练得到一个==表示学习模型==，通过这个模型，使得相似的实例在投影空间中比较接近，不相似的实例在投影空间中距离比较远，然后将这个表示学习模型用到下游任务上进行==微调==。</p>
<p>对比学习有如下几个关键点：</p>
<ol>
<li>如何构造相似、不相似实例；</li>
<li>如何构造表示学习模型结构；</li>
<li>如何防止模型坍缩。</li>
</ol>
<p>在MoCo任务中，认为对比学习的关键点包括：</p>
<ol>
<li>代理任务</li>
<li>目标函数</li>
</ol>
<p>在李沐的对比学习视频中，将对比学习划分为四个阶段，并对每个阶段的代表性工作进行了介绍。</p>
<h2 id="百花齐放"><a href="#百花齐放" class="headerlink" title="百花齐放"></a>百花齐放</h2><h3 id="InstDisc-Instance-Discrimination"><a href="#InstDisc-Instance-Discrimination" class="headerlink" title="InstDisc(Instance Discrimination)"></a>InstDisc(Instance Discrimination)</h3><p>论文：<em>Unsupervised Feature Learning via Non-parametric Instance Discrimination</em></p>
<p>发表时间：2018</p>
<p>评价：对比学习的开山之作，之后的很多模型（MOCO等）都是在此基础上进行的。</p>
<p>模型结构：使用一个卷积网络，将每个图片编码为一个特征，目的是希望这些特征在特征空间中尽可能分开。</p>
<p>正样本：图片本身</p>
<p>负样本：所有其它图片</p>
<p>损失函数：使用NCE Loss作为对比学习的损失函数</p>
<p>Memory Bank：存储所有图片的特征，memory bank里的特征是动量更新的。</p>
<p><img src="/2022/10/20/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/image-20221022163248461.png" alt="image-20221022163248461" style="zoom:80%; margin:auto;"></p>
<p>主要贡献：</p>
<ol>
<li>提出了代理任务<strong>个体判别</strong>，将每个实例看作一个类别，目的是学习一个特征，从而把每个图片区分开来。</li>
<li>提出对特征进行动量的更新。</li>
</ol>
<h3 id="InvaSpread"><a href="#InvaSpread" class="headerlink" title="InvaSpread"></a>InvaSpread</h3><p>论文：<em>Unsupervised Embedding Learning via Invariant and Spreading Instance Feature</em></p>
<p>发表时间：2019</p>
<p>评价：SimCLR的前身</p>
<p>模型结构：没有使用额外的结构存储负样本，正负样本来自同一个mini batch，只使用一个编码器进行端到端的学习。</p>
<p>基本思想：相似的图片具有相似的特征，不同的图片特征差异也大。</p>
<p>正样本：</p>
<p>负样本：</p>
<h2 id="CV双雄"><a href="#CV双雄" class="headerlink" title="CV双雄"></a>CV双雄</h2><h2 id="不用负样本"><a href="#不用负样本" class="headerlink" title="不用负样本"></a>不用负样本</h2><h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2>]]></content>
      <tags>
        <tag>对比学习</tag>
        <tag>综述</tag>
      </tags>
  </entry>
  <entry>
    <title>经典模型：MoCo</title>
    <url>/2022/10/22/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AMoCo/</url>
    <content><![CDATA[<p>MoCo系列是在视觉领域使用对比学习的里程碑之作。共推出了三个版本。</p>
<h2 id="MoCo-v1"><a href="#MoCo-v1" class="headerlink" title="MoCo v1"></a>MoCo v1</h2><ul>
<li>论文名称：<em>Momentum Contrast for Unsupervised Visual Representation Learning</em></li>
<li>发表信息：CVPR 2020，Kaiming He</li>
<li>主要贡献：提出了一种构造大而一致字典的方法，提供一个稳定的自监督信号。同时MoCo的训练成本相比于其它对比学习方法，更加affordable。</li>
</ul>
<h3 id="前置信息"><a href="#前置信息" class="headerlink" title="前置信息"></a>前置信息</h3><h4 id="动量"><a href="#动量" class="headerlink" title="动量"></a>动量</h4><p>$y_t&#x3D;my_{t-1}+(1-m)x_t$，目的是让当前时刻的输出不仅仅依赖于当前时刻的输入，也要考虑上一时刻的输出。</p>
<p>在MoCo中，作者利用动量的特性，缓慢更新编码器，从而让中间学习字典中的特征尽可能保持一致。</p>
<h4 id="NCE-Loss-Noise-Contrastive-Estimation"><a href="#NCE-Loss-Noise-Contrastive-Estimation" class="headerlink" title="NCE Loss(Noise Contrastive Estimation)"></a>NCE Loss(Noise Contrastive Estimation)</h4><p>在有监督分类任务中，常使用Cross Entropy Loss计算loss，CE Loss的公式如下所示，其中K表示类别数。</p>
<p>$$S_i&#x3D;-log\frac{e^i}{\sum^K_{j&#x3D;0}e^j}$$</p>
<p>将CE Loss应用到对比学习任务上，由于类别数巨大，softmax操作将失效，同时由于样本数多，计算复杂度极高。为了解决这个问题，在对比学习中提出了NCE Loss，进行了如下改进：</p>
<ol>
<li>将类别数归为data sample和noise sample两类；</li>
<li>只从数据集中抽取部分作为noise sample去计算。（样本越大，近似效果越好，因此MoCo要构造一个大字典）</li>
</ol>
<h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><h4 id="动态字典"><a href="#动态字典" class="headerlink" title="动态字典"></a>动态字典</h4><p>在NLP任务中，对比学习取得了很好的效果，因为NLP任务中的输入来自离散的信号空间，可以生成tokenize的字典，可以利用这个字典，展开无监督的学习（把字典中的每个key看作一个类别，构造有监督学习范式），建模比较容易。而视觉任务的输入是在一个连续的、高维的空间中，语义信息较弱。</p>
<p>作者受NLP任务的启发，将对比学习中的正负样本生成过程，看作NLP中构造字典的过程。对于如下对比学习过程，$x_1$代表原始图像，$x_1^1$和$x_1^2$是对其经过两种不同的变换得到的，一般把前者看作anchor，后者看作positive，将anchor和positive输入编码器$E_{11}$和$E_{12}$（两个编码器模型可以相同，也可以不同），得到两个输出特征$f_{11}$和$f_{12}$，对于其它的原始图像，全部看作相对于anchor的negative，经过编码器$E_{12}$得到输出特征。</p>
<p>字典是针对&#x3D;&#x3D;特征&#x3D;&#x3D;而言的，作者将$f_{11}$看作NLP中的query，其余特征全部看作key。</p>
<img src="/2022/10/22/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AMoCo/image-20221023100541076.png" alt="image-20221023100541076" style="zoom:67%; margin:auto;">

<p>作者认为，为了获得良好的效果，字典应该具有两个特性：</p>
<ol>
<li>尽可能的大（更好地从连续的高维的空间中做抽样）</li>
<li>在训练时保持一致性（字典中的key使用相同或相似的编码器得到，使得其与query对比时，不受编码器不同的干扰）</li>
</ol>
<h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>Moco模型的结构与上述结构类似，只有queue和momentum encoder两部分有所不同。</p>
<img src="/2022/10/22/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AMoCo/image-20221023162341556.png" alt="image-20221023162341556" style="zoom: 67%; margin: auto;">

<ul>
<li>queue：用队列存储字典，当新的mini-batch的特征进入字典时，队首的特征需要移出队列，并非每次更新整个字典，因此字典的大小可以设的非常大。</li>
<li>momentum encoder：为了使得队列中的特征尽量保持一致，采用动量策略限制编码器的更新。</li>
</ul>
<p>通过队列与动量编码器两种策略，MoCo实现了构造一个又大又一致的字典。</p>
<h4 id="对比学习配置"><a href="#对比学习配置" class="headerlink" title="对比学习配置"></a>对比学习配置</h4><ul>
<li><p>代理任务：InstDisc，若query和key是同一个图像的不同视角，则认为其可配对。</p>
</li>
<li><p>损失函数：InfoNCE，NCE loss的变体，作者任务将分类问题看作简单的二分类还是太粗暴了，Info loss直观地看是一个K+1类的CE Loss（其中K是负样本数），目的是把q分成$k_+$这一类。</p>
</li>
</ul>
<img src="/2022/10/22/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AMoCo/image-20221023171055941.png" alt="image-20221023171055941" style="zoom: 50%; margin:auto;">

<ol>
<li><p>其中超参数是一个温度参数，用于控制分布形状。</p>
<ol start="2">
<li>当温度参数变小时，分布将变得更集中（模型将只关注困难样本，将导致模型难以收敛，不好泛化）。</li>
</ol>
</li>
</ol>
]]></content>
      <tags>
        <tag>对比学习</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Synthetic Generation of Face Videos with Plethysmograph Physiology</title>
    <url>/2022/10/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ASynthetic-Generation-of-Face-Videos-with-Plethysmograph-Physiology/</url>
    <content><![CDATA[<h2 id="文章概述"><a href="#文章概述" class="headerlink" title="文章概述"></a>文章概述</h2><ul>
<li>发表信息：2022 CVPR</li>
</ul>
<h3 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h3><ol>
<li>提出了一个可扩展的基于物理的学习模型，可以根据潜在的血容量变化，以高保真度渲染真实的rPPG视频（可以使用任意图像和目标rPPG信号作为生成模型的输入）；</li>
<li>合成的视频可以直接用于提高rPPG模型的性能中去，另外，可以用渲染模型为少数群体生成数据，从而减少rPPG框架中人口统计学的偏差；</li>
<li>发布了一个真实的rPPG数据集UCLA-rPPG，这个数据集包含了不同肤色的数据。</li>
</ol>
<h2 id="模型框架"><a href="#模型框架" class="headerlink" title="模型框架"></a>模型框架</h2><p>模型的整体框架如图所示：</p>
<ul>
<li>输入人脸图像：根据种族分类的BUPT-Balancedface数据集</li>
<li>输入PPG信号：BIDMC PPG和Respiration数据集</li>
</ul>
<img src="/2022/10/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ASynthetic-Generation-of-Face-Videos-with-Plethysmograph-Physiology/image-20221024094446621.png" alt="image-20221024094446621" style="zoom:80%; margin:auto;">

<p>对于输入图像，被编码为UV反射图、3D Mesh、光照模型L和相机模型c，UV反射图经分解得到Blood Map，然后根据输入的rPPG信号，将Blood Map修改为PPG的UV图，PPG的UV图与光照模型L和相机模型c根据随机动作生成最终的合成的rPPG视频。</p>
<p><em>ps: 各个步骤具体的实现专业性较强，且与我的主要工作相关性较弱，待之后用到时再进行学习。</em></p>
<h3 id="模型评价指标"><a href="#模型评价指标" class="headerlink" title="模型评价指标"></a>模型评价指标</h3><p> 模型的损失函数由两部分组成：</p>
<ol>
<li>Loss of appearance：重建的UV map和原始线性RGB空间的map间的L2距离；</li>
<li>Loss of CameraPrior：相机光敏灵敏度的先验值。</li>
</ol>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>文章的实验在作者制作的UCLA-rPPG数据集上进行。</p>
<h3 id="UCLA-rPPG数据集"><a href="#UCLA-rPPG数据集" class="headerlink" title="UCLA-rPPG数据集"></a>UCLA-rPPG数据集</h3><ul>
<li>subjects：102个具有各种肤色（肤色类型根据Fitzpatrick指标从1~6变化）、年龄、性别、种族和民族的人</li>
<li>帧率：30fps</li>
<li>视频数：503</li>
<li>时长：1min</li>
</ul>
<h3 id="实验方法"><a href="#实验方法" class="headerlink" title="实验方法"></a>实验方法</h3><p>为了验证合成的数据集对真实数据集训练模型的提升，作者先在合成的数据集上进行模型，再在真实的数据集上进行微调，从而验证合成数据集的作用。</p>
<p>同时，为了探究模型对于不同肤色类型人群的效果，将subjects根据Fitzpatrick的6个皮肤等级，划分为3种类型。</p>
<p>实验结果显示，使用合成的数据集训练+真实数据集微调的方法，取得了比单独使用真实数据集更好的效果。</p>
<img src="/2022/10/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ASynthetic-Generation-of-Face-Videos-with-Plethysmograph-Physiology/image-20221025104458708.png" alt="image-20221025104458708" style="zoom:80%; margin:auto;">

<h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h3><p>为了探究肤色对模型的影响，作者根据肤色类型进行了消融实验，实验结果如下图所示：</p>
<img src="/2022/10/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ASynthetic-Generation-of-Face-Videos-with-Plethysmograph-Physiology/image-20221025111711200.png" alt="image-20221025111711200" style="zoom:80%; margin:auto;">

<p>由实验结果可知，在所有肤色上训练得到的模型要好过在单一肤色上训练得到的模型。</p>
<h3 id="跨数据集测试"><a href="#跨数据集测试" class="headerlink" title="跨数据集测试"></a>跨数据集测试</h3><p>将在UCLA数据集上训练得到的模型直接应用到UBFC数据集上，从实验结果可以看出，使用合成的数据集训练得到的模型具有更好的泛化能力。</p>
<img src="/2022/10/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ASynthetic-Generation-of-Face-Videos-with-Plethysmograph-Physiology/image-20221025141510331.png" alt="image-20221025141510331" style="zoom: 67%; margin:auto;">

<h2 id="模型评价"><a href="#模型评价" class="headerlink" title="模型评价"></a>模型评价</h2><p>模型仍存在如下限制：</p>
<ol>
<li>生成的视频不是photo-realistic，可能会因为sim2rel gap产生负面影响；</li>
<li>生成的视频不包含背景，但有研究发现，背景可以利用进行更好的信号提取；</li>
<li>直接对Blood UV map根据目标rPPG信号进行线性变换，作者相信使用基于生物物理学的方法可以达到更好的效果；</li>
</ol>
<p>除了将这个模型用于生成rPPG视频外，还可以用于绕过基于rPPG的deepfake detectors。</p>
]]></content>
      <tags>
        <tag>rPPG</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：The Way to my Heart is through Contrastive Learning:Remote PPG from Unlabelled Video</title>
    <url>/2022/10/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AThe-Way-to-my-Heart-is-through-Contrastive-Learning-Remote-PPG-from-Unlabelled-Video/</url>
    <content><![CDATA[<ul>
<li>发表信息：ICCV 2021</li>
<li>参考博客：<a href="https://blog.csdn.net/m0_46792836/article/details/122203112">https://blog.csdn.net/m0_46792836/article/details/122203112</a></li>
<li>Code：<a href="https://github.com/ToyotaResearchInstitute/RemotePPG">https://github.com/ToyotaResearchInstitute/RemotePPG</a></li>
</ul>
<h2 id="模型结构">模型结构</h2>
<p>基于对比学习的自监督心率估计模型如下图所示，模型流程如下：</p>
<p>首先从一段视频中抽取W秒的视频<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>a</mi></msub></mrow><annotation encoding="application/x-tex">x_a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，将<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>a</mi></msub></mrow><annotation encoding="application/x-tex">x_a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>输入显著性采样器S，得到扭曲的视频切片<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>x</mi><mi>a</mi><mi>s</mi></msubsup></mrow><annotation encoding="application/x-tex">x^s_a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.911392em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>，对于<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>x</mi><mi>a</mi><mi>s</mi></msubsup></mrow><annotation encoding="application/x-tex">x^s_a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.911392em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>，通过几种不同的操作输入PPG信号估计器<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">g_{\theta}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>：</p>
<ol>
<li>直接输入信号估计器，得到预测值<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>a</mi></msub></mrow><annotation encoding="application/x-tex">y_a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>；</li>
<li>从先验分布中采样随机频率比<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>f</mi></msub></mrow><annotation encoding="application/x-tex">r_f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>，将<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>f</mi></msub></mrow><annotation encoding="application/x-tex">r_f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>与<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>x</mi><mi>a</mi><mi>s</mi></msubsup></mrow><annotation encoding="application/x-tex">x^s_a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.911392em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>输入频率重采样器R，得到负样本<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>x</mi><mi>n</mi><mi>s</mi></msubsup></mrow><annotation encoding="application/x-tex">x_n^s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.911392em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>，将负样本输入信号估计器得到负样本的PPG信号<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">y_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>；</li>
<li>对负样本的PPG信号<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">y_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，使用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>f</mi></msub></mrow><annotation encoding="application/x-tex">r_f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>的逆进行重采样，得到正样本的PPG信号<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">y_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>。</li>
</ol>
<p>对于得到的三个输出<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>a</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>n</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">y_a,y_n,y_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>，使用PSE、MSE度量距离，使得<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>a</mi></msub></mrow><annotation encoding="application/x-tex">y_a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>与<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">y_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的距离尽可能远，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>a</mi></msub></mrow><annotation encoding="application/x-tex">y_a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>与<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">y_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>的距离尽可能接近。</p>
<img src="/2022/10/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AThe-Way-to-my-Heart-is-through-Contrastive-Learning-Remote-PPG-from-Unlabelled-Video/image-20221020174825059.png" alt="image-20221020174825059" style="zoom:67%; margin:auto;">
<h3 id="显著性采样器【可选】">显著性采样器【可选】</h3>
<p>输入的视频序列要经过一个显著性采样器S的处理。在模型中，使用一个截断的ResNet18网络（使用ImageNet上的预训练网络初始化）作为显著性采样器，使用显著性采样器有两个目的：</p>
<ol>
<li>为rPPG估计器正在学习的内容提供透明度，当标注数据少时有意义；</li>
<li>对输入图像进行变形，以在空间上加强任务显著性区域。</li>
</ol>
<p>对于显著性采样器，使用两个损失项进行训练：</p>
<img src="/2022/10/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AThe-Way-to-my-Heart-is-through-Contrastive-Learning-Remote-PPG-from-Unlabelled-Video/image-20221021090651144.png" alt="image-20221021090651144" style="zoom: 67%; margin:auto;">
<ul>
<li>Sparsitiy：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>s</mi><mi>i</mi><mi>j</mi></msubsup></mrow><annotation encoding="application/x-tex">s_i^j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.219436em;vertical-align:-0.276864em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.942572em;"><span style="top:-2.4231360000000004em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.1809080000000005em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.276864em;"><span></span></span></span></span></span></span></span></span></span>指的是显著性map中第i帧的位置j处的值，这个损失项偏向于熵更小，也就是空间上稀疏的解（只关注重点区域）。</li>
<li>Temporal：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>d</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mi>j</mi></msubsup></mrow><annotation encoding="application/x-tex">d_{i,i+1}^j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.355544em;vertical-align:-0.412972em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.942572em;"><span style="top:-2.4231360000000004em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.1809080000000005em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.412972em;"><span></span></span></span></span></span></span></span></span></span>指的是相邻两帧在像素j的显著性差异，这个损失项偏向于相邻帧间平滑的解。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># apply sparsity loss </span></span><br><span class="line"><span class="keyword">if</span> args.ss_sparsity &gt; <span class="number">0</span>:</span><br><span class="line">    entropy = -torch.<span class="built_in">sum</span>(torch.log(ss_out[<span class="number">1</span>].view(-<span class="number">1</span>)) * ss_out[<span class="number">1</span>].view(-<span class="number">1</span>))</span><br><span class="line">    loss += entropy * args.ss_sparsity / torch.numel(ss_out[<span class="number">1</span>])</span><br><span class="line"><span class="comment"># apply temporal consistency loss</span></span><br><span class="line"><span class="keyword">if</span> args.ss_temporal &gt; <span class="number">0</span>:</span><br><span class="line">    ss_diff = ss_out[<span class="number">1</span>][:,<span class="number">1</span>:,:,:,:] - ss_out[<span class="number">1</span>][:,:-<span class="number">1</span>,:,:,:]</span><br><span class="line">    ssd = torch.<span class="built_in">sum</span>(ss_diff.view(-<span class="number">1</span>) * ss_diff.view(-<span class="number">1</span>))</span><br><span class="line">    loss += ssd * args.ss_temporal / torch.numel(ss_out[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>经过训练后，对显著性图进行可视化如图所示：</p>
<img src="/2022/10/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AThe-Way-to-my-Heart-is-through-Contrastive-Learning-Remote-PPG-from-Unlabelled-Video/image-20221021091723473.png" alt="image-20221021091723473" style="zoom:67%; margin:auto;">
<h3 id="PPG估计器">PPG估计器</h3>
<p>使用PhysNet的改进版本作为PPG估计器，主要是修改了PhysNet的Decoder部分，作者认为PhysNet在将输出返回到原始视频长度时，在输出的PPG信号中引入了混叠，作者在这里采用上采样插值与3D卷积消除混叠。PPG估计器的模型架构如下图所示：</p>
<img src="/2022/10/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AThe-Way-to-my-Heart-is-through-Contrastive-Learning-Remote-PPG-from-Unlabelled-Video/image-20221021093510555.png" alt="image-20221021093510555" style="zoom: 67%; margin:auto;">
<h2 id="对比学习关注问题">对比学习关注问题</h2>
<h3 id="正负样本构造">正负样本构造</h3>
<p>通过模型部分的介绍，我们知道，负样本是通过重采样得到的，作者选用与原始输入频率不同的输入作为负样本。</p>
<img src="/2022/10/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AThe-Way-to-my-Heart-is-through-Contrastive-Learning-Remote-PPG-from-Unlabelled-Video/image-20221021094926639.png" alt="image-20221021094926639" style="zoom:67%; margin:auto;">
<p>从代码中可以更具体地看到实现细节：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># iccv\src\archs\FrequencyContrast.py</span></span><br><span class="line"></span><br><span class="line">D = x_a.shape[<span class="number">2</span>]	<span class="comment"># 每个视频片段的帧数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Resample input</span></span><br><span class="line">freq_factor = <span class="number">1.25</span> + (torch.rand(<span class="number">1</span>, device=x_a.device) / <span class="number">4</span>)	<span class="comment"># 随机产生1.25~1.5之间的频率因子</span></span><br><span class="line">target_size = <span class="built_in">int</span>(D / freq_factor)	</span><br><span class="line">resampler = nn.Upsample(size=(target_size, x_a.shape[<span class="number">3</span>], x_a.shape[<span class="number">4</span>]),</span><br><span class="line">                        mode=<span class="string">&#x27;trilinear&#x27;</span>,align_corners=<span class="literal">False</span>)	</span><br><span class="line">x_n = resampler(x_a)	<span class="comment"># [B, C, target_size, w, h]</span></span><br><span class="line">x_n = F.pad(x_n, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, D - target_size))<span class="comment"># &gt;&gt; [B, C, D, W, H]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Pass both samples through backbone</span></span><br><span class="line">y_a = self.backbone(x_a).squeeze(<span class="number">4</span>).squeeze(<span class="number">3</span>)	</span><br><span class="line">y_n = self.backbone(x_n).squeeze(<span class="number">4</span>).squeeze(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Remove padding from negative branch</span></span><br><span class="line">y_n = y_n[:,:,:target_size]	<span class="comment"># [B, 1, target_size]	# 负样本</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Resample negative PPG to create positive branch</span></span><br><span class="line">self.upsampler = nn.Upsample(size=(dataset.options.D,), mode=<span class="string">&#x27;linear&#x27;</span>, align_corners=<span class="literal">False</span>)	</span><br><span class="line">y_p = self.upsampler(y_n)	<span class="comment"># 正样本</span></span><br></pre></td></tr></table></figure>
<h3 id="损失函数">损失函数</h3>
<p>使用多视角三态损失（MVTL）作为对比训练的损失函数，根据作者的假设，心率在一定的时间内是相对稳定的，即在每个视图中的信号应该是相似的。</p>
<p>MVTL计算锚点和正视图的所有组合的距离<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{tot}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，锚点和负视图的所有组合的距离<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">N_{tot}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，使用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow></msub><mo>−</mo><msub><mi>N</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{tot}-N_{tot}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>并使用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><msup><mi>N</mi><mn>2</mn></msup></msub></mrow><annotation encoding="application/x-tex">V_{N^2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.86095em;vertical-align:-0.17762em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.52238em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7463142857142857em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.17762em;"><span></span></span></span></span></span></span></span></span></span>进行调节，作为监督学习的损失。在论文中，使用视频视图<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mi>N</mi></msub><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">V_N=4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">4</span></span></span></span>，视频长度<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mi>L</mi></msub><mo>=</mo><mn>5</mn><mi>s</mi></mrow><annotation encoding="application/x-tex">V_L=5s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">5</span><span class="mord mathnormal">s</span></span></span></span>。</p>
<p>对于<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{tot}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">N_{tot}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的计算，使用PSD MSE（功率谱密度均方误差）作为两个PPG信号间的距离度量，首先计算每个信号的PSD，然后使用MSE计算误差。</p>
<p>MVTL的计算代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># iccv\src\losses\MultiViewTripletLoss.py</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate NormPSD for each branch, if needed</span></span><br><span class="line">num_temp_views = <span class="built_in">len</span>(branches[<span class="string">&#x27;anc&#x27;</span>])	<span class="comment"># num_views</span></span><br><span class="line"><span class="keyword">if</span> self.norm_psd <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> branches.keys():</span><br><span class="line">        <span class="keyword">for</span> temp_i <span class="keyword">in</span> <span class="built_in">range</span>(num_temp_views):</span><br><span class="line">            branches[key][temp_i] = self.norm_psd(branches[key][temp_i])	<span class="comment"># 计算PSD</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Tally the triplet loss</span></span><br><span class="line">pos_loss = self.compare_view_lists(branches[<span class="string">&#x27;anc&#x27;</span>], branches[<span class="string">&#x27;pos&#x27;</span>])</span><br><span class="line"><span class="comment"># 计算两分支views间的loss</span></span><br><span class="line">neg_loss = self.compare_view_lists(branches[<span class="string">&#x27;anc&#x27;</span>], branches[<span class="string">&#x27;neg&#x27;</span>])	</span><br><span class="line"><span class="keyword">return</span> (pos_loss - neg_loss) / num_temp_views * num_temp_views</span><br></pre></td></tr></table></figure>
<h3 id="学习模型的选择与使用">学习模型的选择与使用</h3>
<p>由于对比学习没有标注数据，作者使用<strong>无关功率比IPR</strong>作为验证指标，首先对输出PPG信号计算PSD，将信号频率分为相关频率（根据文章假设选取40-250bpm）与无关频率，IPR即不相关范围内的功率/总功率，从在验证集上损失最小的epoch中，选取IPR最低的模型。</p>
<h2 id="一些思考">一些思考</h2>
<ol>
<li>显著性采样器好像并没有太大效果，甚至在对比学习任务上取得了负效果。</li>
</ol>
<img src="/2022/10/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AThe-Way-to-my-Heart-is-through-Contrastive-Learning-Remote-PPG-from-Unlabelled-Video/image-20221021183916735.png" alt="image-20221021183916735" style="zoom: 67%; margin:auto;">
<ol start="2">
<li>正负样本的构造方法、以及对比学习技术的应用，或许可以基于一些新的对比学习方法，需要再学习一下。</li>
<li>既然可以考虑自监督的任务，那么是否也可以考虑连续学习、迁移学习、域自适应任务？</li>
</ol>
]]></content>
      <tags>
        <tag>rPPG</tag>
        <tag>对比学习</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：XCiT: Cross-Covariance Image Transformers</title>
    <url>/2022/10/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AXCiT-Cross-Covariance-Image-Transformers/</url>
    <content><![CDATA[<h2 id="模型原理"><a href="#模型原理" class="headerlink" title="模型原理"></a>模型原理</h2><h3 id="模型框架"><a href="#模型框架" class="headerlink" title="模型框架"></a>模型框架</h3><img src="/2022/10/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AXCiT-Cross-Covariance-Image-Transformers/image-20221020111505288.png" alt="image-20221020111505288" style="zoom:67%; margin:auto;">

<h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><ul>
<li><a href="https://github.com/facebookresearch/xcit">https://github.com/facebookresearch/xcit</a></li>
</ul>
]]></content>
      <tags>
        <tag>Transformer</tag>
        <tag>项目</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Dual-GAN: Joint BVP and Noise Modeling for Remote Physiological Measurement</title>
    <url>/2022/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADual-GAN-Joint-BVP-and-Noise-Modeling-for-Remote-Physiological-Measurement/</url>
    <content><![CDATA[<ul>
<li>发表信息：CVPR 2021</li>
</ul>
<p><em>又是一篇没有代码的论文。。。看个思想吧</em></p>
<h2 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h2><h3 id="对偶学习"><a href="#对偶学习" class="headerlink" title="对偶学习"></a>对偶学习</h3><p><em>参考博客：<a href="https://www.jiqizhixin.com/articles/2016-11-18">https://www.jiqizhixin.com/articles/2016-11-18</a></em></p>
<p>对偶学习最早应用在NLP领域，利用互为对偶的人工智能任务（例如中译英和英译中，语音识别和语音合成等），利用互为对偶的任务相互提供反馈，使从没有标注的数据中学习成为可能，为解决无监督学习提供了新的思路。</p>
<p>例如，对于英汉互译模型，只有英文句子x作为输入，首先输入模型1翻译为中文，再由模型2翻译为英文，通过比较这个英文句子和原始的英文句子，就可以对两个翻译模型进行评价，从而实现无监督学习。</p>
<img src="https://pic.36krcnd.com/201708/08083825/nhnzya3bdxiaoz42" alt="img" style="zoom:100%; margin:auto;">

<p><em>这种方法其实与强化学习方法的思想相似，利用对偶游戏结果作为反馈。</em></p>
<h3 id="Dual-GAN"><a href="#Dual-GAN" class="headerlink" title="Dual GAN"></a>Dual GAN</h3><p>Dual GAN就是将对偶学习的思想引入GAN中，进行无监督学习，将GAN原来的生成和识别两个问题，进一步扩展为两个相互耦合的GAN，即使用两个生成器和判别器。</p>
<p>例如对于素描与照片的转换任务，生成器A将照片1转换为素描，再由生成器B转化为照片，对于素描2，由生成器B转为照片，再由生成器A转换为素描，通过衡量两次转后得到的素描与照片与初始输入的差距，对生成器A、B进行评估。</p>
<h3 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h3><ul>
<li>STMap：Spatial-Temporal Map，即时空图</li>
</ul>
<h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p>文章使用的Dual GAN架构如图所示，使用非直接监督的方式，实现了对BVP信号预测器和噪声分布的建模。</p>
<img src="/2022/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADual-GAN-Joint-BVP-and-Noise-Modeling-for-Remote-Physiological-Measurement/image-20221026091934621.png" alt="image-20221026091934621" style="zoom:67%; margin:auto;">

<h3 id="模型基于的想法"><a href="#模型基于的想法" class="headerlink" title="模型基于的想法"></a>模型基于的想法</h3><ol>
<li>不同ROI的时间信号，应该有不同的噪声分布和BVP信号分布；</li>
<li>使用合成了人工噪声的生理信号，有利于基于基于DL的生理信号测量方法；</li>
<li>很多工作忽略了对主导视频内容的背景噪声的建模。</li>
</ol>
<h3 id="BVP-Modeling"><a href="#BVP-Modeling" class="headerlink" title="BVP Modeling"></a>BVP Modeling</h3><p>该部分首先从视频帧序列中获取STMap，然后进行对抗学习：</p>
<ul>
<li>输入：STMap</li>
<li>生成器：从STMap中预测BVP信号（和心率信号，这是一个额外的任务）；</li>
<li>判别器：判断BVP信号是真实的还是合成的；</li>
</ul>
<p>对于生成器，为了缓解不同ROI区域BVP信号和噪声分布的不同，作者提出了一个即插即用的ROI对齐与融合模块（ROI-AF Block，ROI Assignment and Fusion Block），使用ROI-wise的卷积，从一个更大的感受野中融合BVP特征。</p>
<p>ROI-AF Block的结构如图所示。其中N代表ROI数，C代表通道数，L代表帧数。在ST Map中，一行代表一个ROI区域，首先在Alignment阶段，对每个ROI区域进行特征对齐；在Fusion阶段，上部的Global Average Pooling和FC层作为通道注意力模型，获得融合的特征映射，最后将输出reshape到与原来相同的维度。</p>
<img src="/2022/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADual-GAN-Joint-BVP-and-Noise-Modeling-for-Remote-Physiological-Measurement/image-20221026112336966.png" alt="image-20221026112336966" style="zoom:80%; margin:auto;">

<p>需要注意的是，这里的BVP信号估计器需要完成两个任务，其中HR估计的任务用于辅助模型训练。BVP信号估计器的整体结构如图：</p>
<img src="/2022/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADual-GAN-Joint-BVP-and-Noise-Modeling-for-Remote-Physiological-Measurement/image-20221026214637932.png" alt="image-20221026214637932" style="zoom:67%; margin:auto;">

<p>对于判别器，同时将STMap和BVP信号作为输入，并且与后面的噪声模型共享同一个判别器。</p>
<img src="/2022/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADual-GAN-Joint-BVP-and-Noise-Modeling-for-Remote-Physiological-Measurement/image-20221026214808146.png" alt="image-20221026214808146" style="zoom: 67%; margin:auto;">

<h3 id="Noise-Modeling"><a href="#Noise-Modeling" class="headerlink" title="Noise Modeling"></a>Noise Modeling</h3><p>该部分使用双通道的生成器，以实现生成带有噪声的STMap。</p>
<ul>
<li>输入：真实的BVP信号、随机的噪声变量；</li>
<li>STMap生成器：从真实的BVP信号生成STMap；</li>
<li>noise生成器：根据噪声变量生成STMap的噪声；</li>
<li>判别器：使用与上述相同的判别器，判断STMap是真实的还是合成的。</li>
</ul>
<h3 id="训练策略"><a href="#训练策略" class="headerlink" title="训练策略"></a>训练策略</h3><ol>
<li><p>训练BVP信号估计器。使用STMap信号作为输入，得到预测的BVP信号和心率数据，与对应的ground truth计算loss。</p>
<p>这里用信号估计器其实做了两个任务，即心率估计+BVP信号估计。</p>
</li>
</ol>
<img src="/2022/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADual-GAN-Joint-BVP-and-Noise-Modeling-for-Remote-Physiological-Measurement/image-20221026210519839.png" alt="image-20221026210519839" style="zoom:67%; margin:auto;">

<ol start="2">
<li>固定BVP信号估计器，训练STMap生成器。其目标是，使得使用真实的BVP信号，经过STMap生成器，生成的STMap经训练好的BVP估计器得到的BVP信号，与真实的BVP信号尽可能接近。（⭐这里可以看作对偶学习思想的体现）</li>
</ol>
<img src="/2022/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADual-GAN-Joint-BVP-and-Noise-Modeling-for-Remote-Physiological-Measurement/image-20221026210717463.png" alt="image-20221026210717463" style="zoom:67%; margin:auto;">

<ol start="3">
<li>训练BVP信号估计器和噪声生成器。通过最小化Ljoint（使得生成的与真实的尽可能接近）训练得到BVP信号估计器的噪声生成器。</li>
</ol>
<img src="/2022/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADual-GAN-Joint-BVP-and-Noise-Modeling-for-Remote-Physiological-Measurement/image-20221026211814134.png" alt="image-20221026211814134" style="zoom:67%; margin:auto;">

<ol start="4">
<li>固定所有生成器，训练判别器。通过最大化Ljoint（对抗学习嘛）实现。</li>
</ol>
<h2 id="模型评价"><a href="#模型评价" class="headerlink" title="模型评价"></a>模型评价</h2><p>Dual-GAN在HR、HRV和RF估计的任务上，在多个数据集上，进行intra-dataset和cross-dataset测试都达到了SOTA。</p>
<h3 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h3><ul>
<li>把Dual-GAN应用到生理信号测量中，增加对噪声分布的建模（使用Dual GAN可以解决缺乏训练数据集的问题），可以获得一个对未见过的噪声更健壮的模型；</li>
<li>提出了一个即插即用的ROI-AF模块，用在传统的卷积层之后，处理来自不同ROI的噪声与BVP信号的分布不一致；</li>
</ul>
<h2 id="拓展"><a href="#拓展" class="headerlink" title="拓展"></a>拓展</h2><ol>
<li><p>STMap和ROI区域是怎么得到的？</p>
<p>作者在文章中采用STMap计算方法，是Rhythmnet中的方法，我将在另一篇文章中进行介绍，而Rhythmnet的一作与Zitong Yu之后合作写了另一篇心率相关的文章CVD。</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>rPPG</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：RhythmNet: End-to-End Heart Rate Estimation From Face via Spatial-Temporal Representation</title>
    <url>/2022/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARhythmNet-End-to-End-Heart-Rate-Estimation-From-Face-via-Spatial-Temporal-Representation/</url>
    <content><![CDATA[<ul>
<li><p>发表信息：TIP 2020</p>
</li>
<li><p>Code：<a href="https://github.com/AnweshCR7/RhythmNet">https://github.com/AnweshCR7/RhythmNet</a></p>
<p><em>提出了VIPL-HR数据集和STMap的提取方法。</em></p>
</li>
</ul>
<h2 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h2><h3 id="基于视频的HR估计方法"><a href="#基于视频的HR估计方法" class="headerlink" title="基于视频的HR估计方法"></a>基于视频的HR估计方法</h3><p>现存的基于视频的HR估计方法主要可以分为两种：</p>
<ol>
<li>基于rPPG的方法：通过捕捉面部颜色变化提取HR信号；</li>
<li>基于BCG的方法：BCG即心冲击描记器，通过捕捉头部运动提取HR信号，这种头部运动是随着每次心跳，血液周期性地喷射产生的。</li>
</ol>
<p>现存的非接触式的HR估计方法都是基于rPPG的方法，原因是在约束较少的场景下，面部的颜色变化比微弱的头部运动更容易捕捉。</p>
<p>（2021年有篇文章用多模态的方法，联合使用了rPPG和BCG信号。）</p>
<h3 id="GRU-Gated-Recurrent-Unit"><a href="#GRU-Gated-Recurrent-Unit" class="headerlink" title="GRU(Gated Recurrent Unit)"></a>GRU(Gated Recurrent Unit)</h3><p>GRU是循环神经网络的一种，也是为了解决长期记忆和反向传播中的梯度等问题提出来的。其效果与LSTM相似，但更容易训练，其输入输出结构与普通RNN相同，如图所示，其中$x^t$表示当前节点的输入，$h^{t-1}$表示上一个节点传下来的隐状态。</p>
<img src="https://pic2.zhimg.com/v2-49244046a83e30ef2383b94644bf0f31_r.jpg" alt="img" style="zoom: 50%; margin:auto;">

<h2 id="模型评价"><a href="#模型评价" class="headerlink" title="模型评价"></a>模型评价</h2><p>模型旨在实现在低约束场景下的HR估计，包括人体姿势变化、光照变化和设备差异。</p>
<h3 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h3><ol>
<li>建立了一个可训练的端到端（端到端的方法更能满足真实使用场景的需求）的HR估计器，估计器可以应对<strong>低约束场景</strong>下的多种问题；</li>
<li>对相邻HR之间的关系使用GRU（Gated Recurrent Unit）进行有效建模；</li>
<li>建立了一个更接近真实场景的数据集VIPL-HR，包括三种不同的录制设备（RGB-D相机、智能手机和web-camera），涵盖了面部姿势、scale和光照的9种场景。</li>
</ol>
<img src="/2022/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARhythmNet-End-to-End-Heart-Rate-Estimation-From-Face-via-Spatial-Temporal-Representation/image-20221027094214511.png" alt="image-20221027094214511" style="zoom:67%; margin:auto;">

<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>RhythmNet的整体结构如图所示：</p>
<p>模型首先对输入视频进行人脸识别和人脸关键点检测，得到ROI，然后对ROI计算STMap，作为HR信号的低级特征。然后将STMap输入CNN-RNN模型，得到最终的HR。</p>
<img src="/2022/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARhythmNet-End-to-End-Heart-Rate-Estimation-From-Face-via-Spatial-Temporal-Representation/image-20221027101908561.png" alt="image-20221027101908561" style="zoom:67%; margin:auto;">

<h3 id="人脸识别、关键点检测和分割"><a href="#人脸识别、关键点检测和分割" class="headerlink" title="人脸识别、关键点检测和分割"></a>人脸识别、关键点检测和分割</h3><p>文章使用开源的人脸检测器SeetaFace在视频的每一帧检测人脸，并定位了81个面部关键点。对于定义的ROI区域，使用皮肤分割去除非人脸部分。</p>
<h3 id="STMap"><a href="#STMap" class="headerlink" title="STMap"></a>STMap</h3><p>STMap的整体流程如下图所示：</p>
<img src="/2022/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARhythmNet-End-to-End-Heart-Rate-Estimation-From-Face-via-Spatial-Temporal-Representation/image-20221027161400274.png" alt="image-20221027161400274" style="zoom:80%; margin:auto;">

<ol>
<li><p>根据检测到的面部关键点，对齐不同帧间的人脸，并将对齐后的人脸变换到YUV颜色空间；</p>
<p>YUV颜色空间是在皮肤分割中常用的色彩表示，其可由RGB色彩经如下变换得到。</p>
<img src="/2022/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARhythmNet-End-to-End-Heart-Rate-Estimation-From-Face-via-Spatial-Temporal-Representation/image-20221027164127947.png" alt="image-20221027164127947" style="zoom: 67%; margin:auto;">
</li>
<li><p>将整个人脸划分为n个ROI块；</p>
</li>
<li><p>对于每个ROI，计算ROI在每帧色彩通道上的均值，并将该ROI的信号逐帧连接起来。即得到Txnxc形状的输出即STMap。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对每一帧的ROI在通道上取均值</span></span><br><span class="line"><span class="keyword">for</span> idx, frame <span class="keyword">in</span> <span class="built_in">enumerate</span>(processed_frames[start_frame_index:end_frame_index]):</span><br><span class="line">    roi_blocks = chunkify(frame)</span><br><span class="line">    <span class="comment"># 每一个roi block</span></span><br><span class="line">    <span class="keyword">for</span> block_idx, block <span class="keyword">in</span> <span class="built_in">enumerate</span>(roi_blocks):</span><br><span class="line">        avg_pixels = cv2.mean(block)	<span class="comment"># 按通道取均值</span></span><br><span class="line">        spatio_temporal_map[idx, block_idx, <span class="number">0</span>] = avg_pixels[<span class="number">0</span>]</span><br><span class="line">        spatio_temporal_map[idx, block_idx, <span class="number">1</span>] = avg_pixels[<span class="number">1</span>]</span><br><span class="line">        spatio_temporal_map[idx, block_idx, <span class="number">2</span>] = avg_pixels[<span class="number">2</span>]</span><br></pre></td></tr></table></figure></li>
</ol>
<p>⭐这里有一个值得注意的策略是，对于某些帧可能识别不到人脸（当对象移动过快或旋转时），会导致STMap和HR信号的丢失。为了解决这个问题，作者随机对一小部分STMap在时间维度上进行mask，从而模拟这种数据丢失的情况，从而加强RhythmNet的健壮性。</p>
<h3 id="HR估计模型"><a href="#HR估计模型" class="headerlink" title="HR估计模型"></a>HR估计模型</h3><ul>
<li>输入：STMap</li>
<li>输出：HR</li>
</ul>
<p>这部分的主要创新点是对两相邻clips间的处理上使用了GRU。对于CNN backbone提取的特征，输入一层GRU中，对于每个clips输出对应的HR，取所有clips的均值作为最终该视频的HR。</p>
<h2 id="关于视频压缩的研究"><a href="#关于视频压缩的研究" class="headerlink" title="关于视频压缩的研究"></a>关于视频压缩的研究</h2><p>VIPL-HR数据集原始大小为1.05TB，为了方便用户下载，作者研究了不同的视频压缩方法和帧resize方法。</p>
<ul>
<li>视频压缩算法：MJPG, FMP4, DIVX, PIM1, X264</li>
<li>resize scales：1&#x2F;2，2&#x2F;3，3&#x2F;4</li>
</ul>
<p>作者使用上述几种方法对数据集进行处理，并选择了一个经典的HR估计方法Hann2013对压缩或resize后的视频进行检测，从而选择出对HR估计影响最小的压缩算法和resize尺度。实验结果如下图所示：</p>
<img src="/2022/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARhythmNet-End-to-End-Heart-Rate-Estimation-From-Face-via-Spatial-Temporal-Representation/image-20221028135023127.png" alt="image-20221028135023127" style="zoom:80%; margin:auto;">

<p>根据实验结果，作者选择了MJPG算法对视频进行压缩，并采用了2&#x2F;3的resize尺度，将数据集大小降低至了48GB。</p>
<p>之后作者又对其提出的RhythmNet测试了数据集压缩与不压缩的影响，结果如图所示：</p>
<img src="/2022/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARhythmNet-End-to-End-Heart-Rate-Estimation-From-Face-via-Spatial-Temporal-Representation/image-20221028140443111.png" alt="image-20221028140443111" style="zoom:67%; margin:auto;">]]></content>
      <tags>
        <tag>rPPG</tag>
      </tags>
  </entry>
</search>
