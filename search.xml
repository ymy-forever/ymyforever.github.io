<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>HelloWorld!</title>
    <url>/2020/08/29/HelloWorld/</url>
    <content><![CDATA[<h2 id="A-new-world"><a href="#A-new-world" class="headerlink" title="A new world!"></a>A new world!</h2><p>哈喽！历经一个下午博客终于搭建好了，原来是那么容易的一件事情，大一的时候想的很复杂，迟迟没能动手，现在也终于有了自己的小博客啦~</p>
<p>未来灌水的文章还是会首先发在CSDN上，这里会分享一些重大的经历~已经大三了！要更努力学习！不要被些奇奇怪怪的事情干扰，奥里给！</p>
]]></content>
  </entry>
  <entry>
    <title>经典模型：Vision Transformer</title>
    <url>/2022/09/26/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AVision-Transformer/</url>
    <content><![CDATA[<h2 id="模型介绍"><a href="#模型介绍" class="headerlink" title="模型介绍"></a>模型介绍</h2><ul>
<li>可参考博客：<a href="https://blog.csdn.net/qq_39478403/article/details/118704747">https://blog.csdn.net/qq_39478403/article/details/118704747</a></li>
</ul>
<h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>ViT主要使用Transformer的encoder部分</p>
<img src="/2022/09/26/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AVision-Transformer/image-20220926083713750.png" alt="image-20220926083713750" style="zoom: 67%; margin:auto;">

<ul>
<li>将一张图像分成若干个大小固定且相同的patch，将每个patch投影到线性空间中再加上位置编码</li>
<li>除了每个patch作为一个token外，在序列中添加一个额外的classification token</li>
</ul>
<p><strong>位置编码：（看下代码咋实现的）</strong></p>
<ul>
<li>使用可学习的一维位置编码（作者发现使用更高维的位置编码并没有带来显著的精度提升）</li>
</ul>
<p><strong>混合结构：</strong></p>
<ul>
<li>可以将原始图像使用CNN进行特征提取，将特征图按patch划分送入Transformer中。</li>
</ul>
<h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><p><strong>ViT使用：</strong></p>
<ul>
<li>ViT与Bert类似，先在大数据集上<strong>训练</strong>，再在downstream任务上<strong>微调</strong>。在微调时，将预训练用的预测头换成一个用0初始化的DxK的前馈层，其中K代表下游任务总的类别数。</li>
<li>在微调时，使用更高分辨率的图像可以获得更好的结果。（patch大小不变，输入序列变成，不影响网络结构），这样会导致之前训练得到的<strong>位置编码</strong>无意义，因此在原来的位置编码上进行一个2D的插值。</li>
</ul>
<p><strong>实验结果：</strong></p>
<ul>
<li>当<strong>考虑预训练的训练代价</strong>时，ViT以更低的代价达到了SOTA水平</li>
<li>在<strong>自监督</strong>问题上，ViT很有应用前景</li>
</ul>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><ul>
<li>Official Code：<a href="https://github.com/google-research/vision_transformer">https://github.com/google-research/vision_transformer</a></li>
<li>timm code：<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py">https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py</a></li>
</ul>
<h3 id="Patch-Embeddings"><a href="#Patch-Embeddings" class="headerlink" title="Patch Embeddings"></a>Patch Embeddings</h3><p>实现功能：将输入图像划分为若干个patch，并将每个patch拉平投影到D维。通过一个二维的卷积操作即可实现。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/patch_embed.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 2D Image to Patch Embedding</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">            self,</span></span><br><span class="line"><span class="params">            img_size=<span class="number">224</span>,</span></span><br><span class="line"><span class="params">            patch_size=<span class="number">16</span>,<span class="comment"># 每个patch大小为16x16x3</span></span></span><br><span class="line"><span class="params">            in_chans=<span class="number">3</span>,</span></span><br><span class="line"><span class="params">            embed_dim=<span class="number">768</span>,<span class="comment"># 将patch映射到768维</span></span></span><br><span class="line"><span class="params">            norm_layer=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">            flatten=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">            bias=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        img_size = to_2tuple(img_size)</span><br><span class="line">        patch_size = to_2tuple(patch_size)</span><br><span class="line">        self.img_size = img_size</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.grid_size = (img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>], img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>])</span><br><span class="line">        self.num_patches = self.grid_size[<span class="number">0</span>] * self.grid_size[<span class="number">1</span>]<span class="comment"># 每张图像对应patch个数</span></span><br><span class="line">        self.flatten = flatten</span><br><span class="line">        </span><br><span class="line">		<span class="comment"># 通过一步卷积操作实现嵌入</span></span><br><span class="line">        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias)</span><br><span class="line">        self.norm = norm_layer(embed_dim) <span class="keyword">if</span> norm_layer <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, C, H, W = x.shape</span><br><span class="line">        _<span class="keyword">assert</span>(H == self.img_size[<span class="number">0</span>], <span class="string">f&quot;Input image height (<span class="subst">&#123;H&#125;</span>) doesn&#x27;t match model (<span class="subst">&#123;self.img_size[<span class="number">0</span>]&#125;</span>).&quot;</span>)</span><br><span class="line">        _<span class="keyword">assert</span>(W == self.img_size[<span class="number">1</span>], <span class="string">f&quot;Input image width (<span class="subst">&#123;W&#125;</span>) doesn&#x27;t match model (<span class="subst">&#123;self.img_size[<span class="number">1</span>]&#125;</span>).&quot;</span>)</span><br><span class="line">        x = self.proj(x)<span class="comment"># 投影</span></span><br><span class="line">        <span class="keyword">if</span> self.flatten:</span><br><span class="line">            x = x.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># BCHW -&gt; BNC</span></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h3 id="可学习的嵌入"><a href="#可学习的嵌入" class="headerlink" title="可学习的嵌入"></a>可学习的嵌入</h3><ul>
<li>cls_token：为了与Bert保持一致，设置可学习的嵌入向量作为用于分类的类别向量。</li>
<li>pos_embed：由于自注意力机制具有<strong>扰动不变性</strong>（打乱tokens中的顺序并不会改变结果），因此需要位置编码标识位置信息。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cls_token</span></span><br><span class="line">self.cls_token = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, embed_dim)) <span class="keyword">if</span> class_token <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"><span class="comment"># pos_embed</span></span><br><span class="line">embed_len = num_patches <span class="keyword">if</span> no_embed_class <span class="keyword">else</span> num_patches + self.num_prefix_tokens</span><br><span class="line">self.pos_embed = nn.Parameter(torch.randn(<span class="number">1</span>, embed_len, embed_dim) * <span class="number">.02</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_pos_embed</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="keyword">if</span> self.no_embed_class:</span><br><span class="line">        <span class="comment"># 先加位置编码再拼接cls_token</span></span><br><span class="line">        <span class="comment"># position embedding does not overlap with class token, add then concat</span></span><br><span class="line">        x = x + self.pos_embed</span><br><span class="line">        <span class="keyword">if</span> self.cls_token <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = torch.cat((self.cls_token.expand(x.shape[<span class="number">0</span>], -<span class="number">1</span>, -<span class="number">1</span>), x), dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 先拼接cls_token再加位置编码</span></span><br><span class="line">        <span class="comment"># pos_embed has entry for class token, concat then add</span></span><br><span class="line">        <span class="keyword">if</span> self.cls_token <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = torch.cat((self.cls_token.expand(x.shape[<span class="number">0</span>], -<span class="number">1</span>, -<span class="number">1</span>), x), dim=<span class="number">1</span>)</span><br><span class="line">        x = x + self.pos_embed</span><br><span class="line">    <span class="keyword">return</span> self.pos_drop(x)</span><br></pre></td></tr></table></figure>

<h3 id="多头注意力机制"><a href="#多头注意力机制" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, num_heads=<span class="number">8</span>, qkv_bias=<span class="literal">False</span>, attn_drop=<span class="number">0.</span>, proj_drop=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">assert</span> dim % num_heads == <span class="number">0</span>, <span class="string">&#x27;dim should be divisible by num_heads&#x27;</span></span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        head_dim = dim // num_heads</span><br><span class="line">        self.scale = head_dim ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        self.qkv = nn.Linear(dim, dim * <span class="number">3</span>, bias=qkv_bias)</span><br><span class="line">        self.attn_drop = nn.Dropout(attn_drop)</span><br><span class="line">        self.proj = nn.Linear(dim, dim)</span><br><span class="line">        self.proj_drop = nn.Dropout(proj_drop)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, N, C = x.shape</span><br><span class="line">        qkv = self.qkv(x).reshape(B, N, <span class="number">3</span>, self.num_heads, C // self.num_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        q, k, v = qkv.unbind(<span class="number">0</span>)   <span class="comment"># make torchscript happy (cannot use tensor as tuple)</span></span><br><span class="line"></span><br><span class="line">        attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) * self.scale<span class="comment"># QK</span></span><br><span class="line">        attn = attn.softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        attn = self.attn_drop(attn)</span><br><span class="line"></span><br><span class="line">        x = (attn @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(B, N, C)<span class="comment"># 乘以权重</span></span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        x = self.proj_drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h3 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Mlp</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; MLP as used in Vision Transformer, MLP-Mixer and related networks</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_features, hidden_features=<span class="literal">None</span>, out_features=<span class="literal">None</span>, act_layer=nn.GELU, bias=<span class="literal">True</span>, drop=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        out_features = out_features <span class="keyword">or</span> in_features</span><br><span class="line">        hidden_features = hidden_features <span class="keyword">or</span> in_features</span><br><span class="line">        bias = to_2tuple(bias)</span><br><span class="line">        drop_probs = to_2tuple(drop)</span><br><span class="line"></span><br><span class="line">        self.fc1 = nn.Linear(in_features, hidden_features, bias=bias[<span class="number">0</span>])</span><br><span class="line">        self.act = act_layer()</span><br><span class="line">        self.drop1 = nn.Dropout(drop_probs[<span class="number">0</span>])</span><br><span class="line">        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias[<span class="number">1</span>])</span><br><span class="line">        self.drop2 = nn.Dropout(drop_probs[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.act(x)</span><br><span class="line">        x = self.drop1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.drop2(x)</span><br><span class="line">        <span class="keyword">return</span> </span><br></pre></td></tr></table></figure>

<h3 id="Block"><a href="#Block" class="headerlink" title="Block"></a>Block</h3><p>实现功能：ViT的每个Block包括一层Attention和一层MLP。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Block</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">            self,</span></span><br><span class="line"><span class="params">            dim,</span></span><br><span class="line"><span class="params">            num_heads,</span></span><br><span class="line"><span class="params">            mlp_ratio=<span class="number">4.</span>,</span></span><br><span class="line"><span class="params">            qkv_bias=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">            drop=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">            attn_drop=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">            init_values=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">            drop_path=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">            act_layer=nn.GELU,</span></span><br><span class="line"><span class="params">            norm_layer=nn.LayerNorm</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.norm1 = norm_layer(dim)</span><br><span class="line">        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)</span><br><span class="line">        self.ls1 = LayerScale(dim, init_values=init_values) <span class="keyword">if</span> init_values <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        <span class="comment"># <span class="doctag">NOTE:</span> drop path for stochastic depth, we shall see if this is better than dropout here</span></span><br><span class="line">        self.drop_path1 = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">        self.norm2 = norm_layer(dim)</span><br><span class="line">        self.mlp = Mlp(in_features=dim, hidden_features=<span class="built_in">int</span>(dim * mlp_ratio), act_layer=act_layer, drop=drop)</span><br><span class="line">        self.ls2 = LayerScale(dim, init_values=init_values) <span class="keyword">if</span> init_values <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        self.drop_path2 = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))</span><br><span class="line">        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>







]]></content>
      <tags>
        <tag>经典模型</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：IDPT, Interconnected Dual Pyramid Transformer for Face Super-Resolution</title>
    <url>/2022/08/14/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AIDPT/</url>
    <content><![CDATA[<ul>
<li><p>研究问题：人脸超分辨率技术</p>
</li>
<li><p>FSR：关注于恢复重要的面部结构</p>
</li>
<li><p>创新点：提出了一个新的、有效的基于Transformer的人脸超分辨率架构</p>
<ol>
<li>设计了金字塔结构的encode&#x2F;decoder的Transformer架构：分别提取粗糙纹理和精细纹理。</li>
<li>通过一个底部的金字塔特征提取器，将双重金字塔Transformer建立起联系。</li>
<li>在每个spatial layer插入一个新的融合调制模块：使用粗糙纹理完善对应的精细纹理，融合浅层的粗糙纹理和对应的深层的精细纹理。</li>
</ol>
</li>
<li><p>FSR研究现状</p>
<ol>
<li>现有技术在解决超低分辨率问题上表现很差</li>
<li>卷积难以描述不同域间的关联和捕捉远域间的依赖</li>
</ol>
</li>
<li><p>网络结构： </p>
<img src="/2022/08/14/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AIDPT/1.png" alt="image-20220821153651772" style="zoom:80%;"></li>
</ul>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>FSR</tag>
        <tag>组内文章</tag>
      </tags>
  </entry>
  <entry>
    <title>经典模型：Swin Transformer</title>
    <url>/2022/09/26/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9ASwin-Transformer/</url>
    <content><![CDATA[<h2 id="模型结构与代码实现">模型结构与代码实现</h2>
<ul>
<li>参考博客：<a href="https://blog.csdn.net/qq_39478403/article/details/120042232?spm=1001.2014.3001.5506">https://blog.csdn.net/qq_39478403/article/details/120042232?spm=1001.2014.3001.5506</a></li>
<li>code：<a href="https://github.com/microsoft/Swin-Transformer">https://github.com/microsoft/Swin-Transformer</a></li>
</ul>
<p>Swin（即Shifted Windows） Transformer可以作为CV的一种通用主干，用在分类、检测、语义分割等多种视觉任务上。</p>
<p>Swin Transformer的提出解决了ViT具有的以下两个问题：</p>
<ol>
<li>ViT中，由于每个token的size大小相同，难以捕捉<strong>多尺度</strong>信息。</li>
<li>ViT的自注意力<strong>计算复杂度</strong>是图像大小的二次方。</li>
</ol>
<p>Swin-T构造了层次化特征图，并将自注意力的计算复杂度降为线性相关。</p>
<h3 id="整体结构">整体结构</h3>
<p>Swin-T的整体架构如下图所示：</p>
<img src="https://img-blog.csdnimg.cn/20210908164930810.png" alt="img" style="zoom:100%; margin:auto;">
<h3 id="Patch-Partition">Patch Partition</h3>
<p>对于每个为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>×</mo><mi>W</mi><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">H \times W \times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span>的输入，划分为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>×</mo><mn>4</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">4 \times 4 \times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span>大小的patch，每张图像被拆分为个patches，将每个patch展平作为一个token。</p>
<h3 id="Linear-Embedding">Linear Embedding</h3>
<p>即一个全连接层，将每个大小为48的token映射到设定的维度C，此时，每张图片的输入变为了<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>H</mi><mn>4</mn></mfrac><mo>×</mo><mfrac><mi>W</mi><mn>4</mn></mfrac><mo>×</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">\frac{H}{4} \times \frac{W}{4} \times C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>，然后输入Swin Transformer Block。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Patch Partition + Linear Embedding</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Image to Patch Embedding</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        img_size (int): Image size.  Default: 224.</span></span><br><span class="line"><span class="string">        patch_size (int): Patch token size. Default: 4.</span></span><br><span class="line"><span class="string">        in_chans (int): Number of input image channels. Default: 3.</span></span><br><span class="line"><span class="string">        embed_dim (int): Number of linear projection output channels. Default: 96.</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module, optional): Normalization layer. Default: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">4</span>, in_chans=<span class="number">3</span>, embed_dim=<span class="number">96</span>, norm_layer=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        img_size = to_2tuple(img_size)</span><br><span class="line">        patch_size = to_2tuple(patch_size)</span><br><span class="line">        patches_resolution = [img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>], img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>]]</span><br><span class="line">        self.img_size = img_size</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.patches_resolution = patches_resolution</span><br><span class="line">        self.num_patches = patches_resolution[<span class="number">0</span>] * patches_resolution[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        self.in_chans = in_chans</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">		<span class="comment"># 通过一个2维卷积实现patch partition与linear embedding</span></span><br><span class="line">        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br><span class="line">        <span class="keyword">if</span> norm_layer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.norm = norm_layer(embed_dim)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.norm = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, C, H, W = x.shape</span><br><span class="line">        <span class="comment"># FIXME look at relaxing size constraints</span></span><br><span class="line">        <span class="keyword">assert</span> H == self.img_size[<span class="number">0</span>] <span class="keyword">and</span> W == self.img_size[<span class="number">1</span>], \</span><br><span class="line">            <span class="string">f&quot;Input image size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) doesn&#x27;t match model (<span class="subst">&#123;self.img_size[<span class="number">0</span>]&#125;</span>*<span class="subst">&#123;self.img_size[<span class="number">1</span>]&#125;</span>).&quot;</span></span><br><span class="line">        x = self.proj(x).flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># 先划分patch与投影，再展平与交换</span></span><br><span class="line">        <span class="keyword">if</span> self.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = self.norm(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="Swin-Transformer-Block">Swin Transformer Block</h3>
<p>对于Transformers中使用的全局自注意力机制，需要计算每个token与其它所有tokens间的关系，计算复杂度为<strong>token数的平方</strong>。不适用于对大量tokens进行密集预测或表示高分辨率图像等视觉问题。</p>
<h4 id="W-MSA">W-MSA</h4>
<p>Swin-T通过<strong>在局部窗口中计算自注意力</strong>，将计算复杂度降低为token数的线性关系，设每个非重叠局部窗口中包含<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>×</mo><mi>M</mi></mrow><annotation encoding="application/x-tex">M\times M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span>个tokens。</p>
<ul>
<li>MSA：有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">hw</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">h</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span>个tokens，每个token在全局计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">hw</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">h</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span>次；</li>
<li>W-MSA：有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">hw</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">h</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span>个tokens，每个token在全局计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>M</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">M^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>次。</li>
</ul>
<p>首先将输入划分为若干个大小为MxM的窗口。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">window_partition</span>(<span class="params">x, window_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        x: (B, H, W, C)</span></span><br><span class="line"><span class="string">        window_size (int): window size</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        windows: (num_windows*B, window_size, window_size, C)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    B, H, W, C = x.shape</span><br><span class="line">    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)</span><br><span class="line">    windows = x.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>).contiguous().view(-<span class="number">1</span>, window_size, window_size, C)</span><br><span class="line">    <span class="keyword">return</span> windows</span><br></pre></td></tr></table></figure>
<h4 id="SW-MSA">SW-MSA</h4>
<p>W-MSA限制了跨窗口token间的交流与联系，从而限制了建模表征能力。作者提出了一种<strong>移位窗口划分</strong>方法SW-MSA，在模型中交替使用两种MSA方法（因此每个stage中Swin Transformer Block的数量都为偶数）。</p>
<p>所谓的移动窗口即将窗口循环位移，如下图所示：</p>
<img src="https://img-blog.csdnimg.cn/20210908164652285.png" alt="img" style="zoom:100%; margin:auto;">
<p>但直接移位得到的窗口大小是不规则的，不利于并行计算，同时9个窗口也提升了计算成本。为了解决这个问题，将重新划分后的窗口进行拼接，如下图所示，得到4个窗口。</p>
<img src="https://img-blog.csdnimg.cn/2021090920295156.png" alt="img" style="zoom:130%; margin:auto;">
<p>4个窗口中来自不同初始位置的patch不应进行自注意计算，因此使用mask机制，将不需要的注意力图置0。</p>
<p>W-MSA和SW-MSA公用一块代码实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">WindowAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Window based multi-head self attention (W-MSA) module with relative position bias.</span></span><br><span class="line"><span class="string">    It supports both of shifted and non-shifted window.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        window_size (tuple[int]): The height and width of the window.</span></span><br><span class="line"><span class="string">        num_heads (int): Number of attention heads.</span></span><br><span class="line"><span class="string">        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True</span></span><br><span class="line"><span class="string">        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set</span></span><br><span class="line"><span class="string">        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0</span></span><br><span class="line"><span class="string">        proj_drop (float, optional): Dropout ratio of output. Default: 0.0</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, window_size, num_heads, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">None</span>, attn_drop=<span class="number">0.</span>, proj_drop=<span class="number">0.</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.window_size = window_size  <span class="comment"># Wh, Ww</span></span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        head_dim = dim // num_heads</span><br><span class="line">        self.scale = qk_scale <span class="keyword">or</span> head_dim ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># define a parameter table of relative position bias</span></span><br><span class="line">        self.relative_position_bias_table = nn.Parameter(</span><br><span class="line">            torch.zeros((<span class="number">2</span> * window_size[<span class="number">0</span>] - <span class="number">1</span>) * (<span class="number">2</span> * window_size[<span class="number">1</span>] - <span class="number">1</span>), num_heads))  <span class="comment"># 2*Wh-1 * 2*Ww-1, nH</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># get pair-wise relative position index for each token inside the window</span></span><br><span class="line">        coords_h = torch.arange(self.window_size[<span class="number">0</span>])</span><br><span class="line">        coords_w = torch.arange(self.window_size[<span class="number">1</span>])</span><br><span class="line">        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  <span class="comment"># 2, Wh, Ww</span></span><br><span class="line">        coords_flatten = torch.flatten(coords, <span class="number">1</span>)  <span class="comment"># 2, Wh*Ww</span></span><br><span class="line">        relative_coords = coords_flatten[:, :, <span class="literal">None</span>] - coords_flatten[:, <span class="literal">None</span>, :]  <span class="comment"># 2, Wh*Ww, Wh*Ww</span></span><br><span class="line">        relative_coords = relative_coords.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).contiguous()  <span class="comment"># Wh*Ww, Wh*Ww, 2</span></span><br><span class="line">        relative_coords[:, :, <span class="number">0</span>] += self.window_size[<span class="number">0</span>] - <span class="number">1</span>  <span class="comment"># shift to start from 0</span></span><br><span class="line">        relative_coords[:, :, <span class="number">1</span>] += self.window_size[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">        relative_coords[:, :, <span class="number">0</span>] *= <span class="number">2</span> * self.window_size[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">        relative_position_index = relative_coords.<span class="built_in">sum</span>(-<span class="number">1</span>)  <span class="comment"># Wh*Ww, Wh*Ww</span></span><br><span class="line">        self.register_buffer(<span class="string">&quot;relative_position_index&quot;</span>, relative_position_index)</span><br><span class="line"></span><br><span class="line">        self.qkv = nn.Linear(dim, dim * <span class="number">3</span>, bias=qkv_bias)</span><br><span class="line">        self.attn_drop = nn.Dropout(attn_drop)</span><br><span class="line">        self.proj = nn.Linear(dim, dim)</span><br><span class="line">        self.proj_drop = nn.Dropout(proj_drop)</span><br><span class="line"></span><br><span class="line">        trunc_normal_(self.relative_position_bias_table, std=<span class="number">.02</span>)</span><br><span class="line">        self.softmax = nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: input features with shape of (num_windows*B, N, C)</span></span><br><span class="line"><span class="string">            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        B_, N, C = x.shape</span><br><span class="line">        qkv = self.qkv(x).reshape(B_, N, <span class="number">3</span>, self.num_heads, C // self.num_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        q, k, v = qkv[<span class="number">0</span>], qkv[<span class="number">1</span>], qkv[<span class="number">2</span>]  <span class="comment"># make torchscript happy (cannot use tensor as tuple)</span></span><br><span class="line"></span><br><span class="line">        q = q * self.scale</span><br><span class="line">        attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">		<span class="comment"># 相对位置偏移</span></span><br><span class="line">        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-<span class="number">1</span>)].view(</span><br><span class="line">            self.window_size[<span class="number">0</span>] * self.window_size[<span class="number">1</span>], self.window_size[<span class="number">0</span>] * self.window_size[<span class="number">1</span>], -<span class="number">1</span>)  <span class="comment"># Wh*Ww,Wh*Ww,nH</span></span><br><span class="line">        relative_position_bias = relative_position_bias.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>).contiguous()  <span class="comment"># nH, Wh*Ww, Wh*Ww</span></span><br><span class="line">        attn = attn + relative_position_bias.unsqueeze(<span class="number">0</span>)</span><br><span class="line">		<span class="comment"># 判断是否需要mask</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            nW = mask.shape[<span class="number">0</span>]</span><br><span class="line">            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">            attn = attn.view(-<span class="number">1</span>, self.num_heads, N, N)</span><br><span class="line">            attn = self.softmax(attn)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            attn = self.softmax(attn)</span><br><span class="line"></span><br><span class="line">        attn = self.attn_drop(attn)</span><br><span class="line"></span><br><span class="line">        x = (attn @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(B_, N, C)</span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        x = self.proj_drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h4 id="相对位置偏置">相对位置偏置</h4>
<p>在计算自注意力时，在计算相似度的过程中对每个head加入相对位置偏置，如下所示：</p>
<p><img src="https://img-blog.csdnimg.cn/20210911231011149.png" alt="img"></p>
<ul>
<li>对于预训练中学到的相对位置偏置，可以通过双三次插值初始化具有不同窗口大小的微调模型。</li>
</ul>
<h3 id="Patch-Merging">Patch Merging</h3>
<p>Patch Merging层的功能是产生一个层次化表示，通过<strong>合并相邻的tokens</strong>，减少tokens的数目。</p>
<img src="https://img-blog.csdnimg.cn/20210913201833609.png" alt="img" style="zoom:120%; margin:auto; ">
<p>对于Stage1和Stage2间的Patch Merging层，将原维度为C的token合并为大小为4C的token，再使用一个线性层将输出维度降低为2C，token的数目降低为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>H</mi><mn>8</mn></mfrac><mo>×</mo><mfrac><mi>W</mi><mn>8</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{H}{8} \times \frac{W}{8}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">8</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">8</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>。</p>
<p>在之后的每个stage中，都会改变张量的维度，从而形成一种层次化的特征。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchMerging</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Patch Merging Layer.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        input_resolution (tuple[int]): Resolution of input feature.</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_resolution, dim, norm_layer=nn.LayerNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.input_resolution = input_resolution</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.reduction = nn.Linear(<span class="number">4</span> * dim, <span class="number">2</span> * dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.norm = norm_layer(<span class="number">4</span> * dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        x: B, H*W, C</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        H, W = self.input_resolution</span><br><span class="line">        B, L, C = x.shape</span><br><span class="line">        <span class="keyword">assert</span> L == H * W, <span class="string">&quot;input feature has wrong size&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> H % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> W % <span class="number">2</span> == <span class="number">0</span>, <span class="string">f&quot;x size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) are not even.&quot;</span></span><br><span class="line"></span><br><span class="line">        x = x.view(B, H, W, C)<span class="comment"># 把输入整形为BHWC</span></span><br><span class="line"></span><br><span class="line">        x0 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x1 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x2 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x3 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x = torch.cat([x0, x1, x2, x3], -<span class="number">1</span>)  <span class="comment"># B H/2 W/2 4*C</span></span><br><span class="line">        x = x.view(B, -<span class="number">1</span>, <span class="number">4</span> * C)  <span class="comment"># B H/2*W/2 4*C</span></span><br><span class="line"></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        x = self.reduction(x)<span class="comment"># 降低维度为原来的1/2</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>经典模型</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Model Behavior Preserving for Class-Incremental Learning</title>
    <url>/2022/08/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AModel-Behavior-Preserving-for-Class-Incremental-Learning/</url>
    <content><![CDATA[<ul>
<li>发表时间：2022</li>
<li>研究内容：类增量学习，探讨在增量学习中应保留旧模型的哪些功能性属性。</li>
<li>研究现状：现有的增量学习方法忽略了CNN模型响应间的内部结构，KD的硬约束导致新模型出现混沌行为。</li>
<li>创新点：<ol>
<li><p>Feature Space：设计了一个INP Loss保持成对实例在旧模型上的<strong>相似性顺序</strong>（反映实例集间的相邻关系）；</p>
<p>INP用于惩罚新模型在学习过程中每个实例相邻关系的变化。</p>
<img src="/2022/08/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AModel-Behavior-Preserving-for-Class-Incremental-Learning/image-20220826093609961.png" alt="image-20220826093609961" style="zoom:67%;margin:auto;">

<p>a. 旧实例A在特征空间中与其它实例的相邻关系；</p>
<p>b. 采用传统的KD，引入新实例G后，绝对位置的微小变化被严格限制；</p>
<p>c. 采用INP Loss当相对位置没变时，就不会限制更新。</p>
</li>
<li><p>Label Space：设计了一个LPP Loss在输出空间的实例标签概率向量中保留<strong>标签排名列表</strong>（反映实例属于每一类的排名）；</p>
</li>
<li><p>介绍了一种可导的排名计算方法用于计算上述Loss。</p>
</li>
</ol>
</li>
</ul>
]]></content>
      <tags>
        <tag>组内文章</tag>
        <tag>连续学习</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：PhysFormer: Facial Video-based Physiological Measurement with Temporal Difference Transformer</title>
    <url>/2022/09/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9APhysFormer-Facial-Video-based-Physiological-Measurement-with-Temporal-Difference-Transformer/</url>
    <content><![CDATA[<ul>
<li><p>发表时间：2021</p>
</li>
<li><p>研究对象：rPPG，使用多波长 RGB 相机检测人体皮肤表面脉冲引起的细微颜色变化，实现测量心脏活动和其它生理信号。</p>
</li>
<li><p>研究意义：传统的检测方法会造成discomfort，并且长期检测不方便</p>
</li>
<li><p>Code：<a href="https://github.com/ZitongYu/PhysFormer">https://github.com/ZitongYu/PhysFormer</a></p>
</li>
<li><p>rPPG研究历史：</p>
<ol>
<li>早期使用经典的信号处理方法检测面部细微的颜色变化；</li>
<li>使用非端到端方法，首先生成预处理的信号特征，然后模型从这些特征图中捕捉rPPG特征（对预处理要求严格，忽略了全局特征）；</li>
<li>端到端的基于深度学习的方法（容易被复杂的背景信息影响）。</li>
</ol>
</li>
<li><p>研究现状：</p>
<p>​		现有的基于卷积神经网络的模型在时间和空间上的感受野受限，忽略了长期的时间和空间上的互动与感知。</p>
</li>
<li><p>PhysFormer网络架构：</p>
<img src="/2022/09/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9APhysFormer-Facial-Video-based-Physiological-Measurement-with-Temporal-Difference-Transformer/image-20220919092041506.png" alt="image-20220919092041506" style="zoom:80%;">

<ol>
<li><p><strong>Stem</strong>：由三个卷积块组成，用于提取粗糙的局部时空特征</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># INPUT:[B,3,T,H,W]</span></span><br><span class="line"><span class="comment"># OUTPUT:[B,D,T,H/8,W/8]</span></span><br><span class="line"><span class="comment"># use</span></span><br><span class="line">x = self.Stem0(x)</span><br><span class="line">x = self.Stem1(x)</span><br><span class="line">x = self.Stem2(x)  <span class="comment"># [B, 64, 160, 64, 64]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># implement</span></span><br><span class="line">self.Stem0 = nn.Sequential(</span><br><span class="line">    nn.Conv3d(<span class="number">3</span>, dim//<span class="number">4</span>, [<span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>], stride=<span class="number">1</span>, padding=[<span class="number">0</span>,<span class="number">2</span>,<span class="number">2</span>]),</span><br><span class="line">    nn.BatchNorm3d(dim//<span class="number">4</span>),</span><br><span class="line">    nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">    nn.MaxPool3d((<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>), stride=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">self.Stem1 = nn.Sequential(</span><br><span class="line">    nn.Conv3d(dim//<span class="number">4</span>, dim//<span class="number">2</span>, [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.BatchNorm3d(dim//<span class="number">2</span>),</span><br><span class="line">    nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">    nn.MaxPool3d((<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>), stride=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">self.Stem2 = nn.Sequential(</span><br><span class="line">    nn.Conv3d(dim//<span class="number">2</span>, dim, [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.BatchNorm3d(dim),</span><br><span class="line">    nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">    nn.MaxPool3d((<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>), stride=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Tube Tokens</strong>：将stem输出划分为若干个时空tube token，将时空邻近语义聚合在一起，并减少后续transformer的计算量。通过一个3D卷积实现。</p>
<p>tip：在嵌入后没有额外加上位置编码，因为在stem里已经捕捉了相关的时空信息。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># INPUT: # [B, 64, 160, 64, 64] ([B,D,T,H/8,W/8])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># use</span></span><br><span class="line">x = self.patch_embedding(x)  <span class="comment"># [B, 64, 40, 4, 4]</span></span><br><span class="line">x = x.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># [B, 40*4*4, 64](B,N,D)</span></span><br><span class="line"><span class="comment"># implement</span></span><br><span class="line"><span class="comment"># Patch embedding    [4x16x16]conv</span></span><br><span class="line">self.patch_embedding = nn.Conv3d(dim, dim, kernel_size=(ft, fh, fw), stride=(ft, fh, fw))</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Temporal Difference Multi-head Self-attention</strong>：</p>
<p>与传统的自注意力机制不同，使用TDC（Temporal Difference Convolution）查询Q和K的投影，可以捕捉局部细粒度的时间差异特征。</p>
<img src="/2022/09/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9APhysFormer-Facial-Video-based-Physiological-Measurement-with-Temporal-Difference-Transformer/image-20221002170008240.png" alt="image-20221002170008240" style="zoom:80%; margin:auto;">

<img src="/2022/09/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9APhysFormer-Facial-Video-based-Physiological-Measurement-with-Temporal-Difference-Transformer/image-20221002170042081.png" alt="image-20221002170042081" style="zoom: 67%; margin:auto;">

<img src="/2022/09/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9APhysFormer-Facial-Video-based-Physiological-Measurement-with-Temporal-Difference-Transformer/image-20221002170158926.png" alt="image-20221002170158926" style="zoom:67%; margin:auto;">

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 多头自注意力机制实现</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadedSelfAttention_TDC_gra_sharp</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Multi-Headed Dot Product Attention with depth-wise Conv3d&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, num_heads, dropout, theta</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        self.proj_q = nn.Sequential(</span><br><span class="line">            CDC_T(dim, dim, <span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">False</span>, theta=theta),  </span><br><span class="line">            nn.BatchNorm3d(dim),</span><br><span class="line">            <span class="comment">#nn.ELU(),</span></span><br><span class="line">        )</span><br><span class="line">        self.proj_k = nn.Sequential(</span><br><span class="line">            CDC_T(dim, dim, <span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">False</span>, theta=theta),  </span><br><span class="line">            nn.BatchNorm3d(dim),</span><br><span class="line">            <span class="comment">#nn.ELU(),</span></span><br><span class="line">        )</span><br><span class="line">        self.proj_v = nn.Sequential(</span><br><span class="line">            nn.Conv3d(dim, dim, <span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, groups=<span class="number">1</span>, bias=<span class="literal">False</span>),  </span><br><span class="line">            <span class="comment">#nn.BatchNorm3d(dim),</span></span><br><span class="line">            <span class="comment">#nn.ELU(),</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.drop = nn.Dropout(dropout)</span><br><span class="line">        self.n_heads = num_heads</span><br><span class="line">        self.scores = <span class="literal">None</span> <span class="comment"># for visualization</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, gra_sharp</span>):    <span class="comment"># [B, 4*4*40, 128]</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        x, q(query), k(key), v(value) : (B(batch_size), S(seq_len), D(dim))</span></span><br><span class="line"><span class="string">        mask : (B(batch_size) x S(seq_len))</span></span><br><span class="line"><span class="string">        * split D(dim) into (H(n_heads), W(width of head)) ; D = H * W</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># (B, S, D) -proj-&gt; (B, S, D) -split-&gt; (B, S, H, W) -trans-&gt; (B, H, S, W)</span></span><br><span class="line">        </span><br><span class="line">        [B, P, C]=x.shape</span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).view(B, C, P//<span class="number">16</span>, <span class="number">4</span>, <span class="number">4</span>)      <span class="comment"># [B, dim, 40, 4, 4]</span></span><br><span class="line">        q, k, v = self.proj_q(x), self.proj_k(x), self.proj_v(x)<span class="comment"># 由CDC计算得到Q和K，3D卷积得到V</span></span><br><span class="line">        q = q.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># [B, 4*4*40, dim]</span></span><br><span class="line">        k = k.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># [B, 4*4*40, dim]</span></span><br><span class="line">        v = v.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># [B, 4*4*40, dim]</span></span><br><span class="line">        </span><br><span class="line">        q, k, v = (split_last(x, (self.n_heads, -<span class="number">1</span>)).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="keyword">for</span> x <span class="keyword">in</span> [q, k, v])</span><br><span class="line">        <span class="comment"># (B, H, S, W) @ (B, H, W, S) -&gt; (B, H, S, S) -softmax-&gt; (B, H, S, S)</span></span><br><span class="line">        scores = q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>) / gra_sharp<span class="comment"># 矩阵相乘，计算得分（权重）</span></span><br><span class="line">		<span class="comment"># tip：在python里，“@”表示数学上的矩阵相乘，“*”表示矩阵对应位置两元素相乘</span></span><br><span class="line">        </span><br><span class="line">        scores = self.drop(F.softmax(scores, dim=-<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># (B, H, S, S) @ (B, H, S, W) -&gt; (B, H, S, W) -trans-&gt; (B, S, H, W)</span></span><br><span class="line">        h = (scores @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous()<span class="comment"># 计算结果</span></span><br><span class="line">        <span class="comment"># -merge-&gt; (B, S, D)</span></span><br><span class="line">        h = merge_last(h, <span class="number">2</span>)</span><br><span class="line">        self.scores = scores</span><br><span class="line">        <span class="keyword">return</span> h, scores</span><br><span class="line"><span class="comment"># TDC</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CDC_T</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 padding=<span class="number">1</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">False</span>, theta=<span class="number">0.6</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(CDC_T, self).__init__()</span><br><span class="line">        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,                                   padding=padding, dilation=dilation, groups=groups, bias=bias)</span><br><span class="line">        self.theta = theta</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out_normal = self.conv(x)<span class="comment"># 正常3D卷积输出</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> math.fabs(self.theta - <span class="number">0.0</span>) &lt; <span class="number">1e-8</span>:</span><br><span class="line">            <span class="keyword">return</span> out_normal</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># pdb.set_trace()</span></span><br><span class="line">            [C_out, C_in, t, kernel_size, kernel_size] = self.conv.weight.shape</span><br><span class="line"></span><br><span class="line">            <span class="comment"># only CD works on temporal kernel size&gt;1</span></span><br><span class="line">            <span class="keyword">if</span> self.conv.weight.shape[<span class="number">2</span>] &gt; <span class="number">1</span>:</span><br><span class="line">                kernel_diff = self.conv.weight[:, :, <span class="number">0</span>, :, :].<span class="built_in">sum</span>(<span class="number">2</span>).<span class="built_in">sum</span>(<span class="number">2</span>) + self.conv.weight[:, :,                               <span class="number">2</span>, :, :].<span class="built_in">sum</span>(<span class="number">2</span>).<span class="built_in">sum</span>(<span class="number">2</span>)</span><br><span class="line">                kernel_diff = kernel_diff[:, :, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">                <span class="comment"># 时间差异项</span></span><br><span class="line">                out_diff = F.conv3d(<span class="built_in">input</span>=x, weight=kernel_diff, bias=self.conv.bias,                                                     stride=self.conv.stride, padding=<span class="number">0</span>, dilation=self.conv.dilation,                                     groups=self.conv.groups)</span><br><span class="line">                <span class="keyword">return</span> out_normal - self.theta * out_diff</span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Spatio-temporal Feed-forward</strong>：在常用的两层线性transformation层之间，引入基于深度的3D卷积，ST-FF可以改善局部不一致性和部分噪声特征，同时丰富的局部性为TD-MHSA提供足够的相对位置信息，从而实现性能提升。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># use</span></span><br><span class="line"><span class="comment"># Transformer</span></span><br><span class="line">self.transformer1 = Transformer_ST_TDC_gra_sharp(num_layers=num_layers//<span class="number">3</span>, dim=dim, 		                             num_heads=num_heads, ff_dim=ff_dim, dropout=dropout_rate, theta=theta)</span><br><span class="line"><span class="comment"># Transformer</span></span><br><span class="line">self.transformer2 = Transformer_ST_TDC_gra_sharp(num_layers=num_layers//<span class="number">3</span>, dim=dim,                                       num_heads=num_heads, ff_dim=ff_dim, dropout=dropout_rate, theta=theta)</span><br><span class="line"><span class="comment"># Transformer</span></span><br><span class="line">self.transformer3 = Transformer_ST_TDC_gra_sharp(num_layers=num_layers//<span class="number">3</span>, dim=dim,                                       num_heads=num_heads, ff_dim=ff_dim, dropout=dropout_rate, theta=theta)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer_ST_TDC_gra_sharp</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Transformer with Self-Attentive Blocks&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_layers, dim, num_heads, ff_dim, dropout, theta</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.blocks = nn.ModuleList([</span><br><span class="line">            Block_ST_TDC_gra_sharp(dim, num_heads, ff_dim, dropout, theta) <span class="keyword">for</span> _ <span class="keyword">in</span>                                                      <span class="built_in">range</span>(num_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, gra_sharp</span>):</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self.blocks:</span><br><span class="line">            x, Score = block(x, gra_sharp)</span><br><span class="line">        <span class="keyword">return</span> x, Score</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Block_ST_TDC_gra_sharp</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Transformer Block&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, num_heads, ff_dim, dropout, theta</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.attn = MultiHeadedSelfAttention_TDC_gra_sharp(dim, num_heads, dropout, theta)</span><br><span class="line">        self.proj = nn.Linear(dim, dim)</span><br><span class="line">        self.norm1 = nn.LayerNorm(dim, eps=<span class="number">1e-6</span>)</span><br><span class="line">        self.pwff = PositionWiseFeedForward_ST(dim, ff_dim)<span class="comment"># ST-FF</span></span><br><span class="line">        self.norm2 = nn.LayerNorm(dim, eps=<span class="number">1e-6</span>)</span><br><span class="line">        self.drop = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, gra_sharp</span>):</span><br><span class="line">        Atten, Score = self.attn(self.norm1(x), gra_sharp)</span><br><span class="line">        h = self.drop(self.proj(Atten))</span><br><span class="line">        x = x + h</span><br><span class="line">        h = self.drop(self.pwff(self.norm2(x)))</span><br><span class="line">        x = x + h</span><br><span class="line">        <span class="keyword">return</span> x, Score</span><br><span class="line">    </span><br><span class="line"><span class="comment"># implement</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionWiseFeedForward_ST</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;FeedForward Neural Networks for each position&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, ff_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        self.fc1 = nn.Sequential(</span><br><span class="line">            nn.Conv3d(dim, ff_dim, <span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>),  </span><br><span class="line">            nn.BatchNorm3d(ff_dim),</span><br><span class="line">            nn.ELU(),</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.STConv = nn.Sequential(</span><br><span class="line">            nn.Conv3d(ff_dim, ff_dim, <span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, groups=ff_dim, bias=<span class="literal">False</span>),  </span><br><span class="line">            nn.BatchNorm3d(ff_dim),</span><br><span class="line">            nn.ELU(),</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.fc2 = nn.Sequential(</span><br><span class="line">            nn.Conv3d(ff_dim, dim, <span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>),  </span><br><span class="line">            nn.BatchNorm3d(dim),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):    <span class="comment"># [B, 4*4*40, 128]</span></span><br><span class="line">        [B, P, C]=x.shape</span><br><span class="line">        <span class="comment">#x = x.transpose(1, 2).view(B, C, 40, 4, 4)      # [B, dim, 40, 4, 4]</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).view(B, C, P//<span class="number">16</span>, <span class="number">4</span>, <span class="number">4</span>)      <span class="comment"># [B, dim, 40, 4, 4]</span></span><br><span class="line">        x = self.fc1(x)		              <span class="comment"># x [B, ff_dim, 40, 4, 4]</span></span><br><span class="line">        <span class="comment"># 使用时空卷积</span></span><br><span class="line">        x = self.STConv(x)		          <span class="comment"># x [B, ff_dim, 40, 4, 4]</span></span><br><span class="line">        x = self.fc2(x)		              <span class="comment"># x [B, dim, 40, 4, 4]</span></span><br><span class="line">        x = x.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># [B, 4*4*40, dim]</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (B, S, D) -&gt; (B, S, D_ff) -&gt; (B, S, D)</span></span><br><span class="line">        <span class="comment">#return self.fc2(F.gelu(self.fc1(x)))</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>创新点：PhysFormer，一种端到端的视频transformer，联合使用了局部的和全局的时空特征。</p>
<ol>
<li>使用<strong>时差引导全局注意力机制</strong>，强化rPPG的周期性特征，针对干扰完善局部时空特征；</li>
<li>使用受label distribution learning和curriculum learning启发的频域动态约束，为PhysFormer提供详细的监督，缓解过拟合。</li>
<li>PhysFormer不需要像其它transformer网络那样在大规模数据集上预训练，仅在rPPG数据集训练即可。</li>
</ol>
</li>
<li><p><strong>Label Distribution Learning</strong>：对于面部的rPPG信号，心率相近的视频会有相似的周期性特征。为了使得模型学习到这种特征，将心率估计问题看作一个多分类问题，有多少个心率就有多少类别，类别概率向量由高斯分布组成。</p>
</li>
<li><p><strong>Curriculum Learning Guided Dynamic Loss</strong>：课程式学习是指模型从容易样本开始学习，逐步学习困难样本。在该任务中，从时域和频域两个方面限制模型学习，时域的限制更直接更容易学习，频域的限制较难学习，因此，使用动态的loss函数，逐步提高频域loss的比例。</p>
</li>
<li><p>实验：不同模型的对比实验、消融实验</p>
</li>
<li><p>Others：注意力图可视化</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>Transformer</tag>
        <tag>组内文章</tag>
        <tag>rPPG</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Rethinking the ST-GCNs for 3D skeleton-based human action recognition</title>
    <url>/2022/09/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARethinking-the-ST-GCNs-for-3D-skeleton-based-human-action-recognition/</url>
    <content><![CDATA[<ul>
<li>发表时间：2021</li>
<li>ST-GCN：Spatial-Temporal Graph Convolutional Network，用于解决骨骼数据的动作识别问题。</li>
<li>研究内容：<ol>
<li>证明了在ST-GCN中很多操作对于人体动作识别是没必要的</li>
<li>提出了一个简单有效的策略捕捉全局图的相关性，对输入序列进行有效建模，同时将输入图序列降入欧几里得空间，可以使用多尺度时域滤波器捕捉动态信息。</li>
</ol>
</li>
<li>研究现状：<ol>
<li>骨骼数据成为人体动作识别的主流输入（与传统的RGB视频数据相比，信息更完整）</li>
<li>直接将结构化的数据重新排列，使得tensor适应基础的神经网络（由于骨骼数据中没有天然的局部性概念，深度学习的能力受到限制）</li>
<li>设计一种适应结构化数据的自定义神经网络（ST-GCN）</li>
</ol>
</li>
</ul>
<p>TBC：GCN好难，看不懂</p>
]]></content>
      <tags>
        <tag>组内文章</tag>
        <tag>人体动作识别</tag>
        <tag>GCN</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Structural Knowledge Organization and Transfer for Class-Incremental Learning</title>
    <url>/2022/08/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AStructural-Knowledge-Organization-and-Transfer-for-Class-Incremental-Learning/</url>
    <content><![CDATA[<ul>
<li><p>发表时间：2021</p>
</li>
<li><p>研究问题：类增量学习</p>
</li>
<li><p>研究现状：</p>
<ol>
<li><p><strong>经典的知识蒸馏方法</strong>忽略了信息点之间的关联，当新数据远多于旧数据时，面临着严重的偏差问题；</p>
<ul>
<li><p>KD：在特征空间中，孤立地限制单个训练样本的位置，样本间的关系可能会被改变，并导致分类错误。</p>
</li>
<li><p>SGKD：保持样本的结构化知识，包括样本的位置和样本间关系，确保蒸馏后样本仍能被正确地分类。</p>
</li>
</ul>
<img src="/2022/08/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AStructural-Knowledge-Organization-and-Transfer-for-Class-Incremental-Learning/image-20220822101131518.png" alt="image-20220822101131518" style="zoom:67%;margin:auto;"></li>
</ol>
</li>
<li><p>创新点：</p>
<ol>
<li><p>使用一个<strong>memory knowledge graph</strong>(MKG)表征历史任务的结构化知识</p>
<ul>
<li>在特征空间中的绝对位置（MKG中用顶点表示已知example间的特征向量）</li>
<li>example间对应关系（边表示，使用余弦距离）</li>
</ul>
</li>
<li><p>使用<strong>图插值机制</strong>丰富知识域、缓解类间样本不平衡问题</p>
<p>通过向MKG中插入假的顶点，扩充和平滑分散的数据集，假顶点通过mix两个真顶点的vector得到。</p>
</li>
<li><p>使用<strong>结构化图知识蒸馏（SGKD）</strong>迁移旧知识</p>
<ul>
<li>顶点蒸馏损失</li>
<li>边蒸馏损失</li>
</ul>
<img src="/2022/08/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AStructural-Knowledge-Organization-and-Transfer-for-Class-Incremental-Learning/image-20220823094630810.png" alt="image-20220823094630810" style="zoom:67%;margin:auto;"></li>
</ol>
</li>
<li><p>人脸识别是怎么实现增加新样本的？</p>
<p>人脸识别网络的本质是一个特征提取器，并不是分类器，识别人脸的时候，通过计算输出特征和人脸库中人脸的距离判断人脸所属对象。未涉及类增量学习。</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>组内文章</tag>
        <tag>连续学习</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Revisiting Pixel-Wise Supervision for Face Anti-Spoofing</title>
    <url>/2022/09/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARevisiting-Pixel-Wise-Supervision-for-Face-Anti-Spoofing/</url>
    <content><![CDATA[<ul>
<li><p>发表时间：2021</p>
</li>
<li><p>研究内容：像素级的人脸识别反欺诈方法</p>
</li>
<li><p>创新点：提出基于<strong>金字塔</strong>的监督方法，模型从多空间尺度上学习局部和全局的语义信息</p>
</li>
<li><p>Presentation Attack Detection研究历史：</p>
<ol>
<li><p>传统算法关注于<strong>活体</strong>和<strong>手工特征</strong>的检测，需要丰富的任务级的先验知识。</p>
<p>活体检测：关注眨眼、面部和头部动作、视线追踪以及远程生理信号（这种方法需要长期的互动，容易被video attacks伪造）。</p>
<p>经典的handcrafted descriptors：从多种色彩空间中提取有效的欺诈模式，这种PA方法可以通过训练分类器捕捉，但在遇到未见过的场景或未知的PAs时就失效了。</p>
</li>
<li></li>
<li></li>
</ol>
</li>
<li></li>
</ul>
]]></content>
      <tags>
        <tag>组内文章</tag>
        <tag>人脸识别</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Social Distancing Alert with Smartwatches</title>
    <url>/2022/09/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ASocial-Distancing-Alert-with-Smartwatches/</url>
    <content><![CDATA[<ul>
<li><p>发表时间：2022</p>
</li>
<li><p>研究内容：基于智能手表的社交距离警报系统SoDA，SoDA使用加速器和陀螺仪的数据和简单有效的视觉Transformer模型，识别违反社交距离的活动。</p>
</li>
<li><p>code：<a href="https://github.com/aiotgroup/SoDA">https://github.com/aiotgroup/SoDA</a></p>
</li>
<li><p>创新点：</p>
<ol>
<li>应用价值</li>
<li>创建了一个数据集</li>
<li>证明了ViT是一种有效的方法？</li>
</ol>
</li>
<li><p>模型结构：</p>
<img src="/2022/09/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ASocial-Distancing-Alert-with-Smartwatches/image-20220920104544756.png" alt="image-20220920104544756" style="zoom:80%;"></li>
</ul>
]]></content>
      <tags>
        <tag>Transformer</tag>
        <tag>组内文章</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络基础知识</title>
    <url>/2022/09/27/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<h2 id="3D-CNN"><a href="#3D-CNN" class="headerlink" title="3D CNN"></a>3D CNN</h2><p>参考博客：</p>
<p>​		<a href="https://blog.csdn.net/auto1993/article/details/70948249">https://blog.csdn.net/auto1993/article/details/70948249</a></p>
<p>​		<a href="https://blog.csdn.net/YOULANSHENGMENG/article/details/121328554">https://blog.csdn.net/YOULANSHENGMENG/article/details/121328554</a></p>
<p>使用3D CNN可以捕获视频中的<strong>时间</strong>和<strong>空间</strong>的特征信息。</p>
<h3 id="Conv3D"><a href="#Conv3D" class="headerlink" title="Conv3D"></a>Conv3D</h3><p>3D卷积：对于下面的采用3D卷积核进行的卷积操作，通过堆叠多个连续的帧组成一个立方体，在立方体中运用3D卷积核，卷积层中每一个特征map都与上一层中多个邻近的连续帧相连，以此捕捉运动信息。下图卷积操作的时间维度为3（对连续的三帧图像进行卷积操作）。</p>
<img src="https://img-blog.csdn.net/20170429133650515?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvQVVUTzE5OTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img" style="zoom:80%; margin:auto;">

<p>tip：<strong>3D卷积核只能从cube中提取一种类型的特征</strong>（在整个cube中卷积核的权值是共享的），若要提取多种特征，可以采用多种卷积核。</p>
<p>在pytorch中，同样有Conv3D的实现，使用样例如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample intput | 随机输入</span></span><br><span class="line"><span class="comment"># (batch_size, channel, fram_size, height, width)</span></span><br><span class="line">net_input = torch.randn(<span class="number">32</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 所有维度同一个参数配置</span></span><br><span class="line">conv = nn.Conv3d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">net_output = conv(net_input)</span><br><span class="line"><span class="built_in">print</span>(net_output.shape)  <span class="comment"># shape=[32, 64, 5, 112, 112] | 相当于每一个维度上的卷积核大小都是3，步长都是2，pad都是1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 每一维度不同参数配置</span></span><br><span class="line">conv = nn.Conv3d(<span class="number">3</span>, <span class="number">64</span>, (<span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">net_output = conv(net_input)</span><br><span class="line"><span class="built_in">print</span>(net_output.shape) <span class="comment"># shape=[32, 64, 9, 112, 112]</span></span><br></pre></td></tr></table></figure>

<h2 id="差分卷积（Difference-Convolution）"><a href="#差分卷积（Difference-Convolution）" class="headerlink" title="差分卷积（Difference Convolution）"></a>差分卷积（Difference Convolution）</h2><p>参考博客：<a href="https://zhuanlan.zhihu.com/p/392986663">https://zhuanlan.zhihu.com/p/392986663</a></p>
<pre class="mermaid">graph LR
A[LBP]-->B[CDC]
B-->c[3D-CDC]</pre>

<p>空间差分特征具有如下两个优点：</p>
<ol>
<li>对于光照变化具有鲁棒性；</li>
<li>描述了细粒度的纹理信息。</li>
</ol>
<h3 id="LBP（Local-Binary-Patterns）"><a href="#LBP（Local-Binary-Patterns）" class="headerlink" title="LBP（Local Binary Patterns）"></a>LBP（Local Binary Patterns）</h3><p>LBP即局部二值模式，是一种经典的传统手工特征提取方法。</p>
<p>在3x3邻域，将周围像素点的灰度值与中心像素值进行比较，大于中心像素记为1，小于中心像素记为0。这样将产生8个二进制数，然后转换为十进制的LBP码，用LBP码反映该区域的纹理信息。</p>
<img src="https://pic3.zhimg.com/80/v2-8ba548951eb9a627c45e67a654641576_720w.webp" alt="img" style="zoom:100%; margin:auto;">

<p>对于图像中的每个像素点，都对应一个LBP码，LBP码聚合了邻域内的差分信息，对光照变化较为鲁棒，同时描述了细粒度的纹理信息，早期在人脸识别中广泛使用。</p>
<h3 id="中心差分卷积（Central-Difference-Convolution）"><a href="#中心差分卷积（Central-Difference-Convolution）" class="headerlink" title="中心差分卷积（Central Difference Convolution）"></a>中心差分卷积（Central Difference Convolution）</h3><p>Q：为什么vanilla卷积不好使？</p>
<p>A：vanilla卷积是直接聚合局部的亮度级的信息，容易受到光照等因素影响，同时难以表征细粒度的特征。应用到活体检测任务中，受光照等因素影响会导致模型泛化能力变弱，难以表征细粒度的特征则会导致模型难以学到防伪本质的细节信息。根据上述描述，使用空间差分特征可以缓解vanilla卷积存在的问题。</p>
<p>CDC的工作原理如下图所示：</p>
<img src="https://pic4.zhimg.com/80/v2-c088862cb3f9790751f558ecac7b49c3_720w.webp" alt="img" style="zoom:100%; margin:auto;">

<p>其数值形式的卷积过程如下所示，用区域内的像素与中心像素作差值，然后再进行卷积操作。</p>
<img src="https://upload-images.jianshu.io/upload_images/19293651-b1bbd94eaca4a61b.png?imageMogr2/auto-orient/strip|imageView2/2/w/496/format/webp" alt="img" style="zoom:100%;">

<img src="/2022/09/27/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/image-20221015151341198.png" alt="image-20221015151341198" style="zoom:80%; margin:auto;">

<p>θ控制差分卷积的贡献，即gradient-level的信息。</p>
<p>其实现代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># https://github.com/ZitongYu/CDCN/blob/master/CVPR2020_paper_codes/models/CDCNs.py</span></span><br><span class="line"><span class="comment"># vanilla convolution</span></span><br><span class="line">out_normal = self.conv(x)</span><br><span class="line"><span class="comment"># central difference term</span></span><br><span class="line">[C_out,C_in, kernel_size,kernel_size] = self.conv.weight.shape</span><br><span class="line">kernel_diff = self.conv.weight.<span class="built_in">sum</span>(<span class="number">2</span>).<span class="built_in">sum</span>(<span class="number">2</span>)</span><br><span class="line">kernel_diff = kernel_diff[:, :, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">out_diff = F.conv2d(<span class="built_in">input</span>=x, weight=kernel_diff, bias=self.conv.bias, stride=self.conv.stride, padding=<span class="number">0</span>,                       groups=self.conv.groups)</span><br><span class="line"><span class="comment"># CDC OUTPUT</span></span><br><span class="line"><span class="keyword">return</span> out_normal - self.theta * out_diff</span><br></pre></td></tr></table></figure>

<h3 id="时空差分卷积（3D-CDC）"><a href="#时空差分卷积（3D-CDC）" class="headerlink" title="时空差分卷积（3D-CDC）"></a>时空差分卷积（3D-CDC）</h3><p>vanilla 3D的卷积操作难以感知细粒度的时空差异信息。Zitong Yu（这是什么怪物啊）设计了三种3D-CDC，用于不同场景下增强时域特征。</p>
<img src="https://pic4.zhimg.com/80/v2-6b6cbc16d5dd7fb0bc044b238e3a948f_720w.webp" alt="img" style="zoom:120%;">

<ul>
<li>3D-CDC-ST：聚合局部时空区域内的所有中心差分信息，<strong>擅长动态纹理表征</strong>。</li>
<li>3D-CDC-T：聚合相邻帧间的局部时空区域内的中心差分信息，<strong>擅长捕捉精细的时域上下文信息</strong>。在PhysFormer里使用的就是这种差分卷积。</li>
<li>3D-CDC-TR：计算差分前采用temporal average pooling融合上下文信息，<strong>抗时域间噪声扰动</strong>。</li>
</ul>
<p>三种差分卷积的公式如下所示：</p>
<img src="/2022/09/27/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/image-20221015172029434.png" alt="image-20221015172029434" style="zoom: 80%; margin:auto;">

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># https://github.com/ZitongYu/3DCDC-NAS/blob/master/3DCDC.py</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># vanilla</span></span><br><span class="line">self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,</span><br><span class="line">                      dilation=dilation, groups=groups, bias=bias)</span><br><span class="line">out_normal = self.conv(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># CDC_ST</span></span><br><span class="line">kernel_diff = self.conv.weight.<span class="built_in">sum</span>(<span class="number">2</span>).<span class="built_in">sum</span>(<span class="number">2</span>).<span class="built_in">sum</span>(<span class="number">2</span>)</span><br><span class="line">kernel_diff = kernel_diff[:, :, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">out_diff = F.conv3d(<span class="built_in">input</span>=x, weight=kernel_diff, bias=self.conv.bias, stride=self.conv.stride,</span><br><span class="line">                    padding=<span class="number">0</span>, dilation=self.conv.dilation, groups=self.conv.groups)</span><br><span class="line"><span class="keyword">return</span> out_normal - self.theta * out_diff</span><br><span class="line"></span><br><span class="line"><span class="comment"># CDC_T</span></span><br><span class="line">kernel_diff = self.conv.weight[:, :, <span class="number">0</span>, :, :].<span class="built_in">sum</span>(<span class="number">2</span>).<span class="built_in">sum</span>(<span class="number">2</span>) + self.conv.weight[:, :, <span class="number">2</span>, :, :].<span class="built_in">sum</span>(<span class="number">2</span>).<span class="built_in">sum</span>(<span class="number">2</span>)</span><br><span class="line">kernel_diff = kernel_diff[:, :, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">out_diff = F.conv3d(<span class="built_in">input</span>=x, weight=kernel_diff, bias=self.conv.bias, stride=self.conv.stride,</span><br><span class="line">                    padding=<span class="number">0</span>, dilation=self.conv.dilation, groups=self.conv.groups)</span><br><span class="line"><span class="keyword">return</span> out_normal - self.theta * out_diff</span><br><span class="line"></span><br><span class="line"><span class="comment"># CDC_TR</span></span><br><span class="line">local_avg = self.avgpool(x)</span><br><span class="line">kernel_diff = self.conv.weight[:, :, <span class="number">0</span>, :, :].<span class="built_in">sum</span>(<span class="number">2</span>).<span class="built_in">sum</span>(<span class="number">2</span>) + self.conv.weight[:, :, <span class="number">2</span>, :, :].<span class="built_in">sum</span>(<span class="number">2</span>).<span class="built_in">sum</span>(<span class="number">2</span>)</span><br><span class="line">kernel_diff = kernel_diff[:, :, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">out_diff = F.conv3d(<span class="built_in">input</span>=local_avg, weight=kernel_diff, bias=self.conv.bias, stride=self.conv.stride,</span><br><span class="line">                    padding=<span class="number">0</span>, groups=self.conv.groups)</span><br><span class="line"><span class="keyword">return</span> out_normal - self.theta * out_diff</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>神经网络基础知识</tag>
      </tags>
  </entry>
  <entry>
    <title>经典模型：Video Swin Transformer</title>
    <url>/2022/09/28/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AVideo-Swin-Transformer/</url>
    <content><![CDATA[<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>Video Swin Transformer的结构整体如下图所示，与Swin-T相比多了时间维度。</p>
<img src="/2022/09/28/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AVideo-Swin-Transformer/image-20220928191836623.png" alt="image-20220928191836623" style="zoom:80%; margin:auto;">

<h3 id="3D-Patch-Partition"><a href="#3D-Patch-Partition" class="headerlink" title="3D Patch Partition"></a>3D Patch Partition</h3><p>在初始时，设置每个token的大小为2x4x4x3，因此，每个视频被划分为$\frac{T}{2}\times \frac{H}{4} \times \frac{W}{4}$个tokens，每个token的维度为96。</p>
<h3 id="Linear-Embedding"><a href="#Linear-Embedding" class="headerlink" title="Linear Embedding"></a>Linear Embedding</h3><p>将每一个token投影到指定的维度C。</p>
<h3 id="Video-Swin-Transformer-Block"><a href="#Video-Swin-Transformer-Block" class="headerlink" title="Video Swin Transformer Block"></a>Video Swin Transformer Block</h3><p>对于两个相邻的Video Swin Transformer Block，仍采用和Swin Transformer相同的处理方法，每个3D W-MSA后接一个3D SW-MSA。</p>
<img src="/2022/09/28/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AVideo-Swin-Transformer/image-20220928200155711.png" alt="image-20220928200155711" style="zoom:67%; margin:auto;">

<h4 id="3D-W-MSA"><a href="#3D-W-MSA" class="headerlink" title="3D W-MSA"></a>3D W-MSA</h4><p>3D W-MSA与W-MSA相似，对于大小为8x8x8的输入，使用4x4x4的窗口大小，在stage1中窗口的数目为2x2x2，在每个窗口内部进行自注意力计算。</p>
<img src="/2022/09/28/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AVideo-Swin-Transformer/image-20220928210505074.png" alt="image-20220928210505074" style="zoom:67%; margin:auto;">

<h4 id="3D-SW-MSA"><a href="#3D-SW-MSA" class="headerlink" title="3D SW-MSA"></a>3D SW-MSA</h4><p>3D SW-MSA与SW-MSA相同，使用移动窗口补充计算不同窗口中token间的自注意力，为了降低移动窗口后增加的窗口数，同样使用拼接与mask技术，保持窗口数的恒定。</p>
<h3 id="Patch-Merging"><a href="#Patch-Merging" class="headerlink" title="Patch Merging"></a>Patch Merging</h3><p>在进行patch合并时，不从时间维度进行下采样，而是从空间维度对2x2的patch进行合并，合并之后使用一个线性层投影将其维度减半。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><h3 id="不同的时空注意力设计"><a href="#不同的时空注意力设计" class="headerlink" title="不同的时空注意力设计"></a>不同的时空注意力设计</h3><ul>
<li>joint：在每一个3D窗口中联合计算时空注意力。</li>
<li>split：在空间swin transformer的基础上添加了两个时间transformer层</li>
<li>factorized：先是一个空间MSA层，再接一个时间MSA层。</li>
</ul>
<p>实验证明，在视频分类任务中，综合考虑速度与精度，joint模式达到了最佳，作者认为空间域的局部性减少了joint的计算量，同时保持了有效性。</p>
<h3 id="3D-token的时间维度与窗口的时间维度"><a href="#3D-token的时间维度与窗口的时间维度" class="headerlink" title="3D token的时间维度与窗口的时间维度"></a>3D token的时间维度与窗口的时间维度</h3><p>总的来说，3D token的时间维度与窗口的时间维度越大，精度越高，相应的计算成本也越高。</p>
]]></content>
      <tags>
        <tag>经典模型</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Deep Learning Methods for Remote Heart Rate Measurement: A Review and Future Research Agenda</title>
    <url>/2022/09/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeep-Learning-Methods-for-Remote-Heart-Rate-Measurement-A-Review-and-Future-Research-Agenda/</url>
    <content><![CDATA[<p>参考博客：<a href="https://blog.csdn.net/m0_46792836/article/details/121222265">https://blog.csdn.net/m0_46792836/article/details/121222265</a></p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><strong>rPPG</strong>，即remote photoplethysmography，远程光电容积脉搏波描记法，通过摄像头捕捉皮肤细微的亮度变化监测心率。</p>
<p>PPG是一种最常用的测量心率的方法，使用一个光源和光电探测器测量皮下血管体积的变化，当光源照在组织上时，光探测器可以捕捉到血液流动反射或透射光强度的微小变化，产生所谓的PPG信号，光的吸收遵循Beer–Lambert定律，该定律指出，血液吸收的光与光进入皮肤的渗透力和血液中血红蛋白的浓度成正比。在心动周期中，血红蛋白浓度的微小变化引起血管吸收的光量的32个波动，导致33个皮肤强度值的变化。</p>
<p>但对于穿戴式或接触式的监测设备，不适用于检测新生儿或皮肤脆弱的患者，长期检测可能会导致患者不舒服或皮肤感染。</p>
<p>在rPPG中，使用相机作为光探测器捕捉皮肤微小的颜色变化，自然光作为光源。其DRM（色反射）模型如下图所示，可以看到相机同时捕捉到皮肤表面产生的镜面反射和身体产生的漫反射，其中镜面反射并不包含有意义的生理信息，因此需要对捕捉到的信号进行精细处理。</p>
<p><img src="/2022/09/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeep-Learning-Methods-for-Remote-Heart-Rate-Measurement-A-Review-and-Future-Research-Agenda/image-20220929213602840.png" alt="image-20220929213602840" style="zoom:67%; margin:auto;"></p>
<p>传统的远程心率测量方法如下图所示，首先对视频进行人脸检测，接着从人脸上选择感兴趣的区域（ROI），以此获得包含强信号的区域，此后从ROI内的像素提取rPPG信号，最后对rPPG信号进一步处理（如频率分析和峰值检测等）获取心率信息。</p>
<p><img src="/2022/09/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeep-Learning-Methods-for-Remote-Heart-Rate-Measurement-A-Review-and-Future-Research-Agenda/image-20220930082244032.png" alt="image-20220930082244032" style="zoom: 67%; margin:auto;"></p>
<p>基于深度学习的远程心率检测方法可以分为<strong>端到端</strong>和<strong>混合深度学习</strong>的方法。</p>
<h2 id="端到端的深度学习方法"><a href="#端到端的深度学习方法" class="headerlink" title="端到端的深度学习方法"></a>端到端的深度学习方法</h2><p>所谓的端到端的方法，即模型直接将视频作为输入，产生心率或rPPG信号输出。这种方法需要大量的训练数据，同时训练结果难以验证，我们需要做更多的工作对模型进行解释。</p>
<p>端到端的心率检测方法又可以分为如下两种：</p>
<p><img src="/2022/09/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeep-Learning-Methods-for-Remote-Heart-Rate-Measurement-A-Review-and-Future-Research-Agenda/image-20220930085223073.png" alt="image-20220930085223073" style="zoom: 67%; margin: auto;"></p>
<h3 id="2D-CNN"><a href="#2D-CNN" class="headerlink" title="2D CNN"></a>2D CNN</h3><p>2D CNN只考虑了视频帧的空间信息。</p>
<ul>
<li>HR-CNN（2018）：包含提取器和HR估计器的两步CNN，提取器从视频帧序列中提取rPPG信号，使其信噪比最大化。HR-CNN解决了<strong>视频压缩伪影</strong>的问题。</li>
<li>DeepPhys（2018）：同时训练一个运动模型和外观模型，运动模型将相邻视频帧间的归一化差作为模型的输入表示，对帧中的运动和颜色变化进行建模；外观模型通过注意力机制引导运动模型学习运动表征。该模型可以更好地<strong>捕捉不同光照条件下的生理信号</strong>，对光照变化和被试运动更有鲁棒性。</li>
<li>MTTS-CAN（2020）：DeepPhys的改进，引入时间移位模块（TSM）捕获时间信息，TSM允许相邻帧之间的信息交换，避免昂贵的3D卷积操作。</li>
</ul>
<h3 id="时空网络——3D-CNN"><a href="#时空网络——3D-CNN" class="headerlink" title="时空网络——3D CNN"></a>时空网络——3D CNN</h3><p>3D CNN可以利用视频中包含的时间信息，时空网络（STNs）有效地表示视频流中生理信号的时空信息。</p>
<ul>
<li>3D CNN PhysNet：旨在定位每个个体心跳的峰值，以准确估计被试的HR和HRV。</li>
<li>两阶段STN：包括一个时空视频增强网络（3D STVEN）和一个时空3D CNN（rPPGNet），压缩的面部视频通过3D STVEN以提高视频质量，同时保留尽可能多的信息；增强后的视频输入rPPGNet以提取rPPG信号，rPPGNet使用注意力机制从皮肤区域获取主导的rPPG特征。</li>
<li>AutoHR：使用神经结构搜索（NAS）自动找到最适合的主干3D CNN，使用一种三维卷积操作时域差分卷积（TDC）帮助跟踪感兴趣区域。</li>
</ul>
<h3 id="时空网络——2D-CNN-RNN"><a href="#时空网络——2D-CNN-RNN" class="headerlink" title="时空网络——2D CNN+RNN"></a>时空网络——2D CNN+RNN</h3><p>使用2D CNN提取空间信息，用RNN提取时间前后信息并结合。</p>
<ul>
<li>基于RNN的PhysNet：首先将输入信息输入到2D CNN中提取RGB视频帧的空间特征，然后利用RNN在时域内传播这些空间特征。但研究证明，基于3D CNN的PhysNet比基于RNN的PhysNet获得了更好的性能。</li>
</ul>
<p><img src="/2022/09/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeep-Learning-Methods-for-Remote-Heart-Rate-Measurement-A-Review-and-Future-Research-Agenda/image-20220930105928378.png" alt="image-20220930105928378" style="zoom: 67%; margin:auto;"></p>
<h2 id="混合深度学习方法"><a href="#混合深度学习方法" class="headerlink" title="混合深度学习方法"></a>混合深度学习方法</h2><p>所谓的混合深度学习方法，是指深度学习技术只应用在检测过程中的某些部分，如信号优化、信号提取或心率估计。</p>
<h3 id="用于信号优化的深度学习方法"><a href="#用于信号优化的深度学习方法" class="headerlink" title="用于信号优化的深度学习方法"></a>用于信号优化的深度学习方法</h3><p>所谓的信号优化就是使用人脸检测或皮肤分割，以<strong>忽略不相关的背景信息</strong>。</p>
<ul>
<li>创建一个用于皮肤检测的2D CNN，用于分割出皮肤所在区域，对检测到的皮肤区域进行常规rPPG算法。但这种方法利用了人脸的所有皮肤区域提取rPPG信号，可能包含不必要的噪声。</li>
<li>Deep-HR：采用接受域块（RFB）网络对感兴趣区域进行目标检测，该方法设计了GAN增强检测到的ROI，对检测到的ROI进行再生，将这个高质量的ROI（这个过程也可以看作后面的信号提取的过程）用于后续的信号提取。</li>
</ul>
<h3 id="用于信号提取的深度学习"><a href="#用于信号提取的深度学习" class="headerlink" title="用于信号提取的深度学习"></a>用于信号提取的深度学习</h3><p>信号提取的目标是从视频中<strong>提取高质量的rPPG信号</strong>进行HR估计（感觉是指去噪这个过程）。</p>
<ul>
<li><p>LSTM：使用LSTM网络对噪声污染的rPPG信号进行滤波，得到无噪声的rPPG信号。由于数据不足的问题，在训练时可以首先在合成数据上进行训练，然后在真实数据集上微调。</p>
</li>
<li><p>2D CNN MetaPhys：使用预训练的2D CNN TS-CAN用于信号提取，并提出元学习方法，利用模型不可知元学习作为个性化参数更新模式，可以在只有少量训练样本的情况下快速适应。作者认为该方法可以减少由于肤色造成的偏差。</p>
</li>
<li><p>3D CNN Siamese-rPPG：基于Siamese 3D CNN框架，作者认为不同的面部区域应反映相同的rPPG特征，建立额头分支和脸颊分支进行特征提取，将两个分支的输出通过运算融合，得到最终的预测的rPPG信号。</p>
<p><img src="/2022/09/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeep-Learning-Methods-for-Remote-Heart-Rate-Measurement-A-Review-and-Future-Research-Agenda/image-20220930142543663.png" alt="image-20220930142543663" style="zoom: 67%; margin: auto;"></p>
</li>
<li><p>3D CNN HeartTrack：利用带有注意力机制的3D CNN进行信号提取，在三维时空注意网络中，利用硬注意力机制忽略不相关的背景信息，利用软注意力机制过滤所覆盖的区域。再将提取到的信号送入1D CNN进行时间序列分析。</p>
</li>
</ul>
<h3 id="用于心率估计的深度学习"><a href="#用于心率估计的深度学习" class="headerlink" title="用于心率估计的深度学习"></a>用于心率估计的深度学习</h3><p>对于提取到的rPPG信号，传统的方法是使用带通滤波器滤波，然后进行频率分析或峰值检测来估计心率。对于深度学习方法，<strong>将心率估计看作回归问题求解</strong>。</p>
<p>HR信号有两种表示：</p>
<ol>
<li>频谱图像：对提取的rPPG信号进行短时傅里叶变换和带通滤波，得到频域表示，将频域表示和时域信号结合，形成频谱图像。</li>
<li>时空图：将ROI像素RGB通道的颜色信息串接在时间序列中，成行排列，形成时空地图。这种信号表示方法可以抑制与HR信号无关的信息。</li>
</ol>
<h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><ul>
<li>流行病控制</li>
<li>防伪：通过捕捉异常生物信号检测深度造假的视频</li>
<li>远程医疗</li>
<li>增强生物识别技术的安全性</li>
<li>驾驶状态检测</li>
<li>？从自然灾害中寻找幸存者</li>
<li>新生儿检测</li>
<li>健康跟踪</li>
</ul>
<h2 id="研究缺口"><a href="#研究缺口" class="headerlink" title="研究缺口"></a>研究缺口</h2><ul>
<li>影响因素：基于rPPG的远程HR测量收到光照变化、运动伪影、肤色变化和视频压缩等诸多因素的影响。<strong>新方法应提供如何从技术和生物物理角度处理这些挑战的见解</strong></li>
<li>测量其他生命体征</li>
<li>数据集：目前数据集抓鱼用于解决运动伪影、照明变化，对于肤色变化、多人检测、远距离、新生儿检测也需要克服。</li>
<li>在不同HR范围上的表现</li>
<li>对基于深度学习方法上的理解</li>
</ul>
]]></content>
      <tags>
        <tag>rPPG</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：MAE</title>
    <url>/2022/10/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AMAE/</url>
    <content><![CDATA[<p>参考：<a href="https://www.bilibili.com/video/BV1sq4y1q77t/">https://www.bilibili.com/video/BV1sq4y1q77t/</a></p>
<p>代码：<a href="https://github.com/facebookresearch/mae">https://github.com/facebookresearch/mae</a></p>
<h2 id="模型概述"><a href="#模型概述" class="headerlink" title="模型概述"></a>模型概述</h2><p>MAE即Masked Autoencoders（其auto是指“自“的意思，而不是自动，指label也来自图片本身），可以看作CV版的Bert，基于ViT方法。</p>
<p>MAE的整体思路是将图片中的一些patches进行mask，然后对masked掉的patches进行预测。如下图所示：</p>
<img src="/2022/10/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AMAE/image-20221003114506533.png" alt="image-20221003114506533" style="zoom:80%; margin:auto;">

<h2 id="NLP与CV"><a href="#NLP与CV" class="headerlink" title="NLP与CV"></a>NLP与CV</h2><p>作者认为造成NLP与CV任务中masked autoencoding的不同，主要来自以下几个方面：</p>
<ol>
<li>文本与图像的信息密度不同，文本是人为生成的，具有<strong>高语义和信息密度</strong>，而图像则具有大量的冗余信息</li>
<li>对于自编码器的解码器，CV任务需要还原的是语义层次比较低的像素信息（解码器更加复杂），而NLP任务需要还原的是语义层次比较高的文本信息（解码器简单）</li>
</ol>
<h2 id="MAE具体实现"><a href="#MAE具体实现" class="headerlink" title="MAE具体实现"></a>MAE具体实现</h2><ul>
<li><p>masking：将一张图片分成若干个patches，然后随机选取部分patches进行mask，仅保留少量的patches，以减少冗余信息。</p>
</li>
<li><p>MAE Encoder：采用完全和ViT相同的方法，对于被masked掉的patches，直接不输入Encoder。</p>
</li>
<li><p>MAE Decoder：解码器同样使用一个Transformer，对于所有patches，通过一个共享的、可以学到的向量来表示，对于每一个块要加上位置编码信息，解码器只有在预训练的时候需要使用。</p>
</li>
<li><p>Reconstruction target：对于解码器的输出通过一个线形层投影到对应的输出shape，使用MSE对被masked的patches计算loss。</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Benchmarking Joint Face Spoofing and Forgery Detection with Visual and Physiological Cues</title>
    <url>/2022/10/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ABenchmarking-Joint-Face-Spoofing-and-Forgery-Detection-with-Visual-and-Physiological-Cues/</url>
    <content><![CDATA[<ul>
<li>发表时间：2022</li>
<li>研究内容：基于视觉和生理线索的联合人脸欺骗和伪造检测标杆</li>
<li>FAS：Face Anti-Spoofing，活体检测</li>
<li>Face Forgery Detection：人脸伪造检测（Deepfake检测）</li>
</ul>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>目前大多数的<strong>活体检测</strong>和<strong>人脸伪造检测</strong>方法关注以下两个方面：</p>
<ol>
<li>单模态的视觉外观或生理信号（rPPG信号）</li>
<li>分离FAS或人脸伪造检测的特征表示</li>
</ol>
<p>但这两种方面存在如下问题：</p>
<ol>
<li>单模态的外观和rPPG特征对于高保真的3D面罩或视频回放袭击来说相对脆弱。</li>
<li>对于FAS和人脸欺诈识别任务，有丰富的常用特征，可用于以多任务学习的方式设计一个FAS和人脸欺诈识别联合系统。</li>
</ol>
<p>本文<strong>贡献</strong>：</p>
<ol>
<li>建立了第一个活体检测和伪造识别的benchmark，该benchmark同时使用视觉外观和生理rPPG信号。</li>
<li>设计了一个双分支的生理网络，同时使用面部时空的rPPG信号和其对应的连续小波变换作为输入。加强了rPPG周期性的区别。</li>
<li>在进行多模态融合前，对视觉外观和rPPG特征实施a weighted batch and layer normalization，以消除模态偏差，提高融合效果。</li>
<li>研究发现，无论单模态还是多模态模型，通过在活体检测和人脸伪造检测两个任务上进行联合训练可以显著提高模型泛化能力。</li>
</ol>
<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>基于rPPG或外观+rPPG的活体检测与人脸伪造识别技术还不成熟。主要包括如下两个原因：</p>
<ol>
<li>由于外部干扰，时域rPPG信号的周期性识别受到限制（无法区分是真实的还是伪造的）。</li>
<li>由于模态偏差，外观与rPPG信号的直接融合会导致负效果。</li>
</ol>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p><strong>Physiological rPPG based FAS：</strong>使用rPPG信号进行活体检测。主要包括如下几种做法：</p>
<ul>
<li>考虑真实的面部和打印出的面部的心跳区别；</li>
<li>通过比较面部的rPPG信号和背景噪音判断是否为活体；</li>
<li>使用transformer架构提取全局的周期性的信息用于活体检测；</li>
<li>通过评分级融合结合外观和rPPG预测实现活体检测</li>
</ul>
<p><strong>Physiological rPPG based face forgery detection：</strong>使用rPPG信号进行伪造人脸识别，主要包括如下几种做法：</p>
<ul>
<li>与活体检测类似，通过判断心率是否包含固定周期性模式；</li>
<li>从时间域和功率谱域提取rPPG信号的特征，描述时间一致性和空间相干性；</li>
</ul>
<h2 id="实现方法"><a href="#实现方法" class="headerlink" title="实现方法"></a>实现方法</h2><p><strong>Joint Face Spoofing and Forgery Detection：</strong>联合训练两种任务</p>
<ul>
<li><p>联合训练架构：在多任务训练中有如下三种设置方式：</p>
<img src="/2022/10/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ABenchmarking-Joint-Face-Spoofing-and-Forgery-Detection-with-Visual-and-Physiological-Cues/image-20221005201731187.png" alt="image-20221005201731187" style="zoom:67%; margin:auto;">
</li>
<li><p>联合训练采样策略</p>
<ul>
<li>随机采样：每个batch中的数据从两任务的混合数据中随机采样（比例不定）</li>
<li>同步采样：每个batch中的数据来自两任务的数据各占一半</li>
<li>交替采样：每个SGD step轮流学习每个任务（即一个batch学习FAS，一个batch学习伪造人脸识别）</li>
<li>按任务采样：先学习一个任务（全部数据），再学习另一个任务。</li>
</ul>
</li>
</ul>
<p><strong>Two-branch Physiological Network：</strong>同时使用面部时空的rPPG信号和其对应的连续小波变换作为输入。</p>
<p>文章使用的两分支的生理网络如下图所示，使用两个结构相同但参数不同的编码器获得时域、时频域特征，将两种特征拼接形成通用的rPPG特征，然后接一个二分类头进行预测，同时为了训练的稳定性，对于两个分支添加额外的二分类头，模型的Loss由上述三个头的loss组成。</p>
<img src="/2022/10/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ABenchmarking-Joint-Face-Spoofing-and-Forgery-Detection-with-Visual-and-Physiological-Cues/image-20221005204141328.png" alt="image-20221005204141328" style="zoom:67%; margin:auto;">

<ul>
<li>MSTmap（Multi-scale Spatial-temporal map）：多尺度时空图，考虑局部和全局的生理信号。</li>
<li>WaveletMap：基于连续小波变换的时频图</li>
</ul>
<p><strong>Weighted Normalization for Appearance and rPPG Fusion：</strong>多模态融合</p>
<p>对于每个模态使用batch normalization和layer normalization，然后再将两个模态进行连接。如下所示：</p>
<img src="/2022/10/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ABenchmarking-Joint-Face-Spoofing-and-Forgery-Detection-with-Visual-and-Physiological-Cues/image-20221005212654204.png" alt="image-20221005212654204" style="zoom: 50%; margin:auto;">



<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>未来研究方向：</p>
<ol>
<li>探索多任务学习和多模态融合的策略</li>
<li>除了基于rPPG的颜色变化，利用面部运动的语义线索和上下文动态进行活体检测和伪造人脸识别。</li>
</ol>
]]></content>
      <tags>
        <tag>rPPG</tag>
        <tag>FAS</tag>
      </tags>
  </entry>
  <entry>
    <title>论文技巧</title>
    <url>/2022/10/06/%E8%AE%BA%E6%96%87%E6%8A%80%E5%B7%A7/</url>
    <content><![CDATA[<h2 id="如何找研究想法"><a href="#如何找研究想法" class="headerlink" title="如何找研究想法"></a>如何找研究想法</h2><ul>
<li>参考：<a href="https://www.bilibili.com/video/BV1qq4y1z7F2/">https://www.bilibili.com/video/BV1qq4y1z7F2/</a></li>
</ul>
<ol>
<li>打补丁法：针对新发表的论文中存在的问题打补丁（注意：若一篇文章已经是打补丁的文章，最好不要继续在其上继续打补丁。）</li>
</ol>
<h2 id="如何判断研究工作的价值"><a href="#如何判断研究工作的价值" class="headerlink" title="如何判断研究工作的价值"></a>如何判断研究工作的价值</h2><ul>
<li>参考：<a href="https://www.bilibili.com/video/BV1oL411c7Us/">https://www.bilibili.com/video/BV1oL411c7Us/</a></li>
</ul>
<p>好的研究工作应该：用有<strong>新意</strong>的方法，<strong>有效</strong>的解决一个<strong>研究</strong>问题</p>
<ul>
<li>研究问题：与之相对的是工程问题</li>
</ul>
<p>量化地来看，研究价值&#x3D;新意度x有效性x问题大小</p>
<h2 id="论文写作"><a href="#论文写作" class="headerlink" title="论文写作"></a>论文写作</h2><ul>
<li>参考：<a href="https://www.bilibili.com/video/BV1hY411T7vy/">https://www.bilibili.com/video/BV1hY411T7vy/</a></li>
</ul>
<h3 id="Research-Researchers-and-Readers"><a href="#Research-Researchers-and-Readers" class="headerlink" title="Research, Researchers, and Readers"></a>Research, Researchers, and Readers</h3><ul>
<li>论文可以看作想象中的对话，根据读者的知识储备决定写作内容与风格</li>
</ul>
<h3 id="Asking-Questions-Finding-Answers"><a href="#Asking-Questions-Finding-Answers" class="headerlink" title="Asking Questions, Finding Answers"></a>Asking Questions, Finding Answers</h3><ul>
<li>so what</li>
</ul>
]]></content>
      <tags>
        <tag>论文技巧</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Remote Photoplethysmograph Signal Measurement from Facial Videos Using Spatio-Temporal Networks</title>
    <url>/2022/10/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARemote-Photoplethysmograph-Signal-Measurement-from-Facial-Videos-Using-Spatio-Temporal-Networks/</url>
    <content><![CDATA[<ul>
<li>发表时间：2019</li>
<li>code：<a href="https://github.com/ZitongYu/PhysNet">https://github.com/ZitongYu/PhysNet</a></li>
<li>HRV：Heart Rate Variability，心率变异性，从更精细的角度描述心脏活动</li>
<li>AF：atrial fibrillatio，心房震颤</li>
<li>时空网络：主流的时空网络有两种，第一种是基于3D卷积的时空网络，第二种是基于RNN的神经网络（如LSTM等）。</li>
</ul>
<h2 id="简介">简介</h2>
<p>文章提出了一个深度时空网络PhysNet，用于从原始面部视频中重建精确的rPPG信号，包括每个时间位置与其对应的脉冲峰值，可以获得平均HR，以及HRV和IBIs信息，用于AF检测与情绪识别中。</p>
<h3 id="研究现状">研究现状</h3>
<ol>
<li>早期使用两阶段的方法，阶段1检测或跟踪面部以提取rPPG信号，阶段2通过频率分析估计对应的平均心率。但这种方法有两个缺点，1是自定义的面部区域是根据经验知识得到的，可能并非最佳的区域。2是该方法需要手工特征或过滤器，可能泛化能力弱并且丢失与心率相关的重要信息；</li>
<li>对于基于深度学习的心率检测方法，通常有如下几个缺点，1是将心率估计问题看作简单的一阶段回归问题，得到平均心率，而丢失了单个脉冲峰值信息，从而限制了在医学上的应用；2是使用的方法并非端到端的系统，仍然需要预处理或后处理步骤，引入了手工特征；3是使用的方法基于2D空间神经网络，没有考虑对rPPG测量重要的时间特征。</li>
</ol>
<h3 id="模型简介">模型简介</h3>
<p>文章提出的rPPG信号测量方法的框架如下图所示：</p>
<img src="/2022/10/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARemote-Photoplethysmograph-Signal-Measurement-from-Facial-Videos-Using-Spatio-Temporal-Networks/image-20221008100615766.png" alt="image-20221008100615766" style="zoom:80%; margin:auto;">
<p>文章的主要贡献包括如下几点：</p>
<ol>
<li>提出了第一个基于端到端的rPPG信号测量网络PhysNet，并将之前忽略的时间信息考虑进去；</li>
<li>PhysNet在测量平均心率和HRV特征上都达到了很好的性能；</li>
<li>PhysNet具有很好的泛化性</li>
</ol>
<h2 id="PhysNet">PhysNet</h2>
<p>PhysNet的结构如下图所示：</p>
<img src="/2022/10/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARemote-Photoplethysmograph-Signal-Measurement-from-Facial-Videos-Using-Spatio-Temporal-Networks/image-20221008105124878.png" alt="image-20221008105124878" style="zoom:80%; margin:auto;">
<p>主流的时空网络有基于3DCNN和基于RNN两种，因此作者也提出了两种PhyNet。</p>
<h3 id="3DCNN-based-PhysNet">3DCNN based PhysNet</h3>
<ul>
<li>采用一个3x3x3的卷积在空间域和时间域同时提取rPPG特征。</li>
<li>采用基于时间的encoder-decoder结构PhysNet-3DCNN-ED，可以更有效地利用时间信息，减少冗余与噪音。</li>
</ul>
<h3 id="RNN-based-PhysNet">RNN based PhysNet</h3>
<ul>
<li>首先使用2DCNN提取空间特征，然后使用基于RNN的模块实现空间特征在时域的传播</li>
</ul>
<h3 id="损失函数">损失函数</h3>
<p>使用负皮尔森相关系数作为损失函数，以最大化趋势相似度，最小化峰值定位误差。</p>
<img src="/2022/10/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARemote-Photoplethysmograph-Signal-Measurement-from-Facial-Videos-Using-Spatio-Temporal-Networks/image-20221008111446331.png" alt="image-20221008111446331" style="zoom: 50%; margin:auto;">
<h2 id="实验设置">实验设置</h2>
<h3 id="数据集">数据集</h3>
<p>文章使用了两个数据集，使用OBF数据集进行训练与测试，使用MAHNOB-HCI数据集进行模型泛化能力的交叉验证。</p>
<ol>
<li>OBF数据集：采集自100个健康的成人和6个患有心房震颤的病人，每个人有两个时长为5分钟的视频，每个视频包括其对应的ECG、呼吸、ppg信号，每个视频的分辨率为1920x2080，帧率为60fps。</li>
<li>MAHNOB-HCI数据集：采集自27个人的共527个视频，每个视频的分辨率为780x580，帧率为61fps。</li>
</ol>
<h3 id="训练设置">训练设置</h3>
<p>对于每一个视频，进行如下操作：</p>
<ul>
<li>在第一帧中使用Viola-Jones面部检测器切割出面部区域，并在之后的帧中固定这个区域；</li>
<li>将面部图片正则化到128x128</li>
<li>将视频和对应的真实信号进行下采样，视频下采样至30fps，信号下采样至30HZ</li>
<li>使用Adam，lr=1e-4，epoch=15</li>
</ul>
<h3 id="测试设置">测试设置</h3>
<p>对于HR与HRV的评估，使用如下几个性能指标：</p>
<ol>
<li>SD（standard deviation）：标准差</li>
<li>RMSE（root mean square error）：均方根误差</li>
<li>R（Pearson’s correlation coefficien）：皮尔森相关系数</li>
<li>MAE（mean absolute error）：平均绝对误差</li>
</ol>
<h2 id="代码实现（UBFC版）">代码实现（UBFC版）</h2>
<h3 id="数据集UBFC-rPPG">数据集UBFC-rPPG</h3>
<p><strong>UBFC-rPPG</strong>：出自论文《Unsupervised skin tissue segmentation for remote photoplethysmography》</p>
<ul>
<li>
<p>video：43个</p>
</li>
<li>
<p>time：2min</p>
</li>
<li>
<p>帧率：30fps</p>
</li>
<li>
<p>分辨率：640x480</p>
</li>
<li>
<p>ground truth：与视频同步的心率信号，对于ground truth，第一行是gtTrace，第二行是gtHR，第三行是gtTime。</p>
<img src="/C:/Users/zxkj/AppData/Roaming/Typora/typora-user-images/image-20221010183747585.png" alt="image-20221010183747585" style="zoom:80%;">
</li>
</ul>
<h3 id="K-交叉验证（但好像没用到）">K-交叉验证（但好像没用到）</h3>
<p>K-交叉验证是一种<strong>模型评估</strong>方法，指将原始数据均分成k组，轮流将每个子集数据分别作为验证集，其余k-1组子集数据作为训练集，这样将得到k个模型，用k个模型的平均性能作为模型的评价指标。</p>
<h3 id="DataLoader">DataLoader</h3>
<p>PhysNet实现中，训练集和测试集是通过如下代码手动划分的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (test):</span><br><span class="line">    self.vdPath_list = os.listdir(<span class="string">&quot;/data/maoguanhui/UBFC/&quot;</span>)[<span class="number">30</span>:<span class="number">42</span>]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    self.vdPath_list = os.listdir(<span class="string">&quot;/data/maoguanhui/UBFC/&quot;</span>)[:<span class="number">30</span>]</span><br></pre></td></tr></table></figure>
<p>之后，将每个视频划分为7个片段，每个片段包括160帧</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clip = idx % <span class="number">7</span>  <span class="comment"># 第几个剪辑片段</span></span><br><span class="line">idx = <span class="built_in">int</span>(idx / <span class="number">7</span>)  <span class="comment"># 第几个视频</span></span><br><span class="line">start_frame = <span class="number">160</span> * clip</span><br></pre></td></tr></table></figure>
<p>然后从ground truth中获取计算loss需要的信息，首先获取平均HR，用于计算师兄添加的fre_loss</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clhr = <span class="built_in">list</span>(data[<span class="number">1</span>].split())</span><br><span class="line">clhr = [<span class="built_in">str</span>.replace(<span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;E&#x27;</span>) <span class="keyword">for</span> <span class="built_in">str</span> <span class="keyword">in</span> clhr]</span><br><span class="line">sumHR = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">for</span> kj <span class="keyword">in</span> <span class="built_in">range</span>(start_frame, start_frame + <span class="number">160</span>):</span><br><span class="line">    sumHR += <span class="built_in">float</span>(clhr[kj])</span><br><span class="line">clip_average_HR = sumHR / <span class="number">160</span></span><br></pre></td></tr></table></figure>
<p>再获取PPG信号，用于计算皮尔森系数对应的rPPG loss。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Trace = []</span><br><span class="line">data = <span class="built_in">list</span>(data[<span class="number">0</span>].split())</span><br><span class="line">data = [<span class="built_in">str</span>.replace(<span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;E&#x27;</span>) <span class="keyword">for</span> <span class="built_in">str</span> <span class="keyword">in</span> data]</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data)):</span><br><span class="line">    Trace.append(<span class="built_in">float</span>(data[j]))</span><br></pre></td></tr></table></figure>
<h3 id="网络结构">网络结构</h3>
<p>在PhysNet的网络实现中，有多种网络，文章中选择使用PhysNet_padding_ED_peak</p>
]]></content>
      <tags>
        <tag>rPPG</tag>
      </tags>
  </entry>
  <entry>
    <title>视频理解</title>
    <url>/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<h2 id="双流网络">双流网络</h2>
<ul>
<li>论文：《Two-stream convolutional networks for action recognition in videos》</li>
</ul>
<p>提出的具有两个分支的神经网络，将两个分支的输出融合后进行分类。</p>
<img src="/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/image-20221009101114213.png" alt="image-20221009101114213" style="zoom:80%; margin:auto;">
<ul>
<li>空间流神经网络：捕捉apperance信息，将视频帧作为处理对象，就是普通的图像处理网络，可以使用imagenet上的预训练网络。</li>
<li>时间流神经网络：捕捉两帧间光流的运动信息，一般将光流的运动信息分解为水平方向和竖直方向，如下图所示。对于L帧的视频，最终将得到L-1个光流图</li>
</ul>
<img src="/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/image-20221009100000394.png" alt="image-20221009100000394" style="zoom:80%; margin:auto;">
<h3 id="光流图">光流图</h3>
<p>对于从视频帧中得到的光流图，要利用光流图间的信息，作者提出了两种光流图叠加方式：</p>
<ol>
<li>optical flow stacking：直接将光流图对应位置叠加在一起，简单但没有充分利用光流信息</li>
<li>trajectory stacking：沿光流轨迹进行数值叠加（理论上有效，但实验效果不如1）</li>
</ol>
<img src="/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/image-20221009100819605.png" alt="image-20221009100819605" style="zoom:80%; margin:auto;">
<p><strong>Bi-directional optical flow</strong>：</p>
<p>​	双向光流，在一个帧的前向和后向区间进行光流计算，达到双向传递的效果。</p>
<h2 id="I3D">I3D</h2>
<ul>
<li>论文《Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset》</li>
<li>参考：<a href="https://www.bilibili.com/video/BV1tY4y1p7hq">https://www.bilibili.com/video/BV1tY4y1p7hq</a></li>
</ul>
<p>这篇论文主要有两个贡献：</p>
<ol>
<li>提出了I3D</li>
<li>提出了一个数据集Kinetics（类别均衡、规模适中、可玩性强，可作为视频任务的预训练数据集，地位相当于图片任务中的ImageNet）</li>
</ol>
<h3 id="Inflated-3D-Network">Inflated 3D Network</h3>
<p>把一个2D模型直接扩张到3D，使用2D模型的参数去初始化3D模型。现在常说的I3D网络一般是指基于ResNet的3D网络。</p>
<p>对于视频动作识别任务，网络发展主要经过如下a,b,c,d四个阶段。其中e为作者在Kinetics数据集上提出的I3D网络。</p>
<img src="/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/image-20221010205800716.png" alt="image-20221010205800716" style="zoom:80%; margin:auto;">
<p>作者提出这种网络的根据是，随着训练集规模的增大，使用3D CNN可以取得比2D CNN更好的效果，同时，使用光流信息，可以帮助模型取得更好的结果。</p>
<p><strong>Inflating 2D ConvNets into 3D</strong></p>
<p><em>2D Network &gt;&gt;&gt; 3D Network</em></p>
<p>保持2D网络的架构不变，直接将2D的卷积层、池化层等膨胀到3D（？实现了后向兼容），得到对应的3D神经网络架构。</p>
<p>tip：作者以及后人经过实验发现，最好不要做<strong>时间维度上的下采样</strong>。</p>
<p><strong>Bootstrapping 3D Filters from 2D  Filters</strong></p>
<p><em>2D Model &gt;&gt;&gt; 3D Model</em></p>
<p>将2D模型的参数在时间维度上进行复制，即得到对应的3D模型。通过代码的实现如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">temporal_dim = weights3d[key3d].shape[<span class="number">2</span>]</span><br><span class="line">inflated_2d = nd.broadcast_to(temporal_2d, shape=[<span class="number">0</span>, <span class="number">0</span>, temporal_dim, <span class="number">0</span>, <span class="number">0</span>]) / temporal_dim</span><br></pre></td></tr></table></figure>
<p>代码中除以temporal_dim的操作，是为了实现一个rescaling，使得对于分类任务，2D网络和3D网络的输出完全一致。</p>
<h2 id="基于深度学习的视频动作识别综述">基于深度学习的视频动作识别综述</h2>
<ul>
<li>论文：《A Comprehensive Study of Deep Video Action Recognition》</li>
<li>参考：<a href="https://www.bilibili.com/video/BV1fL4y157yA">https://www.bilibili.com/video/BV1fL4y157yA</a></li>
</ul>
<p>截至Video Transformer提出前，视频动作识别模型的发展历程如下图所示：</p>
<img src="/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/image-20221011160226907.png" alt="image-20221011160226907" style="zoom:67%; margin:auto;">
<h3 id="Hand-crafted–-CNN">Hand-crafted–&gt;CNN</h3>
<p>DeepVideo：没啥意思，通过一系列2D卷积操作，对视频进行分类。</p>
<h3 id="双流网络-2">双流网络</h3>
<p>双流网络是在2D CNN基础上，添加一个处理光流的分支。针对初版双流网络的问题，主要在如下四个方面进行改进。</p>
<ol>
<li>将late fusion改造为一种合适的early fusion；</li>
<li>把双流网络中使用的CNN网络进行变体，如融入之后提出的ResNet等；</li>
<li>双流网络的空间流和时间流分支是直接抽取特征后进行分类，可以换成RNN或LSTM等充分利用时序信息；</li>
<li>双流网络利用的是短时间内的视频信息，而一个动作往往持续时间较长，如何进行长视频的理解也是需要解决的问题。</li>
</ol>
<p>==双流网络目前仍存在的一个问题是，抽取光流的过程耗时巨大，在推理时，仍然需要花费较长时间去抽取光流，无法达到实时处理的要求（实时处理一般要求帧率达25fps）；另一方面，光流图的存储占据空间较多。==</p>
<h4 id="Beyond-Short-Snippets（引入LSTM）">Beyond Short Snippets（引入LSTM）</h4>
<p>提高特征提取使用的帧数</p>
<ul>
<li>Conv Pooling：</li>
<li>LSTM：带来的提升有限，可能是因为短时内语义信息没有显著变化，而LSTM只有在语义信息发生较大变化时作用显著。</li>
</ul>
<h4 id="Convolutional-Fusion（使用early-fusion）">Convolutional Fusion（使用early fusion）</h4>
<ul>
<li>spatial fusion：在空间层面上对特征图进行fusion</li>
<li>在网络的哪个部分进行fusion</li>
<li>temporal fusion：如何在时间轴维度上进行fusion</li>
</ul>
<h4 id="TSN（Temporal-Segment-Networks，解决长视频理解问题）">TSN（Temporal Segment Networks，解决长视频理解问题）</h4>
<p>将一个视频分成多个段，每个段分别送入双流网络（不同段的双流网络共享一组参数），将不同段在空间流输出的结果进行segmental consensus（共识），同样对时间流的输出结果进行相同计算，然后两个流的输出融合后，得到最终输出结果。</p>
<img src="/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/image-20221012102455303.png" alt="image-20221012102455303" style="zoom:80%; margin:auto;">
<p>除此之外，这篇论文也提出了一些技巧：</p>
<ol>
<li>Cross Modality Pre-training：将空间流和时间流（即光流）输入看作多模态问题，使用ImageNet上训练的模型对光流分支进行初始化，需要对预训练模型进行一下改造（将原来3通道的卷积加权平均，应用到20个通道的光流输入上），这种思想与I3D之后使用的方法类似。</li>
<li>Partial BN：缓解BN在视频理解任务上的过拟合问题，在视频数据集上对预训练模型进行训练时，冻住除第一层外的所有BN层。</li>
<li>corner cropping：强制模型在边角位置进行裁剪</li>
<li>scale- jittering：通过改变输入的长宽比，增加输入的多样性</li>
</ol>
<h3 id="3D-CNN">3D CNN</h3>
<p>在双流网已经取得很好效果的时候，人们关注于3D CNN网络，旨在提高模型的速度，以达到实时的效果。</p>
<pre class="mermaid">graph LR
A[C3D]-->B[I3D]
B-->C[Non-local]
C-->D[R2+1D]
D-->E[SlowFast]</pre>
<h4 id="C3D（2015）">C3D（2015）</h4>
<p>C3D提出了类似于VGG的3D卷积神经网络，在sports 1million数据集上进行训练，该模型可以分为特征抽取和分类两部分，作者提供了训练好的特征抽取模型接口，使得C3D在视频理解领域流行起来。</p>
<img src="/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/image-20221012112941810.png" alt="image-20221012112941810" style="zoom:80%; margin:auto;">
<h4 id="I3D（2017）">I3D（2017）</h4>
<p>I3D的主要贡献包括如下两方面：</p>
<ol>
<li>降低了视频理解模型的训练难度（将2D模型膨胀至3D）</li>
<li>提出了一个很好、很大的数据集</li>
</ol>
<h4 id="Non-local（2018）">Non-local（2018）</h4>
<p>使用自注意力机制替代LSTM，用在视频理解网络中，进行长距离建模。</p>
<p>Non-local模块与自注意力模块相近，只是扩展到了3D上，在<strong>时间和空间</strong>维度上进行自注意力计算，同样具有即插即用的性质。</p>
<img src="/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/image-20221012115426776.png" alt="image-20221012115426776" style="zoom: 67%; margin:auto;">
<h4 id="R-2-1-D（2018）">R(2+1)D（2018）</h4>
<p>作者做了大量的实验，探究视频理解任务中的网络结构。经过实验证明，使用R(2+1)D的结构，把3D卷积拆分成两个卷积，先在空间上做2D的卷积，再在时间上做1D的卷积。</p>
<img src="/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/image-20221014103245936.png" alt="image-20221014103245936" style="zoom:67%; margin:auto;">
<p>R(2+1)D的具体拆分方法如下图所示：其中a表示正常的3D卷积，b表示R(2+1)D，Mi指特征投射操作，进行维度变化，使得R(2+1)D网络参数尽可能与3D网络保持一致，从而证明R(2+1)D网络的优越性。</p>
<img src="/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/image-20221014104229339.png" alt="image-20221014104229339" style="zoom:80%; margin:auto;">
<p>该方法的有效性，可以从以下几个方面解释：</p>
<ol>
<li>relu操作增多，提高了模型的非线性学习能力</li>
<li>1D和2D网络比3D网络更容易训练与优化</li>
</ol>
<h4 id="SlowFast（2019）">SlowFast（2019）</h4>
<p>SlowFast借鉴了双流网络的思想，但并没有使用光流。</p>
<p>SlowFast网络的原理来自人体视觉细胞，用80%的细胞去捕捉慢的场景信息，用20%的细胞捕捉快的高频率的运动信息。对应到网络结构上，抽取低帧率图像输入到慢分支中（小输入大网络），抽取高帧率图像输出快分支中（大输入小网络），同时每组卷积层后都跟一个later connection，</p>
<img src="/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/image-20221014105622585.png" alt="image-20221014105622585" style="zoom:67%; margin:auto;">
<h3 id="Video-Transformer">Video Transformer</h3>
<p>将Transformer直接应用到视频理解领域，扩展到在时间和空间两个维度进行自注意力计算，计算成本极高，很难训练起来。因此人们通过对时间和空间注意力<strong>拆分</strong>的方式，降低计算成本的同时，尽可能提升模型性能。</p>
<h4 id="Timesformer（2021）">Timesformer（2021）</h4>
<p>Timesformer比较了几种Video Transformer的实现方式。经过实验验证，T+S的实现取得了最好的效果。</p>
<img src="/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/image-20221014113000002.png" alt="image-20221014113000002" style="zoom:80%; margin:auto;">
<ul>
<li>S：即经典的用在图像处理上的ViT</li>
<li>ST：暴力地在时间和空间两个维度计算注意力，但内存占用太大，难以实现</li>
<li>T+S：借鉴了R(2+1)D的思想，先在时间上计算自注意力，再在空间上计算自注意力</li>
<li>L+G：先在局部的小窗口计算自注意力，再在全局计算自注意力</li>
<li>T+W+H：只沿着特定的轴做attention，</li>
</ul>
<p>以上几种方法进行可视化后如下图所示，其中蓝色的色块表示基准点，其它相同颜色的色块则表示用于同基准点计算attention。</p>
<img src="/2022/10/09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/image-20221015084134927.png" alt="image-20221015084134927" style="zoom: 67%; margin: auto;">
<ul>
<li>L+G：其中黄色的色块表示计算local attention的色块，紫色的色块表示计算global attention的色块（为了降低计算量，global也缩小了范围）</li>
</ul>
<h4 id="transformer未来方向">transformer未来方向</h4>
<p>在视频理解领域，transformer大致有如下几个研究方向：</p>
<ol>
<li>利用transformer长时间建模的能力，进行长视频理解</li>
<li>多模态学习</li>
<li>自监督学习</li>
</ol>
]]></content>
      <tags>
        <tag>视频理解</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Masked Autoencoders Enable Efficient Knowledge Distillers</title>
    <url>/2022/10/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AMasked-Autoencoders-Enable-Efficient-Knowledge-Distillers/</url>
    <content><![CDATA[<ul>
<li>参考：<a href="https://mp.weixin.qq.com/s/q0Bq-R2ZD_lOmxdhW2qy1w">https://mp.weixin.qq.com/s/q0Bq-R2ZD_lOmxdhW2qy1w</a></li>
<li>Code：<a href="https://github.com/UCSC-VLAA/DMAE">https://github.com/UCSC-VLAA/DMAE</a></li>
</ul>
<h2 id="论文简介">论文简介</h2>
<p>作者在MAE框架的基础上加入知识蒸馏技术，在MAE的预训练阶段进行特征对齐蒸馏，提出DMAE，探索了MAE从预训练模型中获取知识的能力。</p>
<p>（感觉这篇文章的意义就是把一个大的MAE，通过知识蒸馏，蒸馏到了一个小的MAE模型上，这个小的MAE模型取得了很好的效果QAQ）</p>
<p>DMAE的整体框架如下图所示：</p>
<img src="https://mmbiz.qpic.cn/mmbiz_png/ibaXaPIy7jV1XPqibfEWTuS2fgQlJhamfNBMNHY1KW1UfgnupzicbFqGe5zrhyqreTHY2ORyBKe8qJ889knzBQjxw/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片" style="zoom:80%;">
]]></content>
      <tags>
        <tag>Transformer</tag>
        <tag>知识蒸馏</tag>
      </tags>
  </entry>
</search>
