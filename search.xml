<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>经典模型：Swin Transformer</title>
    <url>/2022/09/26/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9ASwin-Transformer/</url>
    <content><![CDATA[<h2 id="模型结构">模型结构</h2>
<p>Swin（即Shifted Windows） Transformer可以作为CV的一种通用主干，用在分类、检测、语义分割等多种视觉任务上。</p>
<p>Swin Transformer的提出解决了ViT具有的以下两个问题：</p>
<ol>
<li>ViT中，由于每个token的size大小相同，难以捕捉<strong>多尺度</strong>信息。</li>
<li>ViT的自注意力<strong>计算复杂度</strong>是图像大小的二次方。</li>
</ol>
<p>Swin-T构造了层次化特征图，并将自注意力的计算复杂度降为线性相关。</p>
<h3 id="整体结构">整体结构</h3>
<p>Swin-T的整体架构如下图所示：</p>
<img src="https://img-blog.csdnimg.cn/20210908164930810.png" alt="img" style="zoom:100%; margin:auto;">
<h3 id="Patch-Partition">Patch Partition</h3>
<p>对于每个为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>×</mo><mi>W</mi><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">H \times W \times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span>的输入，划分为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>×</mo><mn>4</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">4 \times 4 \times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span>大小的patch，每张图像被拆分为个patches，将每个patch展平作为一个token。</p>
<h3 id="Linear-Embedding">Linear Embedding</h3>
<p>即一个全连接层，将每个大小为48的token映射到设定的维度C，此时，每张图片的输入变为了<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>H</mi><mn>4</mn></mfrac><mo>×</mo><mfrac><mi>W</mi><mn>4</mn></mfrac><mo>×</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">\frac{H}{4} \times \frac{W}{4} \times C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>，然后输入Swin Transformer Block。</p>
<h3 id="Swin-Transformer-Block">Swin Transformer Block</h3>
<p>对于Transformers中使用的全局自注意力机制，需要计算每个token与其它所有tokens间的关系，计算复杂度为<strong>token数的平方</strong>。不适用于对大量tokens进行密集预测或表示高分辨率图像等视觉问题。</p>
<h4 id="W-MSA">W-MSA</h4>
<p>Swin-T通过<strong>在局部窗口中计算自注意力</strong>，将计算复杂度降低为token数的线性关系，设每个非重叠局部窗口中包含<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>×</mo><mi>M</mi></mrow><annotation encoding="application/x-tex">M \times M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span>个tokens。</p>
<ul>
<li>MSA：有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">hw</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">h</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span>个tokens，每个token在全局计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">hw</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">h</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span>次；</li>
<li>W-MSA：有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">hw</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">h</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span>个tokens，每个token在全局计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>M</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">M^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>次。</li>
</ul>
<h4 id="SW-MSA">SW-MSA</h4>
<p>W-MSA限制了跨窗口token间的交流与联系，从而限制了建模表征能力。作者提出了一种<strong>移位窗口划分</strong>方法SW-MSA，在模型中交替使用两种MSA方法（因此每个stage中Swin Transformer Block的数量都为偶数）。</p>
<p>所谓的移动窗口即将窗口循环位移，如下图所示：</p>
<img src="https://img-blog.csdnimg.cn/20210908164652285.png" alt="img" style="zoom:100%; margin:auto;">
<p>但直接移位得到的窗口大小是不规则的，不利于并行计算，同时9个窗口也提升了计算成本。为了解决这个问题，将重新划分后的窗口进行拼接，如下图所示，得到4个窗口。</p>
<img src="https://img-blog.csdnimg.cn/2021090920295156.png" alt="img" style="zoom:130%; margin:auto;">
<p>4个窗口中来自不同初始位置的patch不应进行自注意计算，因此使用mask机制，将不需要的注意力图置0。</p>
<h4 id="相对位置偏置">相对位置偏置</h4>
<p>在计算自注意力时，在计算相似度的过程中对每个head加入相对位置偏置，如下所示：</p>
<p><img src="https://img-blog.csdnimg.cn/20210911231011149.png" alt="img"></p>
<ul>
<li>对于预训练中学到的相对位置偏置，可以通过双三次插值初始化具有不同窗口大小的微调模型。</li>
</ul>
<h3 id="Patch-Merging">Patch Merging</h3>
<p>Patch Merging层的功能是产生一个层次化表示，通过<strong>合并相邻的tokens</strong>，减少tokens的数目。</p>
<p>对于Stage1和Stage2间的Patch Merging层，将原维度为C的token合并为大小为4C的token，再使用一个线性层将输出维度降低为2C，token的数目降低为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>H</mi><mn>8</mn></mfrac><mo>×</mo><mfrac><mi>W</mi><mn>8</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{H}{8} \times \frac{W}{8}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">8</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">8</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>。</p>
<p>在之后的每个stage中，都会改变张量的维度，从而形成一种层次化的特征。</p>
<h2 id="代码实现">代码实现</h2>
]]></content>
      <tags>
        <tag>经典模型</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>HelloWorld!</title>
    <url>/2020/08/29/HelloWorld/</url>
    <content><![CDATA[<h2 id="A-new-world">A new world!</h2>
<p>哈喽！历经一个下午博客终于搭建好了，原来是那么容易的一件事情，大一的时候想的很复杂，迟迟没能动手，现在也终于有了自己的小博客啦~</p>
<p>未来灌水的文章还是会首先发在CSDN上，这里会分享一些重大的经历~已经大三了！要更努力学习！不要被些奇奇怪怪的事情干扰，奥里给！</p>
]]></content>
  </entry>
  <entry>
    <title>经典模型：Vision Transformer</title>
    <url>/2022/09/26/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AVision-Transformer/</url>
    <content><![CDATA[<h2 id="模型介绍">模型介绍</h2>
<ul>
<li>可参考博客：<a href="https://blog.csdn.net/qq_39478403/article/details/118704747">https://blog.csdn.net/qq_39478403/article/details/118704747</a></li>
</ul>
<h3 id="模型结构-2">模型结构</h3>
<p>ViT主要使用Transformer的encoder部分</p>
<img src="/2022/09/26/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AVision-Transformer/image-20220926083713750.png" alt="image-20220926083713750" style="zoom: 67%; margin:auto;">
<ul>
<li>将一张图像分成若干个大小固定且相同的patch，将每个patch投影到线性空间中再加上位置编码</li>
<li>除了每个patch作为一个token外，在序列中添加一个额外的classification token</li>
</ul>
<p><strong>位置编码：（看下代码咋实现的）</strong></p>
<ul>
<li>使用可学习的一维位置编码（作者发现使用更高维的位置编码并没有带来显著的精度提升）</li>
</ul>
<p><strong>混合结构：</strong></p>
<ul>
<li>可以将原始图像使用CNN进行特征提取，将特征图按patch划分送入Transformer中。</li>
</ul>
<h3 id="实验设置">实验设置</h3>
<p><strong>ViT使用：</strong></p>
<ul>
<li>ViT与Bert类似，先在大数据集上<strong>训练</strong>，再在downstream任务上<strong>微调</strong>。在微调时，将预训练用的预测头换成一个用0初始化的DxK的前馈层，其中K代表下游任务总的类别数。</li>
<li>在微调时，使用更高分辨率的图像可以获得更好的结果。（patch大小不变，输入序列变成，不影响网络结构），这样会导致之前训练得到的<strong>位置编码</strong>无意义，因此在原来的位置编码上进行一个2D的插值。</li>
</ul>
<p><strong>实验结果：</strong></p>
<ul>
<li>当<strong>考虑预训练的训练代价</strong>时，ViT以更低的代价达到了SOTA水平</li>
<li>在<strong>自监督</strong>问题上，ViT很有应用前景</li>
</ul>
<h2 id="代码复现">代码复现</h2>
<ul>
<li>Official Code：<a href="https://github.com/google-research/vision_transformer">https://github.com/google-research/vision_transformer</a></li>
<li>Using Code：<a href="https://github.com/jeonsworld/ViT-pytorch">https://github.com/jeonsworld/ViT-pytorch</a></li>
</ul>
]]></content>
      <tags>
        <tag>经典模型</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：IDPT, Interconnected Dual Pyramid Transformer for Face Super-Resolution</title>
    <url>/2022/08/14/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AIDPT/</url>
    <content><![CDATA[<ul>
<li>
<p>研究问题：人脸超分辨率技术</p>
</li>
<li>
<p>FSR：关注于恢复重要的面部结构</p>
</li>
<li>
<p>创新点：提出了一个新的、有效的基于Transformer的人脸超分辨率架构</p>
<ol>
<li>设计了金字塔结构的encode/decoder的Transformer架构：分别提取粗糙纹理和精细纹理。</li>
<li>通过一个底部的金字塔特征提取器，将双重金字塔Transformer建立起联系。</li>
<li>在每个spatial layer插入一个新的融合调制模块：使用粗糙纹理完善对应的精细纹理，融合浅层的粗糙纹理和对应的深层的精细纹理。</li>
</ol>
</li>
<li>
<p>FSR研究现状</p>
<ol>
<li>现有技术在解决超低分辨率问题上表现很差</li>
<li>卷积难以描述不同域间的关联和捕捉远域间的依赖</li>
</ol>
</li>
<li>
<p>网络结构：</p>
<img src="/2022/08/14/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AIDPT/1.png" alt="image-20220821153651772" style="zoom:80%;">
</li>
</ul>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>FSR</tag>
        <tag>组内文章</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Model Behavior Preserving for Class-Incremental Learning</title>
    <url>/2022/08/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AModel-Behavior-Preserving-for-Class-Incremental-Learning/</url>
    <content><![CDATA[<ul>
<li>发表时间：2022</li>
<li>研究内容：类增量学习，探讨在增量学习中应保留旧模型的哪些功能性属性。</li>
<li>研究现状：现有的增量学习方法忽略了CNN模型响应间的内部结构，KD的硬约束导致新模型出现混沌行为。</li>
<li>创新点：
<ol>
<li>
<p>Feature Space：设计了一个INP Loss保持成对实例在旧模型上的<strong>相似性顺序</strong>（反映实例集间的相邻关系）；</p>
<p>INP用于惩罚新模型在学习过程中每个实例相邻关系的变化。</p>
<img src="/2022/08/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AModel-Behavior-Preserving-for-Class-Incremental-Learning/image-20220826093609961.png" alt="image-20220826093609961" style="zoom:67%;margin:auto;">
<p>a. 旧实例A在特征空间中与其它实例的相邻关系；</p>
<p>b. 采用传统的KD，引入新实例G后，绝对位置的微小变化被严格限制；</p>
<p>c. 采用INP Loss当相对位置没变时，就不会限制更新。</p>
</li>
<li>
<p>Label Space：设计了一个LPP Loss在输出空间的实例标签概率向量中保留<strong>标签排名列表</strong>（反映实例属于每一类的排名）；</p>
</li>
<li>
<p>介绍了一种可导的排名计算方法用于计算上述Loss。</p>
</li>
</ol>
</li>
</ul>
]]></content>
      <tags>
        <tag>组内文章</tag>
        <tag>连续学习</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Social Distancing Alert with Smartwatches</title>
    <url>/2022/09/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ASocial-Distancing-Alert-with-Smartwatches/</url>
    <content><![CDATA[<ul>
<li>
<p>发表时间：2022</p>
</li>
<li>
<p>研究内容：基于智能手表的社交距离警报系统SoDA，SoDA使用加速器和陀螺仪的数据和简单有效的视觉Transformer模型，识别违反社交距离的活动。</p>
</li>
<li>
<p>code：<a href="https://github.com/aiotgroup/SoDA">https://github.com/aiotgroup/SoDA</a></p>
</li>
<li>
<p>创新点：</p>
<ol>
<li>应用价值</li>
<li>创建了一个数据集</li>
<li>证明了ViT是一种有效的方法？</li>
</ol>
</li>
<li>
<p>模型结构：</p>
<img src="/2022/09/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ASocial-Distancing-Alert-with-Smartwatches/image-20220920104544756.png" alt="image-20220920104544756" style="zoom:80%;">
</li>
</ul>
]]></content>
      <tags>
        <tag>Transformer</tag>
        <tag>组内文章</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：PhysFormer: Facial Video-based Physiological Measurement with Temporal Difference Transformer</title>
    <url>/2022/09/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9APhysFormer-Facial-Video-based-Physiological-Measurement-with-Temporal-Difference-Transformer/</url>
    <content><![CDATA[<ul>
<li>
<p>发表时间：2021</p>
</li>
<li>
<p>研究对象：rPPG，使用多波长 RGB 相机检测人体皮肤表面脉冲引起的细微颜色变化，实现测量心脏活动和其它生理信号。</p>
</li>
<li>
<p>研究意义：传统的检测方法会造成discomfort，并且长期检测不方便</p>
</li>
<li>
<p>Code：<a href="https://github.com/ZitongYu/PhysFormer">https://github.com/ZitongYu/PhysFormer</a></p>
</li>
<li>
<p>rPPG研究历史：</p>
<ol>
<li>早期使用经典的信号处理方法检测面部细微的颜色变化；</li>
<li>使用非端到端方法，首先生成预处理的信号特征，然后模型从这些特征图中捕捉rPPG特征（对预处理要求严格，忽略了全局特征）；</li>
<li>端到端的基于深度学习的方法（容易被复杂的背景信息影响）。</li>
</ol>
</li>
<li>
<p>研究现状：</p>
<p>​		现有的基于卷积神经网络的模型在时间和空间上的感受野受限，忽略了长期的时间和空间上的互动与感知。</p>
</li>
<li>
<p>PhysFormer网络架构：</p>
<img src="/2022/09/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9APhysFormer-Facial-Video-based-Physiological-Measurement-with-Temporal-Difference-Transformer/image-20220919092041506.png" alt="image-20220919092041506" style="zoom:80%;">
<ol>
<li><strong>Stem</strong>：提取粗糙的局部时空特征</li>
<li><strong>Tube Tokens</strong>：将stem输出划分为若干个时空tube token，将时空邻近语义聚合在一起，并减少后续transformer的计算量</li>
<li><strong>Temporal Difference Multi-head Self-attention</strong>：与传统的自注意力机制不同，使用TDC计算距离，可以捕捉局部细粒度的时间差异特征</li>
<li><strong>Spatio-temporal Feed-forward</strong>：</li>
</ol>
</li>
<li>
<p>创新点：PhysFormer，一种端到端的视频transformer，联合使用了局部的和全局的时空特征。</p>
<ol>
<li>使用<strong>时差引导全局注意力机制</strong>，强化rPPG的周期性特征，针对干扰完善局部时空特征；</li>
<li>使用受label distribution learning和curriculum learning启发的频域动态约束，为PhysFormer提供详细的监督，缓解过拟合。</li>
<li>PhysFormer不需要像其它transformer网络那样在大规模数据集上预训练，仅在rPPG数据集训练即可。</li>
</ol>
</li>
<li>
<p><strong>Label Distribution Learning</strong>：对于面部的rPPG信号，心率相近的视频会有相似的周期性特征。为了使得模型学习到这种特征，将心率估计问题看作一个多分类问题，有多少个心率就有多少类别，类别概率向量由高斯分布组成。</p>
</li>
<li>
<p><strong>Curriculum Learning Guided Dynamic Loss</strong>：课程式学习是指模型从容易样本开始学习，逐步学习困难样本。在该任务中，从时域和频域两个方面限制模型学习，时域的限制更直接更容易学习，频域的限制较难学习，因此，使用动态的loss函数，逐步提高频域loss的比例。</p>
</li>
<li>
<p>实验：不同模型的对比实验、消融实验</p>
</li>
<li>
<p>Others：注意力图可视化</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>Transformer</tag>
        <tag>组内文章</tag>
        <tag>PPG</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Structural Knowledge Organization and Transfer for Class-Incremental Learning</title>
    <url>/2022/08/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AStructural-Knowledge-Organization-and-Transfer-for-Class-Incremental-Learning/</url>
    <content><![CDATA[<ul>
<li>
<p>发表时间：2021</p>
</li>
<li>
<p>研究问题：类增量学习</p>
</li>
<li>
<p>研究现状：</p>
<ol>
<li>
<p><strong>经典的知识蒸馏方法</strong>忽略了信息点之间的关联，当新数据远多于旧数据时，面临着严重的偏差问题；</p>
<ul>
<li>
<p>KD：在特征空间中，孤立地限制单个训练样本的位置，样本间的关系可能会被改变，并导致分类错误。</p>
</li>
<li>
<p>SGKD：保持样本的结构化知识，包括样本的位置和样本间关系，确保蒸馏后样本仍能被正确地分类。</p>
</li>
</ul>
<img src="/2022/08/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AStructural-Knowledge-Organization-and-Transfer-for-Class-Incremental-Learning/image-20220822101131518.png" alt="image-20220822101131518" style="zoom:67%;margin:auto;">
</li>
</ol>
</li>
<li>
<p>创新点：</p>
<ol>
<li>
<p>使用一个<strong>memory knowledge graph</strong>(MKG)表征历史任务的结构化知识</p>
<ul>
<li>在特征空间中的绝对位置（MKG中用顶点表示已知example间的特征向量）</li>
<li>example间对应关系（边表示，使用余弦距离）</li>
</ul>
</li>
<li>
<p>使用<strong>图插值机制</strong>丰富知识域、缓解类间样本不平衡问题</p>
<p>通过向MKG中插入假的顶点，扩充和平滑分散的数据集，假顶点通过mix两个真顶点的vector得到。</p>
</li>
<li>
<p>使用**结构化图知识蒸馏（SGKD）**迁移旧知识</p>
<ul>
<li>顶点蒸馏损失</li>
<li>边蒸馏损失</li>
</ul>
<img src="/2022/08/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AStructural-Knowledge-Organization-and-Transfer-for-Class-Incremental-Learning/image-20220823094630810.png" alt="image-20220823094630810" style="zoom:67%;margin:auto;">
</li>
</ol>
</li>
<li>
<p>人脸识别是怎么实现增加新样本的？</p>
<p>人脸识别网络的本质是一个特征提取器，并不是分类器，识别人脸的时候，通过计算输出特征和人脸库中人脸的距离判断人脸所属对象。未涉及类增量学习。</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>组内文章</tag>
        <tag>连续学习</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Rethinking the ST-GCNs for 3D skeleton-based human action recognition</title>
    <url>/2022/09/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARethinking-the-ST-GCNs-for-3D-skeleton-based-human-action-recognition/</url>
    <content><![CDATA[<ul>
<li>发表时间：2021</li>
<li>ST-GCN：Spatial-Temporal Graph Convolutional Network，用于解决骨骼数据的动作识别问题。</li>
<li>研究内容：
<ol>
<li>证明了在ST-GCN中很多操作对于人体动作识别是没必要的</li>
<li>提出了一个简单有效的策略捕捉全局图的相关性，对输入序列进行有效建模，同时将输入图序列降入欧几里得空间，可以使用多尺度时域滤波器捕捉动态信息。</li>
</ol>
</li>
<li>研究现状：
<ol>
<li>骨骼数据成为人体动作识别的主流输入（与传统的RGB视频数据相比，信息更完整）</li>
<li>直接将结构化的数据重新排列，使得tensor适应基础的神经网络（由于骨骼数据中没有天然的局部性概念，深度学习的能力受到限制）</li>
<li>设计一种适应结构化数据的自定义神经网络（ST-GCN）</li>
</ol>
</li>
</ul>
<p>TBC：GCN好难，看不懂</p>
]]></content>
      <tags>
        <tag>组内文章</tag>
        <tag>人体动作识别</tag>
        <tag>GCN</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Revisiting Pixel-Wise Supervision for Face Anti-Spoofing</title>
    <url>/2022/09/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARevisiting-Pixel-Wise-Supervision-for-Face-Anti-Spoofing/</url>
    <content><![CDATA[<ul>
<li>
<p>发表时间：2021</p>
</li>
<li>
<p>研究内容：像素级的人脸识别反欺诈方法</p>
</li>
<li>
<p>创新点：提出基于<strong>金字塔</strong>的监督方法，模型从多空间尺度上学习局部和全局的语义信息</p>
</li>
<li>
<p>Presentation Attack Detection研究历史：</p>
<ol>
<li>
<p>传统算法关注于<strong>活体</strong>和<strong>手工特征</strong>的检测，需要丰富的任务级的先验知识。</p>
<p>活体检测：关注眨眼、面部和头部动作、视线追踪以及远程生理信号（这种方法需要长期的互动，容易被video attacks伪造）。</p>
<p>经典的handcrafted descriptors：从多种色彩空间中提取有效的欺诈模式，这种PA方法可以通过训练分类器捕捉，但在遇到未见过的场景或未知的PAs时就失效了。</p>
</li>
<li></li>
<li></li>
</ol>
</li>
<li></li>
</ul>
]]></content>
      <tags>
        <tag>组内文章</tag>
        <tag>人脸识别</tag>
      </tags>
  </entry>
</search>
