<!DOCTYPE html>
<html  lang="zh-CN" >
    <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, minimum-scale=1, initial-scale=1, maximum-scale=5, viewport-fit=cover">
    <title>论文阅读：RhythmNet: End-to-End Heart Rate Estimation From Face via Spatial-Temporal Representation | ymy is watching u!!!!</title>
    <meta name="description" content="发表信息：TIP 2020  Code：https:&#x2F;&#x2F;github.com&#x2F;AnweshCR7&#x2F;RhythmNet 提出了VIPL-HR数据集和STMap的提取方法。   前置知识基于视频的HR估计方法现存的基于视频的HR估计方法主要可以分为两种：  基于rPPG的方法：通过捕捉面部颜色变化提取HR信号； 基于BCG的方法：BCG即心冲击描记器，通过捕捉头部运动提取HR信号，这种头部运动是随着">
<meta property="og:type" content="article">
<meta property="og:title" content="论文阅读：RhythmNet: End-to-End Heart Rate Estimation From Face via Spatial-Temporal Representation">
<meta property="og:url" content="https://ymyforever.netlify.app/2022/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARhythmNet-End-to-End-Heart-Rate-Estimation-From-Face-via-Spatial-Temporal-Representation/index.html">
<meta property="og:site_name" content="ymy is watching u!!!!">
<meta property="og:description" content="发表信息：TIP 2020  Code：https:&#x2F;&#x2F;github.com&#x2F;AnweshCR7&#x2F;RhythmNet 提出了VIPL-HR数据集和STMap的提取方法。   前置知识基于视频的HR估计方法现存的基于视频的HR估计方法主要可以分为两种：  基于rPPG的方法：通过捕捉面部颜色变化提取HR信号； 基于BCG的方法：BCG即心冲击描记器，通过捕捉头部运动提取HR信号，这种头部运动是随着">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic2.zhimg.com/v2-49244046a83e30ef2383b94644bf0f31_r.jpg">
<meta property="og:image" content="https://ymyforever.netlify.app/2022/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARhythmNet-End-to-End-Heart-Rate-Estimation-From-Face-via-Spatial-Temporal-Representation/image-20221027094214511.png">
<meta property="og:image" content="https://ymyforever.netlify.app/2022/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARhythmNet-End-to-End-Heart-Rate-Estimation-From-Face-via-Spatial-Temporal-Representation/image-20221027101908561.png">
<meta property="og:image" content="https://ymyforever.netlify.app/2022/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARhythmNet-End-to-End-Heart-Rate-Estimation-From-Face-via-Spatial-Temporal-Representation/image-20221027161400274.png">
<meta property="og:image" content="https://ymyforever.netlify.app/2022/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARhythmNet-End-to-End-Heart-Rate-Estimation-From-Face-via-Spatial-Temporal-Representation/image-20221027164127947.png">
<meta property="og:image" content="https://ymyforever.netlify.app/2022/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARhythmNet-End-to-End-Heart-Rate-Estimation-From-Face-via-Spatial-Temporal-Representation/image-20221028135023127.png">
<meta property="og:image" content="https://ymyforever.netlify.app/2022/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARhythmNet-End-to-End-Heart-Rate-Estimation-From-Face-via-Spatial-Temporal-Representation/image-20221028140443111.png">
<meta property="article:published_time" content="2022-10-27T00:50:59.000Z">
<meta property="article:modified_time" content="2022-10-28T08:39:45.123Z">
<meta property="article:author" content="ymy">
<meta property="article:tag" content="rPPG">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic2.zhimg.com/v2-49244046a83e30ef2383b94644bf0f31_r.jpg">

    
    <link rel="icon" href="/images.fvicon.ico" type="image/x-icon">

    
<link rel="stylesheet" href="/css/common.min.css">



    
        <link href="//cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.css" rel="stylesheet">
    
    
    
    
        <link href="//cdn.jsdelivr.net/npm/lightgallery.js@1.1.3/dist/css/lightgallery.min.css" rel="stylesheet">
    
    
    
<link rel="stylesheet" href="/css/iconfont.min.css">

    
<meta name="generator" content="Hexo 6.3.0"></head>

    <body>
        <header class="header header-fixture">
    <div class="profile-search-wrap flex sm:block">
        
        
        <div class="profile sm:text-center md:px-1 lg:px-3 sm:pb-4 sm:pt-6">
            <a id="avatar" role="link" href="https://github.com/ymy-forever" class="inline-block lg:w-16 lg:h-16 w-8 h-8 m-2" target="_blank" rel="noopener" rel="noreferrer" >
                <img src="/images/logo.png" class="rounded-full" alt="avatar">
            </a>
            <h2 id="name" class="hidden lg:block">ymy_forever</h2>
            <h3 id="title" class="hidden lg:block">Student &amp; Coder</h3>
            
            <small id="location" class="hidden lg:block">
                <i class="iconfont icon-map-icon"></i>
                Xian, China
            </small>
            
        </div>
        
        
<div class="search flex-1 flex lg:inline-block sm:hidden lg:px-4 lg:mt-2 lg:mb-4 lg:w-full">
    <form id="search-form" class="my-auto flex-1 lg:border lg:border-solid lg:border-gray-200">
        <div class="input-group table bg-gray-100 lg:bg-white w-full">
            <input id="search-input" type="text" placeholder="搜索" class="inline-block w-full bg-gray-100 lg:bg-white p-1">
            <span class="table-cell">
                <button name="search tigger button" disabled>
                    <i class="iconfont icon-search m-2"></i>
                </button>
            </span>
        </div>
    </form>
        
<div id="content-json" data-placeholder="搜索" class="invisible hidden">/content.json</div>
<script id="search-teamplate" type="text/html" data-path="/content.json">
    <div>
        <div class="search-header bg-gray-400">
            <input id="actual-search-input" model="keyword" ref="input" class="inline-block w-full h-10 px-2 py-1" placeholder="搜索" type="text">
        </div>
        <div class="search-result bg-gray-200">
            {{#each searchPosts}}
            <a href="/{{ path }}" class="result-item block px-2 pb-3 mb-1 pt-1 hover:bg-indigo-100">
                <i class="iconfont icon-file"></i>
                <h1 class="result-title inline font-medium text-lg">{{ title }}</h1>
                <p class="result-content text-gray-600 text-sm">{{{ text }}}</p>
            </a>
            {{/each}}
        </div>
    </div>
</script>

</div>


        <button name="menu toogle button" id="menu-toggle-btn" class="block sm:hidden p-3" role="button" aria-expanded="false">
            <i class="iconfont icon-hamburger"></i>
        </button>
    </div>
    <nav id="menu-nav" class="hidden sm:flex flex-col">
        
        
            <div class="menu-item menu-home" role="menuitem">
                <a href="/.">
                    <i class="iconfont icon-home" aria-hidden="true"></i>
                    <span class="menu-title">首页</span>
                </a>
            </div>
        
        
            <div class="menu-item menu-archives" role="menuitem">
                <a href="/archives">
                    <i class="iconfont icon-archive" aria-hidden="true"></i>
                    <span class="menu-title">归档</span>
                </a>
            </div>
        
        
            <div class="menu-item menu-tags" role="menuitem">
                <a href="/tags">
                    <i class="iconfont icon-tag" aria-hidden="true"></i>
                    <span class="menu-title">标签</span>
                </a>
            </div>
        
        
            <div class="menu-item menu-about" role="menuitem">
                <a href="/about">
                    <i class="iconfont icon-cup" aria-hidden="true"></i>
                    <span class="menu-title">关于</span>
                </a>
            </div>
        
        
<div class="social-links flex sm:flex-col lg:hidden mt-5">
    
        <span class="social-item text-center">
            <a target="_blank" rel="noopener" href="https://github.com/ymy-forever">
                <i class="iconfont social-icon icon-github"></i>
                <span class="menu-title hidden lg:inline">menu.github</span>
            </a>
        </span>
    
        <span class="social-item text-center">
            <a href="/atom.xml">
                <i class="iconfont social-icon icon-rss"></i>
                <span class="menu-title hidden lg:inline">menu.rss</span>
            </a>
        </span>
    
</div>


    </nav>
</header>

        <section class="main-section">
            
    <main class="flex-1 px-4 py-14 md:px-5 lg:px-8 lg:py-4 relative min-h-screen">
    

    <article class="content article article-archives article-type-list" itemscope="">
        <header class="article-header">
            
    
        <h1 class="article-title text-lg" itemprop="name">
            论文阅读：RhythmNet: End-to-End Heart Rate Estimation From Face via Spatial-Temporal Representation
        </h1>
    



            <p class="article-meta mb-3 text-xs">
                <span class="article-date">
    <i class="iconfont icon-calendar-check"></i>
	<a href="/2022/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARhythmNet-End-to-End-Heart-Rate-Estimation-From-Face-via-Spatial-Temporal-Representation/" class="article-date">
	  <time datetime="2022-10-27T00:50:59.000Z" itemprop="datePublished">10月 27</time>
	</a>
</span>

                

                
    <span class="article-tags">
    <i class="iconfont icon-tag"></i>
    <a class="article-tag-none-link" href="/tags/rPPG/" rel="tag">rPPG</a>
  </span>


                <span class="_partial/post-comment"><i class="icon icon-comment"></i>
                    <a href="/2022/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARhythmNet-End-to-End-Heart-Rate-Estimation-From-Face-via-Spatial-Temporal-Representation/#comments" class="article-comment-link">
                        评论
                    </a>
                </span>
                
    
        <span class="post-wordcount" itemprop="wordCount">字数统计: 1.2k(字)</span>
    
    
        <span class="post-readcount" itemprop="timeRequired">阅读时长: 4(分)</span>
    


            </p>
        </header>
        <div class="marked-body article-body">
            <ul>
<li><p>发表信息：TIP 2020</p>
</li>
<li><p>Code：<a target="_blank" rel="noopener" href="https://github.com/AnweshCR7/RhythmNet">https://github.com/AnweshCR7/RhythmNet</a></p>
<p><em>提出了VIPL-HR数据集和STMap的提取方法。</em></p>
</li>
</ul>
<h2 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h2><h3 id="基于视频的HR估计方法"><a href="#基于视频的HR估计方法" class="headerlink" title="基于视频的HR估计方法"></a>基于视频的HR估计方法</h3><p>现存的基于视频的HR估计方法主要可以分为两种：</p>
<ol>
<li>基于rPPG的方法：通过捕捉面部颜色变化提取HR信号；</li>
<li>基于BCG的方法：BCG即心冲击描记器，通过捕捉头部运动提取HR信号，这种头部运动是随着每次心跳，血液周期性地喷射产生的。</li>
</ol>
<p>现存的非接触式的HR估计方法都是基于rPPG的方法，原因是在约束较少的场景下，面部的颜色变化比微弱的头部运动更容易捕捉。</p>
<p>（2021年有篇文章用多模态的方法，联合使用了rPPG和BCG信号。）</p>
<h3 id="GRU-Gated-Recurrent-Unit"><a href="#GRU-Gated-Recurrent-Unit" class="headerlink" title="GRU(Gated Recurrent Unit)"></a>GRU(Gated Recurrent Unit)</h3><p>GRU是循环神经网络的一种，也是为了解决长期记忆和反向传播中的梯度等问题提出来的。其效果与LSTM相似，但更容易训练，其输入输出结构与普通RNN相同，如图所示，其中$x^t$表示当前节点的输入，$h^{t-1}$表示上一个节点传下来的隐状态。</p>
<img src="https://pic2.zhimg.com/v2-49244046a83e30ef2383b94644bf0f31_r.jpg" alt="img" style="zoom: 50%; margin:auto;">

<h2 id="模型评价"><a href="#模型评价" class="headerlink" title="模型评价"></a>模型评价</h2><p>模型旨在实现在低约束场景下的HR估计，包括人体姿势变化、光照变化和设备差异。</p>
<h3 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h3><ol>
<li>建立了一个可训练的端到端（端到端的方法更能满足真实使用场景的需求）的HR估计器，估计器可以应对<strong>低约束场景</strong>下的多种问题；</li>
<li>对相邻HR之间的关系使用GRU（Gated Recurrent Unit）进行有效建模；</li>
<li>建立了一个更接近真实场景的数据集VIPL-HR，包括三种不同的录制设备（RGB-D相机、智能手机和web-camera），涵盖了面部姿势、scale和光照的9种场景。</li>
</ol>
<img src="/2022/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARhythmNet-End-to-End-Heart-Rate-Estimation-From-Face-via-Spatial-Temporal-Representation/image-20221027094214511.png" alt="image-20221027094214511" style="zoom:67%; margin:auto;">

<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>RhythmNet的整体结构如图所示：</p>
<p>模型首先对输入视频进行人脸识别和人脸关键点检测，得到ROI，然后对ROI计算STMap，作为HR信号的低级特征。然后将STMap输入CNN-RNN模型，得到最终的HR。</p>
<img src="/2022/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARhythmNet-End-to-End-Heart-Rate-Estimation-From-Face-via-Spatial-Temporal-Representation/image-20221027101908561.png" alt="image-20221027101908561" style="zoom:67%; margin:auto;">

<h3 id="人脸识别、关键点检测和分割"><a href="#人脸识别、关键点检测和分割" class="headerlink" title="人脸识别、关键点检测和分割"></a>人脸识别、关键点检测和分割</h3><p>文章使用开源的人脸检测器SeetaFace在视频的每一帧检测人脸，并定位了81个面部关键点。对于定义的ROI区域，使用皮肤分割去除非人脸部分。</p>
<h3 id="STMap"><a href="#STMap" class="headerlink" title="STMap"></a>STMap</h3><p>STMap的整体流程如下图所示：</p>
<img src="/2022/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARhythmNet-End-to-End-Heart-Rate-Estimation-From-Face-via-Spatial-Temporal-Representation/image-20221027161400274.png" alt="image-20221027161400274" style="zoom:80%; margin:auto;">

<ol>
<li><p>根据检测到的面部关键点，对齐不同帧间的人脸，并将对齐后的人脸变换到YUV颜色空间；</p>
<p>YUV颜色空间是在皮肤分割中常用的色彩表示，其可由RGB色彩经如下变换得到。</p>
<img src="/2022/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARhythmNet-End-to-End-Heart-Rate-Estimation-From-Face-via-Spatial-Temporal-Representation/image-20221027164127947.png" alt="image-20221027164127947" style="zoom: 67%; margin:auto;">
</li>
<li><p>将整个人脸划分为n个ROI块；</p>
</li>
<li><p>对于每个ROI，计算ROI在每帧色彩通道上的均值，并将该ROI的信号逐帧连接起来。即得到Txnxc形状的输出即STMap。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对每一帧的ROI在通道上取均值</span></span><br><span class="line"><span class="keyword">for</span> idx, frame <span class="keyword">in</span> <span class="built_in">enumerate</span>(processed_frames[start_frame_index:end_frame_index]):</span><br><span class="line">    roi_blocks = chunkify(frame)</span><br><span class="line">    <span class="comment"># 每一个roi block</span></span><br><span class="line">    <span class="keyword">for</span> block_idx, block <span class="keyword">in</span> <span class="built_in">enumerate</span>(roi_blocks):</span><br><span class="line">        avg_pixels = cv2.mean(block)	<span class="comment"># 按通道取均值</span></span><br><span class="line">        spatio_temporal_map[idx, block_idx, <span class="number">0</span>] = avg_pixels[<span class="number">0</span>]</span><br><span class="line">        spatio_temporal_map[idx, block_idx, <span class="number">1</span>] = avg_pixels[<span class="number">1</span>]</span><br><span class="line">        spatio_temporal_map[idx, block_idx, <span class="number">2</span>] = avg_pixels[<span class="number">2</span>]</span><br></pre></td></tr></table></figure></li>
</ol>
<p>⭐这里有一个值得注意的策略是，对于某些帧可能识别不到人脸（当对象移动过快或旋转时），会导致STMap和HR信号的丢失。为了解决这个问题，作者随机对一小部分STMap在时间维度上进行mask，从而模拟这种数据丢失的情况，从而加强RhythmNet的健壮性。</p>
<h3 id="HR估计模型"><a href="#HR估计模型" class="headerlink" title="HR估计模型"></a>HR估计模型</h3><ul>
<li>输入：STMap</li>
<li>输出：HR</li>
</ul>
<p>这部分的主要创新点是对两相邻clips间的处理上使用了GRU。对于CNN backbone提取的特征，输入一层GRU中，对于每个clips输出对应的HR，取所有clips的均值作为最终该视频的HR。</p>
<h2 id="关于视频压缩的研究"><a href="#关于视频压缩的研究" class="headerlink" title="关于视频压缩的研究"></a>关于视频压缩的研究</h2><p>VIPL-HR数据集原始大小为1.05TB，为了方便用户下载，作者研究了不同的视频压缩方法和帧resize方法。</p>
<ul>
<li>视频压缩算法：MJPG, FMP4, DIVX, PIM1, X264</li>
<li>resize scales：1&#x2F;2，2&#x2F;3，3&#x2F;4</li>
</ul>
<p>作者使用上述几种方法对数据集进行处理，并选择了一个经典的HR估计方法Hann2013对压缩或resize后的视频进行检测，从而选择出对HR估计影响最小的压缩算法和resize尺度。实验结果如下图所示：</p>
<img src="/2022/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARhythmNet-End-to-End-Heart-Rate-Estimation-From-Face-via-Spatial-Temporal-Representation/image-20221028135023127.png" alt="image-20221028135023127" style="zoom:80%; margin:auto;">

<p>根据实验结果，作者选择了MJPG算法对视频进行压缩，并采用了2&#x2F;3的resize尺度，将数据集大小降低至了48GB。</p>
<p>之后作者又对其提出的RhythmNet测试了数据集压缩与不压缩的影响，结果如图所示：</p>
<img src="/2022/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARhythmNet-End-to-End-Heart-Rate-Estimation-From-Face-via-Spatial-Temporal-Representation/image-20221028140443111.png" alt="image-20221028140443111" style="zoom:67%; margin:auto;">
        </div>
        
<blockquote class="copyright">
    <p><strong>本文链接 : </strong><a class="permalink" href="https://ymyforever.netlify.app/2022/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARhythmNet-End-to-End-Heart-Rate-Estimation-From-Face-via-Spatial-Temporal-Representation/">https://ymyforever.netlify.app/2022/10/27/论文阅读：RhythmNet-End-to-End-Heart-Rate-Estimation-From-Face-via-Spatial-Temporal-Representation/</a></p>
    <p><strong>This article is available under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener noreferrer">Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)</a> License</strong></p>
</blockquote>


    </article>
    
    <section id="comments">
        
    </section>


    

</main>


<aside style="" id="sidebar" class="aside aside-fixture">
    <div class="toc-sidebar">
        <nav id="toc" class="article-toc">
            <h3 class="toc-title">文章目录</h3>
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86"><span class="toc-number">1.</span> <span class="toc-text">前置知识</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E8%A7%86%E9%A2%91%E7%9A%84HR%E4%BC%B0%E8%AE%A1%E6%96%B9%E6%B3%95"><span class="toc-number">1.1.</span> <span class="toc-text">基于视频的HR估计方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GRU-Gated-Recurrent-Unit"><span class="toc-number">1.2.</span> <span class="toc-text">GRU(Gated Recurrent Unit)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7"><span class="toc-number">2.</span> <span class="toc-text">模型评价</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9"><span class="toc-number">2.1.</span> <span class="toc-text">创新点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="toc-number">3.</span> <span class="toc-text">模型结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E3%80%81%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2"><span class="toc-number">3.1.</span> <span class="toc-text">人脸识别、关键点检测和分割</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#STMap"><span class="toc-number">3.2.</span> <span class="toc-text">STMap</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HR%E4%BC%B0%E8%AE%A1%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.3.</span> <span class="toc-text">HR估计模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E%E8%A7%86%E9%A2%91%E5%8E%8B%E7%BC%A9%E7%9A%84%E7%A0%94%E7%A9%B6"><span class="toc-number">4.</span> <span class="toc-text">关于视频压缩的研究</span></a></li></ol>
        </nav>
    </div>
</aside>





        </section>
        <footer class="hidden lg:block fixed bottom-0 left-0 sm:w-1/12 lg:w-1/6 bg-gray-100 z-40">
    
    <div class="footer-social-links">
        
            <a target="_blank" rel="noopener" href="https://github.com/ymy-forever">
                <i class="iconfont icon-github"></i>
            </a>
        
            <a href="/atom.xml">
                <i class="iconfont icon-rss"></i>
            </a>
        
    </div>
    
    
</footer>

        <div id="mask" class="hidden mask fixed inset-0 bg-gray-900 opacity-75 z-40"></div>
        <div id="search-view-container" class="hidden shadow-xl"></div>
        
<script src="/js/dom-event.min.js"></script>



<script src="/js/local-search.min.js"></script>



    <script src="//cdn.jsdelivr.net/npm/lightgallery.js@1.1.3/dist/js/lightgallery.min.js"></script>
    
<script src="/js/light-gallery.min.js"></script>






    </body>
</html>
