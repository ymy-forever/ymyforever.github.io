{"meta":{"title":"ymy is watching u!!!!","subtitle":"","description":"Live my life with passion!","author":"ymy","url":"https://ymyforever.netlify.app","root":"/"},"pages":[{"title":"about","date":"2020-08-30T07:08:08.000Z","updated":"2022-08-14T02:05:40.811Z","comments":true,"path":"about/index.html","permalink":"https://ymyforever.netlify.app/about/index.html","excerpt":"","text":"DLUT计科到西交软院，计算机宇宙的小小探索者，请多指教~"},{"title":"tags","date":"2022-08-14T01:55:25.992Z","updated":"2022-08-14T01:55:25.992Z","comments":false,"path":"tags/index.html","permalink":"https://ymyforever.netlify.app/tags/index.html","excerpt":"","text":""},{"title":"aDream","date":"2020-09-15T02:32:56.000Z","updated":"2020-09-17T03:05:00.616Z","comments":true,"path":"secret/aDream.html","permalink":"https://ymyforever.netlify.app/secret/aDream.html","excerpt":"为什么会做这样的一场梦呢？ 为什么做了还让我醒来呢？ 这大概是我最不愿意醒来的梦吧。","text":"为什么会做这样的一场梦呢？ 为什么做了还让我醒来呢？ 这大概是我最不愿意醒来的梦吧。 求不得，求不得，求不得。 小的时候也做过这种怅然若失的梦，那个时候想要漂亮的文具，现在呢，想要完美的感情。 不过是想要得到偶像的认可罢了。 认可是自己给的，不要再傻了。 加油吧！ 最后，真好，我还有这一方自由乐园。"},{"title":"littletalk","date":"2020-09-14T03:58:28.000Z","updated":"2020-09-17T03:04:46.763Z","comments":true,"path":"secret/littletalk.html","permalink":"https://ymyforever.netlify.app/secret/littletalk.html","excerpt":"也许这就是偶像吧。每一次新的发现都能给我很大的精神震撼。 我一直以为庸人才应当在规则中生活，我这种疯子才应当指定规则。 人生短暂，我们来不及循规蹈矩。 我的生活永远与众不同。 这是多么狂妄自大的话啊，从我认识的其他任何一个人嘴里说出来都显得如此违和，只有你，你配得上这句话。","text":"也许这就是偶像吧。每一次新的发现都能给我很大的精神震撼。 我一直以为庸人才应当在规则中生活，我这种疯子才应当指定规则。 人生短暂，我们来不及循规蹈矩。 我的生活永远与众不同。 这是多么狂妄自大的话啊，从我认识的其他任何一个人嘴里说出来都显得如此违和，只有你，你配得上这句话。 我是一个在规则内循规蹈矩惯了的人，甚至利用规则谋求自己的利益。也是为了父母的规则，因为恐惧，恐惧父母得知女儿竟然和女生搅在一起时，那对我是灭顶之灾。 所以我选择了放弃，长痛不如短痛的放弃。我还爱吗？我告诉自己不爱了。 大学两年来，碌碌无为，既没有增长专业知识，也没有做出什么了不起的事情。我只是校园里千万颗齿轮里的一个，大齿轮，小齿轮，都是齿轮而已。 仰山之高，往往自惭形秽。可是功夫没到家，你没有资格。 对自己狠一点，再狠一点，你想要的，没人会给你，除了自己。 成为盖世英雄，而不要傻傻在原地等着。 你要的认可与肯定，只有自己能给你。 循此苦旅，循此苦旅。"}],"posts":[{"title":"论文阅读：Deep Learning Methods for Remote Heart Rate Measurement: A Review and Future Research Agenda","slug":"论文阅读：Deep-Learning-Methods-for-Remote-Heart-Rate-Measurement-A-Review-and-Future-Research-Agenda","date":"2022-09-29T07:41:40.000Z","updated":"2022-09-30T08:03:03.350Z","comments":true,"path":"2022/09/29/论文阅读：Deep-Learning-Methods-for-Remote-Heart-Rate-Measurement-A-Review-and-Future-Research-Agenda/","link":"","permalink":"https://ymyforever.netlify.app/2022/09/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeep-Learning-Methods-for-Remote-Heart-Rate-Measurement-A-Review-and-Future-Research-Agenda/","excerpt":"","text":"参考博客：https://blog.csdn.net/m0_46792836/article/details/121222265 简介rPPG，即remote photoplethysmography，远程光电容积脉搏波描记法，通过摄像头捕捉皮肤细微的亮度变化监测心率。 PPG是一种最常用的测量心率的方法，使用一个光源和光电探测器测量皮下血管体积的变化，当光源照在组织上时，光探测器可以捕捉到血液流动反射或透射光强度的微小变化，产生所谓的PPG信号，光的吸收遵循Beer–Lambert定律，该定律指出，血液吸收的光与光进入皮肤的渗透力和血液中血红蛋白的浓度成正比。在心动周期中，血红蛋白浓度的微小变化引起血管吸收的光量的32个波动，导致33个皮肤强度值的变化。 但对于穿戴式或接触式的监测设备，不适用于检测新生儿或皮肤脆弱的患者，长期检测可能会导致患者不舒服或皮肤感染。 在rPPG中，使用相机作为光探测器捕捉皮肤微小的颜色变化，自然光作为光源。其DRM（色反射）模型如下图所示，可以看到相机同时捕捉到皮肤表面产生的镜面反射和身体产生的漫反射，其中镜面反射并不包含有意义的生理信息，因此需要对捕捉到的信号进行精细处理。 传统的远程心率测量方法如下图所示，首先对视频进行人脸检测，接着从人脸上选择感兴趣的区域（ROI），以此获得包含强信号的区域，此后从ROI内的像素提取rPPG信号，最后对rPPG信号进一步处理（如频率分析和峰值检测等）获取心率信息。 基于深度学习的远程心率检测方法可以分为端到端和混合深度学习的方法。 端到端的深度学习方法所谓的端到端的方法，即模型直接将视频作为输入，产生心率或rPPG信号输出。这种方法需要大量的训练数据，同时训练结果难以验证，我们需要做更多的工作对模型进行解释。 端到端的心率检测方法又可以分为如下两种： 2D CNN2D CNN只考虑了视频帧的空间信息。 HR-CNN（2018）：包含提取器和HR估计器的两步CNN，提取器从视频帧序列中提取rPPG信号，使其信噪比最大化。HR-CNN解决了视频压缩伪影的问题。 DeepPhys（2018）：同时训练一个运动模型和外观模型，运动模型将相邻视频帧间的归一化差作为模型的输入表示，对帧中的运动和颜色变化进行建模；外观模型通过注意力机制引导运动模型学习运动表征。该模型可以更好地捕捉不同光照条件下的生理信号，对光照变化和被试运动更有鲁棒性。 MTTS-CAN（2020）：DeepPhys的改进，引入时间移位模块（TSM）捕获时间信息，TSM允许相邻帧之间的信息交换，避免昂贵的3D卷积操作。 时空网络——3D CNN3D CNN可以利用视频中包含的时间信息，时空网络（STNs）有效地表示视频流中生理信号的时空信息。 3D CNN PhysNet：旨在定位每个个体心跳的峰值，以准确估计被试的HR和HRV。 两阶段STN：包括一个时空视频增强网络（3D STVEN）和一个时空3D CNN（rPPGNet），压缩的面部视频通过3D STVEN以提高视频质量，同时保留尽可能多的信息；增强后的视频输入rPPGNet以提取rPPG信号，rPPGNet使用注意力机制从皮肤区域获取主导的rPPG特征。 AutoHR：使用神经结构搜索（NAS）自动找到最适合的主干3D CNN，使用一种三维卷积操作时域差分卷积（TDC）帮助跟踪感兴趣区域。 时空网络——2D CNN+RNN使用2D CNN提取空间信息，用RNN提取时间前后信息并结合。 基于RNN的PhysNet：首先将输入信息输入到2D CNN中提取RGB视频帧的空间特征，然后利用RNN在时域内传播这些空间特征。但研究证明，基于3D CNN的PhysNet比基于RNN的PhysNet获得了更好的性能。 混合深度学习方法所谓的混合深度学习方法，是指深度学习技术只应用在检测过程中的某些部分，如信号优化、信号提取或心率估计。 用于信号优化的深度学习方法所谓的信号优化就是使用人脸检测或皮肤分割，以忽略不相关的背景信息。 创建一个用于皮肤检测的2D CNN，用于分割出皮肤所在区域，对检测到的皮肤区域进行常规rPPG算法。但这种方法利用了人脸的所有皮肤区域提取rPPG信号，可能包含不必要的噪声。 Deep-HR：采用接受域块（RFB）网络对感兴趣区域进行目标检测，该方法设计了GAN增强检测到的ROI，对检测到的ROI进行再生，将这个高质量的ROI（这个过程也可以看作后面的信号提取的过程）用于后续的信号提取。 用于信号提取的深度学习信号提取的目标是从视频中提取高质量的rPPG信号进行HR估计（感觉是指去噪这个过程）。 LSTM：使用LSTM网络对噪声污染的rPPG信号进行滤波，得到无噪声的rPPG信号。由于数据不足的问题，在训练时可以首先在合成数据上进行训练，然后在真实数据集上微调。 2D CNN MetaPhys：使用预训练的2D CNN TS-CAN用于信号提取，并提出元学习方法，利用模型不可知元学习作为个性化参数更新模式，可以在只有少量训练样本的情况下快速适应。作者认为该方法可以减少由于肤色造成的偏差。 3D CNN Siamese-rPPG：基于Siamese 3D CNN框架，作者认为不同的面部区域应反映相同的rPPG特征，建立额头分支和脸颊分支进行特征提取，将两个分支的输出通过运算融合，得到最终的预测的rPPG信号。 3D CNN HeartTrack：利用带有注意力机制的3D CNN进行信号提取，在三维时空注意网络中，利用硬注意力机制忽略不相关的背景信息，利用软注意力机制过滤所覆盖的区域。再将提取到的信号送入1D CNN进行时间序列分析。 用于心率估计的深度学习对于提取到的rPPG信号，传统的方法是使用带通滤波器滤波，然后进行频率分析或峰值检测来估计心率。对于深度学习方法，将心率估计看作回归问题求解。 HR信号有两种表示： 频谱图像：对提取的rPPG信号进行短时傅里叶变换和带通滤波，得到频域表示，将频域表示和时域信号结合，形成频谱图像。 时空图：将ROI像素RGB通道的颜色信息串接在时间序列中，成行排列，形成时空地图。这种信号表示方法可以抑制与HR信号无关的信息。 应用场景 流行病控制 防伪：通过捕捉异常生物信号检测深度造假的视频 远程医疗 增强生物识别技术的安全性 驾驶状态检测 ？从自然灾害中寻找幸存者 新生儿检测 健康跟踪 研究缺口 影响因素：基于rPPG的远程HR测量收到光照变化、运动伪影、肤色变化和视频压缩等诸多因素的影响。新方法应提供如何从技术和生物物理角度处理这些挑战的见解 测量其他生命体征 数据集：目前数据集抓鱼用于解决运动伪影、照明变化，对于肤色变化、多人检测、远距离、新生儿检测也需要克服。 在不同HR范围上的表现 对基于深度学习方法上的理解","categories":[],"tags":[{"name":"rPPG","slug":"rPPG","permalink":"https://ymyforever.netlify.app/tags/rPPG/"}]},{"title":"经典模型：Video Swin Transformer","slug":"经典模型：Video-Swin-Transformer","date":"2022-09-28T11:10:10.000Z","updated":"2022-09-29T02:15:04.824Z","comments":true,"path":"2022/09/28/经典模型：Video-Swin-Transformer/","link":"","permalink":"https://ymyforever.netlify.app/2022/09/28/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AVideo-Swin-Transformer/","excerpt":"","text":"模型结构 Video Swin Transformer的结构整体如下图所示，与Swin-T相比多了时间维度。 3D Patch Partition 在初始时，设置每个token的大小为2x4x4x3，因此，每个视频被划分为T2×H4×W4\\frac{T}{2}\\times \\frac{H}{4} \\times \\frac{W}{4}2T​×4H​×4W​个tokens，每个token的维度为96。 Linear Embedding 将每一个token投影到指定的维度C。 Video Swin Transformer Block 对于两个相邻的Video Swin Transformer Block，仍采用和Swin Transformer相同的处理方法，每个3D W-MSA后接一个3D SW-MSA。 3D W-MSA 3D W-MSA与W-MSA相似，对于大小为8x8x8的输入，使用4x4x4的窗口大小，在stage1中窗口的数目为2x2x2，在每个窗口内部进行自注意力计算。 3D SW-MSA 3D SW-MSA与SW-MSA相同，使用移动窗口补充计算不同窗口中token间的自注意力，为了降低移动窗口后增加的窗口数，同样使用拼接与mask技术，保持窗口数的恒定。 Patch Merging 在进行patch合并时，不从时间维度进行下采样，而是从空间维度对2x2的patch进行合并，合并之后使用一个线性层投影将其维度减半。 实验结果 不同的时空注意力设计 joint：在每一个3D窗口中联合计算时空注意力。 split：在空间swin transformer的基础上添加了两个时间transformer层 factorized：先是一个空间MSA层，再接一个时间MSA层。 实验证明，在视频分类任务中，综合考虑速度与精度，joint模式达到了最佳，作者认为空间域的局部性减少了joint的计算量，同时保持了有效性。 3D token的时间维度与窗口的时间维度 总的来说，3D token的时间维度与窗口的时间维度越大，精度越高，相应的计算成本也越高。 代码实现","categories":[],"tags":[{"name":"经典模型","slug":"经典模型","permalink":"https://ymyforever.netlify.app/tags/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/"},{"name":"Transformer","slug":"Transformer","permalink":"https://ymyforever.netlify.app/tags/Transformer/"}]},{"title":"神经网络基础知识","slug":"神经网络基础知识","date":"2022-09-27T09:31:15.000Z","updated":"2022-09-27T09:31:41.226Z","comments":true,"path":"2022/09/27/神经网络基础知识/","link":"","permalink":"https://ymyforever.netlify.app/2022/09/27/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","excerpt":"","text":"","categories":[],"tags":[{"name":"神经网络基础知识","slug":"神经网络基础知识","permalink":"https://ymyforever.netlify.app/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}]},{"title":"经典模型：Swin Transformer","slug":"经典模型：Swin-Transformer","date":"2022-09-26T11:08:46.000Z","updated":"2022-09-29T07:29:27.728Z","comments":true,"path":"2022/09/26/经典模型：Swin-Transformer/","link":"","permalink":"https://ymyforever.netlify.app/2022/09/26/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9ASwin-Transformer/","excerpt":"","text":"模型结构与代码实现 参考博客：https://blog.csdn.net/qq_39478403/article/details/120042232?spm=1001.2014.3001.5506 code：https://github.com/microsoft/Swin-Transformer Swin（即Shifted Windows） Transformer可以作为CV的一种通用主干，用在分类、检测、语义分割等多种视觉任务上。 Swin Transformer的提出解决了ViT具有的以下两个问题： ViT中，由于每个token的size大小相同，难以捕捉多尺度信息。 ViT的自注意力计算复杂度是图像大小的二次方。 Swin-T构造了层次化特征图，并将自注意力的计算复杂度降为线性相关。 整体结构 Swin-T的整体架构如下图所示： Patch Partition 对于每个为H×W×3H \\times W \\times 3H×W×3的输入，划分为4×4×34 \\times 4 \\times 34×4×3大小的patch，每张图像被拆分为个patches，将每个patch展平作为一个token。 Linear Embedding 即一个全连接层，将每个大小为48的token映射到设定的维度C，此时，每张图片的输入变为了H4×W4×C\\frac{H}{4} \\times \\frac{W}{4} \\times C4H​×4W​×C，然后输入Swin Transformer Block。 123456789101112131415161718192021222324252627282930313233343536373839# Patch Partition + Linear Embeddingclass PatchEmbed(nn.Module): r&quot;&quot;&quot; Image to Patch Embedding Args: img_size (int): Image size. Default: 224. patch_size (int): Patch token size. Default: 4. in_chans (int): Number of input image channels. Default: 3. embed_dim (int): Number of linear projection output channels. Default: 96. norm_layer (nn.Module, optional): Normalization layer. Default: None &quot;&quot;&quot; def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None): super().__init__() img_size = to_2tuple(img_size) patch_size = to_2tuple(patch_size) patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]] self.img_size = img_size self.patch_size = patch_size self.patches_resolution = patches_resolution self.num_patches = patches_resolution[0] * patches_resolution[1] self.in_chans = in_chans self.embed_dim = embed_dim # 通过一个2维卷积实现patch partition与linear embedding self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size) if norm_layer is not None: self.norm = norm_layer(embed_dim) else: self.norm = None def forward(self, x): B, C, H, W = x.shape # FIXME look at relaxing size constraints assert H == self.img_size[0] and W == self.img_size[1], \\ f&quot;Input image size (&#123;H&#125;*&#123;W&#125;) doesn&#x27;t match model (&#123;self.img_size[0]&#125;*&#123;self.img_size[1]&#125;).&quot; x = self.proj(x).flatten(2).transpose(1, 2) # 先划分patch与投影，再展平与交换 if self.norm is not None: x = self.norm(x) return x Swin Transformer Block 对于Transformers中使用的全局自注意力机制，需要计算每个token与其它所有tokens间的关系，计算复杂度为token数的平方。不适用于对大量tokens进行密集预测或表示高分辨率图像等视觉问题。 W-MSA Swin-T通过在局部窗口中计算自注意力，将计算复杂度降低为token数的线性关系，设每个非重叠局部窗口中包含M×MM\\times MM×M个tokens。 MSA：有hwhwhw个tokens，每个token在全局计算hwhwhw次； W-MSA：有hwhwhw个tokens，每个token在全局计算M2M^2M2次。 首先将输入划分为若干个大小为MxM的窗口。 123456789101112def window_partition(x, window_size): &quot;&quot;&quot; Args: x: (B, H, W, C) window_size (int): window size Returns: windows: (num_windows*B, window_size, window_size, C) &quot;&quot;&quot; B, H, W, C = x.shape x = x.view(B, H // window_size, window_size, W // window_size, window_size, C) windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C) return windows SW-MSA W-MSA限制了跨窗口token间的交流与联系，从而限制了建模表征能力。作者提出了一种移位窗口划分方法SW-MSA，在模型中交替使用两种MSA方法（因此每个stage中Swin Transformer Block的数量都为偶数）。 所谓的移动窗口即将窗口循环位移，如下图所示： 但直接移位得到的窗口大小是不规则的，不利于并行计算，同时9个窗口也提升了计算成本。为了解决这个问题，将重新划分后的窗口进行拼接，如下图所示，得到4个窗口。 4个窗口中来自不同初始位置的patch不应进行自注意计算，因此使用mask机制，将不需要的注意力图置0。 W-MSA和SW-MSA公用一块代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980class WindowAttention(nn.Module): r&quot;&quot;&quot; Window based multi-head self attention (W-MSA) module with relative position bias. It supports both of shifted and non-shifted window. Args: dim (int): Number of input channels. window_size (tuple[int]): The height and width of the window. num_heads (int): Number of attention heads. qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0 proj_drop (float, optional): Dropout ratio of output. Default: 0.0 &quot;&quot;&quot; def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.): super().__init__() self.dim = dim self.window_size = window_size # Wh, Ww self.num_heads = num_heads head_dim = dim // num_heads self.scale = qk_scale or head_dim ** -0.5 # define a parameter table of relative position bias self.relative_position_bias_table = nn.Parameter( torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)) # 2*Wh-1 * 2*Ww-1, nH # get pair-wise relative position index for each token inside the window coords_h = torch.arange(self.window_size[0]) coords_w = torch.arange(self.window_size[1]) coords = torch.stack(torch.meshgrid([coords_h, coords_w])) # 2, Wh, Ww coords_flatten = torch.flatten(coords, 1) # 2, Wh*Ww relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :] # 2, Wh*Ww, Wh*Ww relative_coords = relative_coords.permute(1, 2, 0).contiguous() # Wh*Ww, Wh*Ww, 2 relative_coords[:, :, 0] += self.window_size[0] - 1 # shift to start from 0 relative_coords[:, :, 1] += self.window_size[1] - 1 relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1 relative_position_index = relative_coords.sum(-1) # Wh*Ww, Wh*Ww self.register_buffer(&quot;relative_position_index&quot;, relative_position_index) self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias) self.attn_drop = nn.Dropout(attn_drop) self.proj = nn.Linear(dim, dim) self.proj_drop = nn.Dropout(proj_drop) trunc_normal_(self.relative_position_bias_table, std=.02) self.softmax = nn.Softmax(dim=-1) def forward(self, x, mask=None): &quot;&quot;&quot; Args: x: input features with shape of (num_windows*B, N, C) mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None &quot;&quot;&quot; B_, N, C = x.shape qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4) q, k, v = qkv[0], qkv[1], qkv[2] # make torchscript happy (cannot use tensor as tuple) q = q * self.scale attn = (q @ k.transpose(-2, -1)) # 相对位置偏移 relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view( self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1) # Wh*Ww,Wh*Ww,nH relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous() # nH, Wh*Ww, Wh*Ww attn = attn + relative_position_bias.unsqueeze(0) # 判断是否需要mask if mask is not None: nW = mask.shape[0] attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0) attn = attn.view(-1, self.num_heads, N, N) attn = self.softmax(attn) else: attn = self.softmax(attn) attn = self.attn_drop(attn) x = (attn @ v).transpose(1, 2).reshape(B_, N, C) x = self.proj(x) x = self.proj_drop(x) return x 相对位置偏置 在计算自注意力时，在计算相似度的过程中对每个head加入相对位置偏置，如下所示： 对于预训练中学到的相对位置偏置，可以通过双三次插值初始化具有不同窗口大小的微调模型。 Patch Merging Patch Merging层的功能是产生一个层次化表示，通过合并相邻的tokens，减少tokens的数目。 对于Stage1和Stage2间的Patch Merging层，将原维度为C的token合并为大小为4C的token，再使用一个线性层将输出维度降低为2C，token的数目降低为H8×W8\\frac{H}{8} \\times \\frac{W}{8}8H​×8W​。 在之后的每个stage中，都会改变张量的维度，从而形成一种层次化的特征。 12345678910111213141516171819202122232425262728293031323334353637class PatchMerging(nn.Module): r&quot;&quot;&quot; Patch Merging Layer. Args: input_resolution (tuple[int]): Resolution of input feature. dim (int): Number of input channels. norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm &quot;&quot;&quot; def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm): super().__init__() self.input_resolution = input_resolution self.dim = dim self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False) self.norm = norm_layer(4 * dim) def forward(self, x): &quot;&quot;&quot; x: B, H*W, C &quot;&quot;&quot; H, W = self.input_resolution B, L, C = x.shape assert L == H * W, &quot;input feature has wrong size&quot; assert H % 2 == 0 and W % 2 == 0, f&quot;x size (&#123;H&#125;*&#123;W&#125;) are not even.&quot; x = x.view(B, H, W, C)# 把输入整形为BHWC x0 = x[:, 0::2, 0::2, :] # B H/2 W/2 C x1 = x[:, 1::2, 0::2, :] # B H/2 W/2 C x2 = x[:, 0::2, 1::2, :] # B H/2 W/2 C x3 = x[:, 1::2, 1::2, :] # B H/2 W/2 C x = torch.cat([x0, x1, x2, x3], -1) # B H/2 W/2 4*C x = x.view(B, -1, 4 * C) # B H/2*W/2 4*C x = self.norm(x) x = self.reduction(x)# 降低维度为原来的1/2 return x","categories":[],"tags":[{"name":"经典模型","slug":"经典模型","permalink":"https://ymyforever.netlify.app/tags/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/"},{"name":"Transformer","slug":"Transformer","permalink":"https://ymyforever.netlify.app/tags/Transformer/"}]},{"title":"经典模型：Vision Transformer","slug":"经典模型：Vision-Transformer","date":"2022-09-26T00:32:10.000Z","updated":"2022-09-27T09:17:44.057Z","comments":true,"path":"2022/09/26/经典模型：Vision-Transformer/","link":"","permalink":"https://ymyforever.netlify.app/2022/09/26/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%EF%BC%9AVision-Transformer/","excerpt":"","text":"模型介绍 可参考博客：https://blog.csdn.net/qq_39478403/article/details/118704747 模型结构ViT主要使用Transformer的encoder部分 将一张图像分成若干个大小固定且相同的patch，将每个patch投影到线性空间中再加上位置编码 除了每个patch作为一个token外，在序列中添加一个额外的classification token 位置编码：（看下代码咋实现的） 使用可学习的一维位置编码（作者发现使用更高维的位置编码并没有带来显著的精度提升） 混合结构： 可以将原始图像使用CNN进行特征提取，将特征图按patch划分送入Transformer中。 实验设置ViT使用： ViT与Bert类似，先在大数据集上训练，再在downstream任务上微调。在微调时，将预训练用的预测头换成一个用0初始化的DxK的前馈层，其中K代表下游任务总的类别数。 在微调时，使用更高分辨率的图像可以获得更好的结果。（patch大小不变，输入序列变成，不影响网络结构），这样会导致之前训练得到的位置编码无意义，因此在原来的位置编码上进行一个2D的插值。 实验结果： 当考虑预训练的训练代价时，ViT以更低的代价达到了SOTA水平 在自监督问题上，ViT很有应用前景 代码实现 Official Code：https://github.com/google-research/vision_transformer timm code：https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py Patch Embeddings实现功能：将输入图像划分为若干个patch，并将每个patch拉平投影到D维。通过一个二维的卷积操作即可实现。 123456789101112131415161718192021222324252627282930313233343536# https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/patch_embed.pyclass PatchEmbed(nn.Module): &quot;&quot;&quot; 2D Image to Patch Embedding &quot;&quot;&quot; def __init__( self, img_size=224, patch_size=16,# 每个patch大小为16x16x3 in_chans=3, embed_dim=768,# 将patch映射到768维 norm_layer=None, flatten=True, bias=True, ): super().__init__() img_size = to_2tuple(img_size) patch_size = to_2tuple(patch_size) self.img_size = img_size self.patch_size = patch_size self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1]) self.num_patches = self.grid_size[0] * self.grid_size[1]# 每张图像对应patch个数 self.flatten = flatten # 通过一步卷积操作实现嵌入 self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias) self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity() def forward(self, x): B, C, H, W = x.shape _assert(H == self.img_size[0], f&quot;Input image height (&#123;H&#125;) doesn&#x27;t match model (&#123;self.img_size[0]&#125;).&quot;) _assert(W == self.img_size[1], f&quot;Input image width (&#123;W&#125;) doesn&#x27;t match model (&#123;self.img_size[1]&#125;).&quot;) x = self.proj(x)# 投影 if self.flatten: x = x.flatten(2).transpose(1, 2) # BCHW -&gt; BNC x = self.norm(x) return x 可学习的嵌入 cls_token：为了与Bert保持一致，设置可学习的嵌入向量作为用于分类的类别向量。 pos_embed：由于自注意力机制具有扰动不变性（打乱tokens中的顺序并不会改变结果），因此需要位置编码标识位置信息。 1234567891011121314151617181920# cls_tokenself.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if class_token else None# pos_embedembed_len = num_patches if no_embed_class else num_patches + self.num_prefix_tokensself.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)def _pos_embed(self, x): if self.no_embed_class: # 先加位置编码再拼接cls_token # position embedding does not overlap with class token, add then concat x = x + self.pos_embed if self.cls_token is not None: x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1) else: # 先拼接cls_token再加位置编码 # pos_embed has entry for class token, concat then add if self.cls_token is not None: x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1) x = x + self.pos_embed return self.pos_drop(x) 多头注意力机制1234567891011121314151617181920212223242526class Attention(nn.Module): def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.): super().__init__() assert dim % num_heads == 0, &#x27;dim should be divisible by num_heads&#x27; self.num_heads = num_heads head_dim = dim // num_heads self.scale = head_dim ** -0.5 self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias) self.attn_drop = nn.Dropout(attn_drop) self.proj = nn.Linear(dim, dim) self.proj_drop = nn.Dropout(proj_drop) def forward(self, x): B, N, C = x.shape qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4) q, k, v = qkv.unbind(0) # make torchscript happy (cannot use tensor as tuple) attn = (q @ k.transpose(-2, -1)) * self.scale# QK attn = attn.softmax(dim=-1) attn = self.attn_drop(attn) x = (attn @ v).transpose(1, 2).reshape(B, N, C)# 乘以权重 x = self.proj(x) x = self.proj_drop(x) return x MLP1234567891011121314151617181920212223class Mlp(nn.Module): &quot;&quot;&quot; MLP as used in Vision Transformer, MLP-Mixer and related networks &quot;&quot;&quot; def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, bias=True, drop=0.): super().__init__() out_features = out_features or in_features hidden_features = hidden_features or in_features bias = to_2tuple(bias) drop_probs = to_2tuple(drop) self.fc1 = nn.Linear(in_features, hidden_features, bias=bias[0]) self.act = act_layer() self.drop1 = nn.Dropout(drop_probs[0]) self.fc2 = nn.Linear(hidden_features, out_features, bias=bias[1]) self.drop2 = nn.Dropout(drop_probs[1]) def forward(self, x): x = self.fc1(x) x = self.act(x) x = self.drop1(x) x = self.fc2(x) x = self.drop2(x) return Block实现功能：ViT的每个Block包括一层Attention和一层MLP。 12345678910111213141516171819202122232425262728293031class Block(nn.Module): def __init__( self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0., init_values=None, drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm ): super().__init__() self.norm1 = norm_layer(dim) self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop) self.ls1 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity() # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here self.drop_path1 = DropPath(drop_path) if drop_path &gt; 0. else nn.Identity() self.norm2 = norm_layer(dim) self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop) self.ls2 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity() self.drop_path2 = DropPath(drop_path) if drop_path &gt; 0. else nn.Identity() def forward(self, x): x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x)))) x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x)))) return x","categories":[],"tags":[{"name":"经典模型","slug":"经典模型","permalink":"https://ymyforever.netlify.app/tags/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/"},{"name":"Transformer","slug":"Transformer","permalink":"https://ymyforever.netlify.app/tags/Transformer/"}]},{"title":"论文阅读：Revisiting Pixel-Wise Supervision for Face Anti-Spoofing","slug":"论文阅读：Revisiting-Pixel-Wise-Supervision-for-Face-Anti-Spoofing","date":"2022-09-22T02:17:48.000Z","updated":"2022-09-22T03:25:46.440Z","comments":true,"path":"2022/09/22/论文阅读：Revisiting-Pixel-Wise-Supervision-for-Face-Anti-Spoofing/","link":"","permalink":"https://ymyforever.netlify.app/2022/09/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARevisiting-Pixel-Wise-Supervision-for-Face-Anti-Spoofing/","excerpt":"","text":"发表时间：2021 研究内容：像素级的人脸识别反欺诈方法 创新点：提出基于金字塔的监督方法，模型从多空间尺度上学习局部和全局的语义信息 Presentation Attack Detection研究历史： 传统算法关注于活体和手工特征的检测，需要丰富的任务级的先验知识。 活体检测：关注眨眼、面部和头部动作、视线追踪以及远程生理信号（这种方法需要长期的互动，容易被video attacks伪造）。 经典的handcrafted descriptors：从多种色彩空间中提取有效的欺诈模式，这种PA方法可以通过训练分类器捕捉，但在遇到未见过的场景或未知的PAs时就失效了。","categories":[],"tags":[{"name":"组内文章","slug":"组内文章","permalink":"https://ymyforever.netlify.app/tags/%E7%BB%84%E5%86%85%E6%96%87%E7%AB%A0/"},{"name":"人脸识别","slug":"人脸识别","permalink":"https://ymyforever.netlify.app/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"}]},{"title":"论文阅读：Rethinking the ST-GCNs for 3D skeleton-based human action recognition","slug":"论文阅读：Rethinking-the-ST-GCNs-for-3D-skeleton-based-human-action-recognition","date":"2022-09-20T02:54:50.000Z","updated":"2022-09-22T02:18:47.677Z","comments":true,"path":"2022/09/20/论文阅读：Rethinking-the-ST-GCNs-for-3D-skeleton-based-human-action-recognition/","link":"","permalink":"https://ymyforever.netlify.app/2022/09/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARethinking-the-ST-GCNs-for-3D-skeleton-based-human-action-recognition/","excerpt":"","text":"发表时间：2021 ST-GCN：Spatial-Temporal Graph Convolutional Network，用于解决骨骼数据的动作识别问题。 研究内容： 证明了在ST-GCN中很多操作对于人体动作识别是没必要的 提出了一个简单有效的策略捕捉全局图的相关性，对输入序列进行有效建模，同时将输入图序列降入欧几里得空间，可以使用多尺度时域滤波器捕捉动态信息。 研究现状： 骨骼数据成为人体动作识别的主流输入（与传统的RGB视频数据相比，信息更完整） 直接将结构化的数据重新排列，使得tensor适应基础的神经网络（由于骨骼数据中没有天然的局部性概念，深度学习的能力受到限制） 设计一种适应结构化数据的自定义神经网络（ST-GCN） TBC：GCN好难，看不懂","categories":[],"tags":[{"name":"组内文章","slug":"组内文章","permalink":"https://ymyforever.netlify.app/tags/%E7%BB%84%E5%86%85%E6%96%87%E7%AB%A0/"},{"name":"人体动作识别","slug":"人体动作识别","permalink":"https://ymyforever.netlify.app/tags/%E4%BA%BA%E4%BD%93%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/"},{"name":"GCN","slug":"GCN","permalink":"https://ymyforever.netlify.app/tags/GCN/"}]},{"title":"论文阅读：Social Distancing Alert with Smartwatches","slug":"论文阅读：Social-Distancing-Alert-with-Smartwatches","date":"2022-09-20T01:34:02.000Z","updated":"2022-09-20T02:49:35.605Z","comments":true,"path":"2022/09/20/论文阅读：Social-Distancing-Alert-with-Smartwatches/","link":"","permalink":"https://ymyforever.netlify.app/2022/09/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ASocial-Distancing-Alert-with-Smartwatches/","excerpt":"","text":"发表时间：2022 研究内容：基于智能手表的社交距离警报系统SoDA，SoDA使用加速器和陀螺仪的数据和简单有效的视觉Transformer模型，识别违反社交距离的活动。 code：https://github.com/aiotgroup/SoDA 创新点： 应用价值 创建了一个数据集 证明了ViT是一种有效的方法？ 模型结构：","categories":[],"tags":[{"name":"Transformer","slug":"Transformer","permalink":"https://ymyforever.netlify.app/tags/Transformer/"},{"name":"组内文章","slug":"组内文章","permalink":"https://ymyforever.netlify.app/tags/%E7%BB%84%E5%86%85%E6%96%87%E7%AB%A0/"}]},{"title":"论文阅读：PhysFormer: Facial Video-based Physiological Measurement with Temporal Difference Transformer","slug":"论文阅读：PhysFormer-Facial-Video-based-Physiological-Measurement-with-Temporal-Difference-Transformer","date":"2022-09-15T02:12:07.000Z","updated":"2022-09-30T08:04:23.336Z","comments":true,"path":"2022/09/15/论文阅读：PhysFormer-Facial-Video-based-Physiological-Measurement-with-Temporal-Difference-Transformer/","link":"","permalink":"https://ymyforever.netlify.app/2022/09/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9APhysFormer-Facial-Video-based-Physiological-Measurement-with-Temporal-Difference-Transformer/","excerpt":"","text":"发表时间：2021 研究对象：rPPG，使用多波长 RGB 相机检测人体皮肤表面脉冲引起的细微颜色变化，实现测量心脏活动和其它生理信号。 研究意义：传统的检测方法会造成discomfort，并且长期检测不方便 Code：https://github.com/ZitongYu/PhysFormer rPPG研究历史： 早期使用经典的信号处理方法检测面部细微的颜色变化； 使用非端到端方法，首先生成预处理的信号特征，然后模型从这些特征图中捕捉rPPG特征（对预处理要求严格，忽略了全局特征）； 端到端的基于深度学习的方法（容易被复杂的背景信息影响）。 研究现状： ​ 现有的基于卷积神经网络的模型在时间和空间上的感受野受限，忽略了长期的时间和空间上的互动与感知。 PhysFormer网络架构： Stem：提取粗糙的局部时空特征 Tube Tokens：将stem输出划分为若干个时空tube token，将时空邻近语义聚合在一起，并减少后续transformer的计算量 Temporal Difference Multi-head Self-attention：与传统的自注意力机制不同，使用TDC计算距离，可以捕捉局部细粒度的时间差异特征 Spatio-temporal Feed-forward： 创新点：PhysFormer，一种端到端的视频transformer，联合使用了局部的和全局的时空特征。 使用时差引导全局注意力机制，强化rPPG的周期性特征，针对干扰完善局部时空特征； 使用受label distribution learning和curriculum learning启发的频域动态约束，为PhysFormer提供详细的监督，缓解过拟合。 PhysFormer不需要像其它transformer网络那样在大规模数据集上预训练，仅在rPPG数据集训练即可。 Label Distribution Learning：对于面部的rPPG信号，心率相近的视频会有相似的周期性特征。为了使得模型学习到这种特征，将心率估计问题看作一个多分类问题，有多少个心率就有多少类别，类别概率向量由高斯分布组成。 Curriculum Learning Guided Dynamic Loss：课程式学习是指模型从容易样本开始学习，逐步学习困难样本。在该任务中，从时域和频域两个方面限制模型学习，时域的限制更直接更容易学习，频域的限制较难学习，因此，使用动态的loss函数，逐步提高频域loss的比例。 实验：不同模型的对比实验、消融实验 Others：注意力图可视化","categories":[],"tags":[{"name":"Transformer","slug":"Transformer","permalink":"https://ymyforever.netlify.app/tags/Transformer/"},{"name":"组内文章","slug":"组内文章","permalink":"https://ymyforever.netlify.app/tags/%E7%BB%84%E5%86%85%E6%96%87%E7%AB%A0/"},{"name":"rPPG","slug":"rPPG","permalink":"https://ymyforever.netlify.app/tags/rPPG/"}]},{"title":"论文阅读：Model Behavior Preserving for Class-Incremental Learning","slug":"论文阅读：Model-Behavior-Preserving-for-Class-Incremental-Learning","date":"2022-08-23T02:45:11.000Z","updated":"2022-08-26T01:43:14.413Z","comments":true,"path":"2022/08/23/论文阅读：Model-Behavior-Preserving-for-Class-Incremental-Learning/","link":"","permalink":"https://ymyforever.netlify.app/2022/08/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AModel-Behavior-Preserving-for-Class-Incremental-Learning/","excerpt":"","text":"发表时间：2022 研究内容：类增量学习，探讨在增量学习中应保留旧模型的哪些功能性属性。 研究现状：现有的增量学习方法忽略了CNN模型响应间的内部结构，KD的硬约束导致新模型出现混沌行为。 创新点： Feature Space：设计了一个INP Loss保持成对实例在旧模型上的相似性顺序（反映实例集间的相邻关系）； INP用于惩罚新模型在学习过程中每个实例相邻关系的变化。 a. 旧实例A在特征空间中与其它实例的相邻关系； b. 采用传统的KD，引入新实例G后，绝对位置的微小变化被严格限制； c. 采用INP Loss当相对位置没变时，就不会限制更新。 Label Space：设计了一个LPP Loss在输出空间的实例标签概率向量中保留标签排名列表（反映实例属于每一类的排名）； 介绍了一种可导的排名计算方法用于计算上述Loss。","categories":[],"tags":[{"name":"组内文章","slug":"组内文章","permalink":"https://ymyforever.netlify.app/tags/%E7%BB%84%E5%86%85%E6%96%87%E7%AB%A0/"},{"name":"连续学习","slug":"连续学习","permalink":"https://ymyforever.netlify.app/tags/%E8%BF%9E%E7%BB%AD%E5%AD%A6%E4%B9%A0/"}]},{"title":"论文阅读：Structural Knowledge Organization and Transfer for Class-Incremental Learning","slug":"论文阅读：Structural-Knowledge-Organization-and-Transfer-for-Class-Incremental-Learning","date":"2022-08-21T08:05:32.000Z","updated":"2022-08-23T02:20:35.961Z","comments":true,"path":"2022/08/21/论文阅读：Structural-Knowledge-Organization-and-Transfer-for-Class-Incremental-Learning/","link":"","permalink":"https://ymyforever.netlify.app/2022/08/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AStructural-Knowledge-Organization-and-Transfer-for-Class-Incremental-Learning/","excerpt":"","text":"发表时间：2021 研究问题：类增量学习 研究现状： 经典的知识蒸馏方法忽略了信息点之间的关联，当新数据远多于旧数据时，面临着严重的偏差问题； KD：在特征空间中，孤立地限制单个训练样本的位置，样本间的关系可能会被改变，并导致分类错误。 SGKD：保持样本的结构化知识，包括样本的位置和样本间关系，确保蒸馏后样本仍能被正确地分类。 创新点： 使用一个memory knowledge graph(MKG)表征历史任务的结构化知识 在特征空间中的绝对位置（MKG中用顶点表示已知example间的特征向量） example间对应关系（边表示，使用余弦距离） 使用图插值机制丰富知识域、缓解类间样本不平衡问题 通过向MKG中插入假的顶点，扩充和平滑分散的数据集，假顶点通过mix两个真顶点的vector得到。 使用结构化图知识蒸馏（SGKD）迁移旧知识 顶点蒸馏损失 边蒸馏损失 人脸识别是怎么实现增加新样本的？ 人脸识别网络的本质是一个特征提取器，并不是分类器，识别人脸的时候，通过计算输出特征和人脸库中人脸的距离判断人脸所属对象。未涉及类增量学习。","categories":[],"tags":[{"name":"论文阅读","slug":"论文阅读","permalink":"https://ymyforever.netlify.app/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"组内文章","slug":"组内文章","permalink":"https://ymyforever.netlify.app/tags/%E7%BB%84%E5%86%85%E6%96%87%E7%AB%A0/"},{"name":"连续学习","slug":"连续学习","permalink":"https://ymyforever.netlify.app/tags/%E8%BF%9E%E7%BB%AD%E5%AD%A6%E4%B9%A0/"}]},{"title":"论文阅读：IDPT, Interconnected Dual Pyramid Transformer for Face Super-Resolution","slug":"论文阅读：IDPT","date":"2022-08-14T01:31:37.000Z","updated":"2022-09-19T03:43:55.484Z","comments":true,"path":"2022/08/14/论文阅读：IDPT/","link":"","permalink":"https://ymyforever.netlify.app/2022/08/14/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AIDPT/","excerpt":"","text":"研究问题：人脸超分辨率技术 FSR：关注于恢复重要的面部结构 创新点：提出了一个新的、有效的基于Transformer的人脸超分辨率架构 设计了金字塔结构的encode&#x2F;decoder的Transformer架构：分别提取粗糙纹理和精细纹理。 通过一个底部的金字塔特征提取器，将双重金字塔Transformer建立起联系。 在每个spatial layer插入一个新的融合调制模块：使用粗糙纹理完善对应的精细纹理，融合浅层的粗糙纹理和对应的深层的精细纹理。 FSR研究现状 现有技术在解决超低分辨率问题上表现很差 卷积难以描述不同域间的关联和捕捉远域间的依赖 网络结构：","categories":[],"tags":[{"name":"论文阅读","slug":"论文阅读","permalink":"https://ymyforever.netlify.app/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"FSR","slug":"FSR","permalink":"https://ymyforever.netlify.app/tags/FSR/"},{"name":"组内文章","slug":"组内文章","permalink":"https://ymyforever.netlify.app/tags/%E7%BB%84%E5%86%85%E6%96%87%E7%AB%A0/"}]},{"title":"HelloWorld!","slug":"HelloWorld","date":"2020-08-29T08:06:44.000Z","updated":"2022-08-13T09:48:18.595Z","comments":true,"path":"2020/08/29/HelloWorld/","link":"","permalink":"https://ymyforever.netlify.app/2020/08/29/HelloWorld/","excerpt":"","text":"A new world!哈喽！历经一个下午博客终于搭建好了，原来是那么容易的一件事情，大一的时候想的很复杂，迟迟没能动手，现在也终于有了自己的小博客啦~ 未来灌水的文章还是会首先发在CSDN上，这里会分享一些重大的经历~已经大三了！要更努力学习！不要被些奇奇怪怪的事情干扰，奥里给！","categories":[],"tags":[]}],"categories":[],"tags":[{"name":"rPPG","slug":"rPPG","permalink":"https://ymyforever.netlify.app/tags/rPPG/"},{"name":"经典模型","slug":"经典模型","permalink":"https://ymyforever.netlify.app/tags/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/"},{"name":"Transformer","slug":"Transformer","permalink":"https://ymyforever.netlify.app/tags/Transformer/"},{"name":"神经网络基础知识","slug":"神经网络基础知识","permalink":"https://ymyforever.netlify.app/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"name":"组内文章","slug":"组内文章","permalink":"https://ymyforever.netlify.app/tags/%E7%BB%84%E5%86%85%E6%96%87%E7%AB%A0/"},{"name":"人脸识别","slug":"人脸识别","permalink":"https://ymyforever.netlify.app/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"},{"name":"人体动作识别","slug":"人体动作识别","permalink":"https://ymyforever.netlify.app/tags/%E4%BA%BA%E4%BD%93%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/"},{"name":"GCN","slug":"GCN","permalink":"https://ymyforever.netlify.app/tags/GCN/"},{"name":"连续学习","slug":"连续学习","permalink":"https://ymyforever.netlify.app/tags/%E8%BF%9E%E7%BB%AD%E5%AD%A6%E4%B9%A0/"},{"name":"论文阅读","slug":"论文阅读","permalink":"https://ymyforever.netlify.app/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"FSR","slug":"FSR","permalink":"https://ymyforever.netlify.app/tags/FSR/"}]}